"""
å¤šè‡‚è€è™æœºç®—æ³•è¯¦è§£ä¸å®ç°
Multi-Armed Bandit (MAB) Algorithms

å¤šè‡‚è€è™æœºæ˜¯å¼ºåŒ–å­¦ä¹ ä¸­çš„ç»å…¸é—®é¢˜ï¼Œä¸“æ³¨äºæ¢ç´¢ä¸åˆ©ç”¨çš„æƒè¡¡ã€‚
è™½ç„¶æ˜¯æœ€ç®€å•çš„å¼ºåŒ–å­¦ä¹ åœºæ™¯ï¼ˆåªæœ‰ä¸€ä¸ªçŠ¶æ€ï¼‰ï¼Œä½†åŒ…å«äº†å¼ºåŒ–å­¦ä¹ çš„æ ¸å¿ƒæ€æƒ³ã€‚

æœ¬æ–‡ä»¶åŒ…å«ï¼š
1. å¤šè‡‚è€è™æœºé—®é¢˜å®šä¹‰
2. è´ªå¿ƒç®—æ³•
3. Îµ-è´ªå¿ƒç®—æ³•
4. Upper Confidence Bound (UCB) ç®—æ³•
5. Thompson Sampling ç®—æ³•
6. ç®—æ³•æ¯”è¾ƒä¸åˆ†æ
"""

import numpy as np
import matplotlib.pyplot as plt
import random
from typing import List, Tuple
import math
from abc import ABC, abstractmethod
from scipy import stats

class BanditEnvironment:
    """
    å¤šè‡‚è€è™æœºç¯å¢ƒ
    æ¯ä¸ªè€è™æœºéƒ½æœ‰ä¸åŒçš„å¥–åŠ±åˆ†å¸ƒ
    """
    
    def __init__(self, n_arms: int, reward_type: str = "gaussian"):
        """
        åˆå§‹åŒ–å¤šè‡‚è€è™æœºç¯å¢ƒ
        
        Args:
            n_arms: è€è™æœºæ•°é‡
            reward_type: å¥–åŠ±ç±»å‹ ("gaussian", "bernoulli")
        """
        self.n_arms = n_arms
        self.reward_type = reward_type
        
        if reward_type == "gaussian":
            # é«˜æ–¯åˆ†å¸ƒå¥–åŠ±ï¼šæ¯ä¸ªè‡‚æœ‰ä¸åŒçš„å‡å€¼ï¼Œæ–¹å·®ä¸º1
            self.true_means = np.random.normal(0, 1, n_arms)
            self.optimal_arm = np.argmax(self.true_means)
        elif reward_type == "bernoulli":
            # ä¼¯åŠªåˆ©åˆ†å¸ƒå¥–åŠ±ï¼šæ¯ä¸ªè‡‚æœ‰ä¸åŒçš„æˆåŠŸæ¦‚ç‡
            self.true_probs = np.random.beta(2, 2, n_arms)
            self.optimal_arm = np.argmax(self.true_probs)
        
        self.reset_stats()
    
    def reset_stats(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        self.arm_counts = np.zeros(self.n_arms)
        self.total_pulls = 0
        self.regret_history = []
        
    def pull_arm(self, arm: int) -> float:
        """
        æ‹‰åŠ¨æŒ‡å®šçš„è€è™æœºè‡‚
        
        Args:
            arm: è€è™æœºè‡‚çš„ç´¢å¼•
            
        Returns:
            è·å¾—çš„å¥–åŠ±
        """
        if arm < 0 or arm >= self.n_arms:
            raise ValueError(f"æ— æ•ˆçš„è‡‚ç´¢å¼•: {arm}")
        
        self.arm_counts[arm] += 1
        self.total_pulls += 1
        
        if self.reward_type == "gaussian":
            reward = np.random.normal(self.true_means[arm], 1)
            # è®¡ç®—ç¬æ—¶é—æ†¾ï¼ˆæœ€ä¼˜è‡‚çš„å¥–åŠ± - å½“å‰é€‰æ‹©çš„æœŸæœ›å¥–åŠ±ï¼‰
            instant_regret = self.true_means[self.optimal_arm] - self.true_means[arm]
        elif self.reward_type == "bernoulli":
            reward = np.random.binomial(1, self.true_probs[arm])
            instant_regret = self.true_probs[self.optimal_arm] - self.true_probs[arm]
        
        self.regret_history.append(instant_regret)
        return reward
    
    def get_cumulative_regret(self) -> float:
        """è·å–ç´¯ç§¯é—æ†¾"""
        return np.sum(self.regret_history)
    
    def get_optimal_arm_ratio(self) -> float:
        """è·å–é€‰æ‹©æœ€ä¼˜è‡‚çš„æ¯”ä¾‹"""
        if self.total_pulls == 0:
            return 0.0
        return self.arm_counts[self.optimal_arm] / self.total_pulls
    
    def display_info(self):
        """æ˜¾ç¤ºç¯å¢ƒä¿¡æ¯"""
        print(f"ğŸ° å¤šè‡‚è€è™æœºç¯å¢ƒä¿¡æ¯")
        print(f"è€è™æœºæ•°é‡: {self.n_arms}")
        print(f"å¥–åŠ±ç±»å‹: {self.reward_type}")
        
        if self.reward_type == "gaussian":
            print("æ¯ä¸ªè‡‚çš„çœŸå®å‡å€¼:")
            for i, mean in enumerate(self.true_means):
                marker = "â­" if i == self.optimal_arm else "  "
                print(f"  è‡‚ {i}: {mean:.3f} {marker}")
        elif self.reward_type == "bernoulli":
            print("æ¯ä¸ªè‡‚çš„æˆåŠŸæ¦‚ç‡:")
            for i, prob in enumerate(self.true_probs):
                marker = "â­" if i == self.optimal_arm else "  "
                print(f"  è‡‚ {i}: {prob:.3f} {marker}")
        
        print(f"æœ€ä¼˜è‡‚: {self.optimal_arm}")

class BanditAgent(ABC):
    """
    å¤šè‡‚è€è™æœºæ™ºèƒ½ä½“æŠ½è±¡åŸºç±»
    """
    
    def __init__(self, n_arms: int):
        self.n_arms = n_arms
        self.reset()
    
    def reset(self):
        """é‡ç½®æ™ºèƒ½ä½“çŠ¶æ€"""
        self.arm_counts = np.zeros(self.n_arms)
        self.arm_rewards = np.zeros(self.n_arms)
        self.total_reward = 0.0
        self.time_step = 0
    
    @abstractmethod
    def select_arm(self) -> int:
        """é€‰æ‹©è¦æ‹‰åŠ¨çš„è‡‚"""
        pass
    
    def update(self, arm: int, reward: float):
        """æ›´æ–°æ™ºèƒ½ä½“çŠ¶æ€"""
        self.arm_counts[arm] += 1
        self.arm_rewards[arm] += reward
        self.total_reward += reward
        self.time_step += 1
    
    def get_estimated_values(self) -> np.ndarray:
        """è·å–æ¯ä¸ªè‡‚çš„ä¼°è®¡ä»·å€¼"""
        with np.errstate(divide='ignore', invalid='ignore'):
            estimated_values = self.arm_rewards / self.arm_counts
            # å°†æœªæ‹‰åŠ¨è¿‡çš„è‡‚çš„ä¼°è®¡å€¼è®¾ä¸º0
            estimated_values[self.arm_counts == 0] = 0
        return estimated_values

class GreedyAgent(BanditAgent):
    """
    è´ªå¿ƒç®—æ³•æ™ºèƒ½ä½“
    æ€»æ˜¯é€‰æ‹©å½“å‰ä¼°è®¡ä»·å€¼æœ€é«˜çš„è‡‚
    """
    
    def __init__(self, n_arms: int, initial_value: float = 0.0):
        super().__init__(n_arms)
        self.initial_value = initial_value
        
    def select_arm(self) -> int:
        if self.time_step < self.n_arms:
            # åˆå§‹é˜¶æ®µï¼šæ¯ä¸ªè‡‚è‡³å°‘å°è¯•ä¸€æ¬¡
            return self.time_step
        
        # é€‰æ‹©ä¼°è®¡ä»·å€¼æœ€é«˜çš„è‡‚
        estimated_values = self.get_estimated_values()
        # å¯¹äºæœªå°è¯•è¿‡çš„è‡‚ï¼Œä½¿ç”¨åˆå§‹å€¼
        estimated_values[self.arm_counts == 0] = self.initial_value
        return np.argmax(estimated_values)

class EpsilonGreedyAgent(BanditAgent):
    """
    Îµ-è´ªå¿ƒç®—æ³•æ™ºèƒ½ä½“
    ä»¥Îµçš„æ¦‚ç‡æ¢ç´¢ï¼Œä»¥(1-Îµ)çš„æ¦‚ç‡åˆ©ç”¨
    """
    
    def __init__(self, n_arms: int, epsilon: float = 0.1, decay: bool = False):
        super().__init__(n_arms)
        self.initial_epsilon = epsilon
        self.epsilon = epsilon
        self.decay = decay
        
    def select_arm(self) -> int:
        if self.decay:
            # éšæ—¶é—´è¡°å‡Îµ
            self.epsilon = self.initial_epsilon / (1 + self.time_step * 0.001)
        
        if random.random() < self.epsilon:
            # æ¢ç´¢ï¼šéšæœºé€‰æ‹©
            return random.randint(0, self.n_arms - 1)
        else:
            # åˆ©ç”¨ï¼šé€‰æ‹©å½“å‰æœ€ä½³
            estimated_values = self.get_estimated_values()
            # æœªå°è¯•è¿‡çš„è‡‚ç»™äºˆè¾ƒé«˜çš„åˆå§‹ä¼°è®¡
            estimated_values[self.arm_counts == 0] = float('inf')
            return np.argmax(estimated_values)

class UCBAgent(BanditAgent):
    """
    Upper Confidence Bound (UCB) ç®—æ³•æ™ºèƒ½ä½“
    åŸºäºç½®ä¿¡åŒºé—´ä¸Šç•Œæ¥å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨
    """
    
    def __init__(self, n_arms: int, c: float = 2.0):
        super().__init__(n_arms)
        self.c = c  # ç½®ä¿¡åº¦å‚æ•°
        
    def select_arm(self) -> int:
        if self.time_step < self.n_arms:
            # åˆå§‹é˜¶æ®µï¼šæ¯ä¸ªè‡‚è‡³å°‘å°è¯•ä¸€æ¬¡
            return self.time_step
        
        # è®¡ç®—UCBå€¼
        ucb_values = np.zeros(self.n_arms)
        estimated_values = self.get_estimated_values()
        
        for arm in range(self.n_arms):
            if self.arm_counts[arm] == 0:
                ucb_values[arm] = float('inf')
            else:
                # UCBå…¬å¼: QÌ‚(a) + c * sqrt(ln(t) / N(a))
                confidence_bonus = self.c * math.sqrt(
                    math.log(self.time_step) / self.arm_counts[arm]
                )
                ucb_values[arm] = estimated_values[arm] + confidence_bonus
        
        return np.argmax(ucb_values)

class ThompsonSamplingAgent(BanditAgent):
    """
    Thompson Sampling ç®—æ³•æ™ºèƒ½ä½“
    åŸºäºè´å¶æ–¯æ¨æ–­çš„æ¦‚ç‡åŒ¹é…æ–¹æ³•
    """
    
    def __init__(self, n_arms: int, prior_alpha: float = 1.0, prior_beta: float = 1.0):
        super().__init__(n_arms)
        # Betaåˆ†å¸ƒçš„å…ˆéªŒå‚æ•°
        self.alpha = np.full(n_arms, prior_alpha)
        self.beta = np.full(n_arms, prior_beta)
        
    def select_arm(self) -> int:
        # ä»æ¯ä¸ªè‡‚çš„BetaåéªŒåˆ†å¸ƒä¸­é‡‡æ ·
        sampled_values = np.zeros(self.n_arms)
        for arm in range(self.n_arms):
            sampled_values[arm] = np.random.beta(self.alpha[arm], self.beta[arm])
        
        return np.argmax(sampled_values)
    
    def update(self, arm: int, reward: float):
        super().update(arm, reward)
        
        # æ›´æ–°Betaåˆ†å¸ƒå‚æ•°
        if reward > 0:
            self.alpha[arm] += 1
        else:
            self.beta[arm] += 1

def run_experiment(env: BanditEnvironment, 
                  agent: BanditAgent, 
                  n_steps: int = 1000,
                  verbose: bool = False) -> dict:
    """
    è¿è¡Œå¤šè‡‚è€è™æœºå®éªŒ
    
    Args:
        env: ç¯å¢ƒ
        agent: æ™ºèƒ½ä½“
        n_steps: å®éªŒæ­¥æ•°
        verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†ä¿¡æ¯
        
    Returns:
        å®éªŒç»“æœå­—å…¸
    """
    env.reset_stats()
    agent.reset()
    
    rewards_history = []
    regret_history = []
    arm_selection_history = []
    
    for step in range(n_steps):
        # æ™ºèƒ½ä½“é€‰æ‹©è‡‚
        selected_arm = agent.select_arm()
        
        # ç¯å¢ƒç»™å‡ºå¥–åŠ±
        reward = env.pull_arm(selected_arm)
        
        # æ™ºèƒ½ä½“æ›´æ–°
        agent.update(selected_arm, reward)
        
        # è®°å½•å†å²
        rewards_history.append(reward)
        regret_history.append(env.regret_history[-1])
        arm_selection_history.append(selected_arm)
        
        if verbose and (step + 1) % 100 == 0:
            print(f"æ­¥éª¤ {step + 1}: é€‰æ‹©è‡‚ {selected_arm}, å¥–åŠ± {reward:.3f}")
    
    results = {
        'total_reward': agent.total_reward,
        'cumulative_regret': env.get_cumulative_regret(),
        'optimal_arm_ratio': env.get_optimal_arm_ratio(),
        'rewards_history': rewards_history,
        'regret_history': regret_history,
        'arm_selection_history': arm_selection_history,
        'estimated_values': agent.get_estimated_values(),
        'arm_counts': agent.arm_counts.copy()
    }
    
    return results

def compare_algorithms():
    """
    æ¯”è¾ƒä¸åŒå¤šè‡‚è€è™æœºç®—æ³•çš„æ€§èƒ½
    """
    print("ğŸ”¬ å¤šè‡‚è€è™æœºç®—æ³•æ€§èƒ½æ¯”è¾ƒ")
    print("=" * 50)
    
    # åˆ›å»ºç¯å¢ƒ
    env = BanditEnvironment(n_arms=10, reward_type="gaussian")
    env.display_info()
    
    # åˆ›å»ºä¸åŒçš„æ™ºèƒ½ä½“
    agents = {
        "è´ªå¿ƒ": GreedyAgent(env.n_arms),
        "Îµ-è´ªå¿ƒ(0.1)": EpsilonGreedyAgent(env.n_arms, epsilon=0.1),
        "Îµ-è´ªå¿ƒè¡°å‡": EpsilonGreedyAgent(env.n_arms, epsilon=0.3, decay=True),
        "UCB": UCBAgent(env.n_arms, c=2.0),
        "Thompson Sampling": ThompsonSamplingAgent(env.n_arms)
    }
    
    n_steps = 2000
    n_runs = 10  # å¤šæ¬¡è¿è¡Œæ±‚å¹³å‡
    
    print(f"\nğŸ¯ å®éªŒè®¾ç½®: {n_steps} æ­¥, {n_runs} æ¬¡è¿è¡Œ")
    print("-" * 50)
    
    results = {}
    
    for agent_name, agent in agents.items():
        print(f"æ­£åœ¨æµ‹è¯• {agent_name}...")
        
        cumulative_regrets = []
        total_rewards = []
        optimal_ratios = []
        
        for run in range(n_runs):
            result = run_experiment(env, agent, n_steps)
            cumulative_regrets.append(result['cumulative_regret'])
            total_rewards.append(result['total_reward'])
            optimal_ratios.append(result['optimal_arm_ratio'])
        
        results[agent_name] = {
            'avg_cumulative_regret': np.mean(cumulative_regrets),
            'std_cumulative_regret': np.std(cumulative_regrets),
            'avg_total_reward': np.mean(total_rewards),
            'std_total_reward': np.std(total_rewards),
            'avg_optimal_ratio': np.mean(optimal_ratios),
            'std_optimal_ratio': np.std(optimal_ratios)
        }
    
    # æ˜¾ç¤ºç»“æœ
    print("\nğŸ“Š ç®—æ³•æ€§èƒ½æ¯”è¾ƒç»“æœ:")
    print("-" * 80)
    print(f"{'ç®—æ³•':<15} {'ç´¯ç§¯é—æ†¾':<15} {'æ€»å¥–åŠ±':<15} {'æœ€ä¼˜è‡‚æ¯”ä¾‹':<15}")
    print("-" * 80)
    
    for agent_name, result in results.items():
        regret_str = f"{result['avg_cumulative_regret']:.1f}Â±{result['std_cumulative_regret']:.1f}"
        reward_str = f"{result['avg_total_reward']:.1f}Â±{result['std_total_reward']:.1f}"
        ratio_str = f"{result['avg_optimal_ratio']:.3f}Â±{result['std_optimal_ratio']:.3f}"
        
        print(f"{agent_name:<15} {regret_str:<15} {reward_str:<15} {ratio_str:<15}")
    
    return results, env

def visualize_single_run():
    """
    å¯è§†åŒ–å•æ¬¡è¿è¡Œçš„è¯¦ç»†è¿‡ç¨‹
    """
    print("\nğŸ¨ å•æ¬¡è¿è¡Œå¯è§†åŒ–æ¼”ç¤º")
    print("=" * 30)
    
    # åˆ›å»ºç¯å¢ƒ
    env = BanditEnvironment(n_arms=5, reward_type="gaussian")
    
    # åˆ›å»ºæ™ºèƒ½ä½“
    agents = {
        "Îµ-è´ªå¿ƒ": EpsilonGreedyAgent(env.n_arms, epsilon=0.1),
        "UCB": UCBAgent(env.n_arms, c=2.0),
        "Thompson Sampling": ThompsonSamplingAgent(env.n_arms)
    }
    
    n_steps = 1000
    
    plt.figure(figsize=(15, 10))
    
    # å­å›¾1: ç´¯ç§¯é—æ†¾
    plt.subplot(2, 2, 1)
    for agent_name, agent in agents.items():
        result = run_experiment(env, agent, n_steps)
        cumulative_regret = np.cumsum(result['regret_history'])
        plt.plot(cumulative_regret, label=agent_name, alpha=0.8)
    
    plt.xlabel('æ—¶é—´æ­¥')
    plt.ylabel('ç´¯ç§¯é—æ†¾')
    plt.title('ç´¯ç§¯é—æ†¾å¯¹æ¯”')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # å­å›¾2: å¹³å‡å¥–åŠ±
    plt.subplot(2, 2, 2)
    window_size = 50
    
    for agent_name, agent in agents.items():
        result = run_experiment(env, agent, n_steps)
        rewards = result['rewards_history']
        # è®¡ç®—ç§»åŠ¨å¹³å‡
        moving_avg = []
        for i in range(len(rewards)):
            start_idx = max(0, i - window_size + 1)
            moving_avg.append(np.mean(rewards[start_idx:i+1]))
        
        plt.plot(moving_avg, label=agent_name, alpha=0.8)
    
    plt.xlabel('æ—¶é—´æ­¥')
    plt.ylabel('å¹³å‡å¥–åŠ±')
    plt.title(f'å¹³å‡å¥–åŠ±å¯¹æ¯” (çª—å£å¤§å°: {window_size})')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # å­å›¾3: è‡‚é€‰æ‹©åˆ†å¸ƒ
    plt.subplot(2, 2, 3)
    agent = UCBAgent(env.n_arms, c=2.0)
    result = run_experiment(env, agent, n_steps)
    
    plt.bar(range(env.n_arms), result['arm_counts'], alpha=0.7)
    plt.xlabel('è€è™æœºè‡‚')
    plt.ylabel('é€‰æ‹©æ¬¡æ•°')
    plt.title('UCBç®—æ³•çš„è‡‚é€‰æ‹©åˆ†å¸ƒ')
    
    # æ ‡è®°æœ€ä¼˜è‡‚
    plt.axvline(x=env.optimal_arm, color='red', linestyle='--', 
                label=f'æœ€ä¼˜è‡‚ {env.optimal_arm}')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # å­å›¾4: ä¼°è®¡å€¼ vs çœŸå®å€¼
    plt.subplot(2, 2, 4)
    true_values = env.true_means
    estimated_values = result['estimated_values']
    
    x = range(env.n_arms)
    plt.bar([i - 0.2 for i in x], true_values, width=0.4, 
            label='çœŸå®å‡å€¼', alpha=0.7)
    plt.bar([i + 0.2 for i in x], estimated_values, width=0.4, 
            label='ä¼°è®¡å‡å€¼', alpha=0.7)
    
    plt.xlabel('è€è™æœºè‡‚')
    plt.ylabel('å¥–åŠ±å‡å€¼')
    plt.title('çœŸå®å€¼ vs ä¼°è®¡å€¼æ¯”è¾ƒ')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('bandit_algorithms_comparison.png', dpi=150, bbox_inches='tight')
    plt.show()
    
    print("å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜ä¸º bandit_algorithms_comparison.png")

def interactive_demo():
    """
    äº¤äº’å¼æ¼”ç¤º
    """
    print("\nğŸ® äº¤äº’å¼å¤šè‡‚è€è™æœºæ¼”ç¤º")
    print("=" * 35)
    
    # åˆ›å»ºç®€å•çš„3è‡‚è€è™æœº
    env = BanditEnvironment(n_arms=3, reward_type="bernoulli")
    env.display_info()
    
    print("\nä½ å¯ä»¥é€‰æ‹©æ‹‰åŠ¨å“ªä¸ªè‡‚ (0, 1, 2)ï¼Œæˆ–è¾“å…¥ 'q' é€€å‡º")
    print("æˆ‘ä»¬æ¥çœ‹çœ‹ä½ èƒ½å¦æ‰¾åˆ°æœ€ä¼˜çš„è€è™æœºï¼")
    
    total_reward = 0
    total_pulls = 0
    
    while True:
        choice = input(f"\nç¬¬ {total_pulls + 1} æ¬¡é€‰æ‹© (0/1/2/q): ").strip().lower()
        
        if choice == 'q':
            break
        
        try:
            arm = int(choice)
            if arm not in [0, 1, 2]:
                print("è¯·è¾“å…¥ 0, 1, 2 ä¸­çš„ä¸€ä¸ªæ•°å­—")
                continue
        except ValueError:
            print("è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—æˆ– 'q'")
            continue
        
        # æ‹‰åŠ¨è€è™æœºè‡‚
        reward = env.pull_arm(arm)
        total_reward += reward
        total_pulls += 1
        
        if reward > 0:
            print(f"ğŸ‰ æ­å–œï¼ä½ è·å¾—äº†å¥–åŠ±ï¼")
        else:
            print(f"ğŸ˜ å¾ˆé—æ†¾ï¼Œè¿™æ¬¡æ²¡æœ‰å¥–åŠ±")
        
        print(f"å½“å‰æ€»å¥–åŠ±: {total_reward}, æ€»å°è¯•æ¬¡æ•°: {total_pulls}")
        print(f"å„è‡‚çš„é€‰æ‹©æ¬¡æ•°: {env.arm_counts}")
        
        if total_pulls >= 20:
            optimal_ratio = env.get_optimal_arm_ratio()
            print(f"\nğŸ“Š ä½ é€‰æ‹©æœ€ä¼˜è‡‚çš„æ¯”ä¾‹: {optimal_ratio:.2%}")
            if optimal_ratio > 0.6:
                print("ğŸ† è¡¨ç°å¾ˆå¥½ï¼ä½ å¾ˆå¿«æ‰¾åˆ°äº†æœ€ä¼˜ç­–ç•¥ï¼")
            elif optimal_ratio > 0.4:
                print("ğŸ‘ è¡¨ç°ä¸é”™ï¼Œç»§ç»­ä¿æŒï¼")
            else:
                print("ğŸ¤” è¿˜æœ‰æ”¹è¿›ç©ºé—´ï¼Œå¤šæ¢ç´¢ä¸€ä¸‹ä¸åŒçš„é€‰æ‹©ï¼")

def explain_bandit_theory():
    """
    è§£é‡Šå¤šè‡‚è€è™æœºç†è®º
    """
    theory = """
    ğŸ“š å¤šè‡‚è€è™æœºç†è®ºè¯¦è§£
    
    ğŸ¯ é—®é¢˜å®šä¹‰:
    - æœ‰ K ä¸ªè€è™æœºè‡‚ï¼Œæ¯ä¸ªè‡‚æœ‰æœªçŸ¥çš„å¥–åŠ±åˆ†å¸ƒ
    - æ¯æ¬¡åªèƒ½é€‰æ‹©ä¸€ä¸ªè‡‚ï¼Œè·å¾—ç›¸åº”çš„å¥–åŠ±
    - ç›®æ ‡ï¼šæœ€å¤§åŒ–é•¿æœŸç´¯ç§¯å¥–åŠ±
    
    ğŸ”„ æ ¸å¿ƒæŒ‘æˆ˜ - æ¢ç´¢ä¸åˆ©ç”¨æƒè¡¡:
    - æ¢ç´¢ (Exploration): å°è¯•æ–°çš„è‡‚æ¥è·å–ä¿¡æ¯
    - åˆ©ç”¨ (Exploitation): é€‰æ‹©å½“å‰è®¤ä¸ºæœ€å¥½çš„è‡‚
    - è¿‡åº¦æ¢ç´¢ï¼šæµªè´¹æ—¶é—´åœ¨æ¬¡ä¼˜è‡‚ä¸Š
    - è¿‡åº¦åˆ©ç”¨ï¼šå¯èƒ½é”™è¿‡çœŸæ­£çš„æœ€ä¼˜è‡‚
    
    ğŸ“Š è¯„ä¼°æŒ‡æ ‡:
    1. ç´¯ç§¯é—æ†¾ (Cumulative Regret):
       R(T) = Î£[Î¼* - Î¼(a_t)]
       å…¶ä¸­ Î¼* æ˜¯æœ€ä¼˜è‡‚çš„æœŸæœ›å¥–åŠ±
    
    2. ç®€å•é—æ†¾ (Simple Regret):
       æœ€ç»ˆæ¨èè‡‚ä¸æœ€ä¼˜è‡‚çš„æœŸæœ›å¥–åŠ±å·®
    
    ğŸ§® ä¸»è¦ç®—æ³•:
    
    1. Îµ-è´ªå¿ƒç®—æ³•:
       - ä»¥ Îµ æ¦‚ç‡éšæœºæ¢ç´¢
       - ä»¥ (1-Îµ) æ¦‚ç‡é€‰æ‹©å½“å‰æœ€ä½³
       - ç®€å•ä½†æœ‰æ•ˆ
    
    2. UCBç®—æ³•:
       - åŸºäºç½®ä¿¡åŒºé—´ä¸Šç•Œ
       - å…¬å¼: QÌ‚(a) + câˆš(ln(t)/N(a))
       - ç†è®ºä¿è¯: O(âˆš(K*ln(T)/T)) é—æ†¾ç•Œ
    
    3. Thompson Sampling:
       - è´å¶æ–¯æ–¹æ³•
       - ä»åéªŒåˆ†å¸ƒä¸­é‡‡æ ·
       - ä¼˜é›…åœ°å¹³è¡¡æ¢ç´¢ä¸åˆ©ç”¨
    
    ğŸ† ç†è®ºç»“æœ:
    - æœ€ä¼˜é—æ†¾ä¸‹ç•Œ: Î©(âˆš(K*T*ln(T)))
    - UCBå’ŒThompson Samplingéƒ½æ¥è¿‘æœ€ä¼˜
    - å¯¹äºç‰¹å®šé—®é¢˜ï¼Œå¯ä»¥è¾¾åˆ°é—®é¢˜ç›¸å…³çš„ç•Œ
    """
    print(theory)

def main():
    """
    å¤šè‡‚è€è™æœºæ•™ç¨‹ä¸»å‡½æ•°
    """
    print("ğŸ° å¤šè‡‚è€è™æœºç®—æ³•å®Œæ•´æ•™ç¨‹")
    print("=" * 50)
    
    # ç†è®ºè§£é‡Š
    explain_bandit_theory()
    
    # ç®—æ³•æ¯”è¾ƒ
    results, env = compare_algorithms()
    
    # å¯è§†åŒ–æ¼”ç¤º
    try:
        visualize_single_run()
    except Exception as e:
        print(f"å¯è§†åŒ–æ—¶å‡ºé”™: {e}")
        print("è¯·ç¡®ä¿å®‰è£…äº†matplotlib: pip install matplotlib")
    
    # äº¤äº’å¼æ¼”ç¤º
    choice = input("\næ˜¯å¦è¦è¿›è¡Œäº¤äº’å¼æ¼”ç¤ºï¼Ÿ(y/n): ").strip().lower()
    if choice == 'y':
        interactive_demo()
    
    print("\nğŸ“ å¤šè‡‚è€è™æœºå­¦ä¹ æ€»ç»“:")
    print("1. å¤šè‡‚è€è™æœºæ˜¯å¼ºåŒ–å­¦ä¹ çš„åŸºç¡€é—®é¢˜")
    print("2. æ ¸å¿ƒåœ¨äºæ¢ç´¢ä¸åˆ©ç”¨çš„æƒè¡¡")
    print("3. Îµ-è´ªå¿ƒç®€å•æœ‰æ•ˆï¼ŒUCBæœ‰ç†è®ºä¿è¯")
    print("4. Thompson Samplingä¼˜é›…åœ°å¤„ç†ä¸ç¡®å®šæ€§")
    print("5. ä¸ºæ›´å¤æ‚çš„å¼ºåŒ–å­¦ä¹ é—®é¢˜å¥ å®šåŸºç¡€")
    
    print("\nğŸ“ˆ è¿›é˜¶å­¦ä¹ å»ºè®®:")
    print("- ä¸Šä¸‹æ–‡è€è™æœº (Contextual Bandits)")
    print("- å¯¹æŠ—æ€§è€è™æœº (Adversarial Bandits)")
    print("- çº¿æ€§è€è™æœº (Linear Bandits)")
    print("- ç¥ç»ç½‘ç»œè€è™æœº (Neural Bandits)")

if __name__ == "__main__":
    main()