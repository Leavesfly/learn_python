# -*- coding: utf-8 -*-
"""
RAGä¸å‘é‡æ•°æ®æ¼”ç¤ºè„šæœ¬
"""

from 19_rag_vector_demo import RAGSystem, TFIDFVectorizer, VectorSimilarity

def quick_demo():
    """å¿«é€Ÿæ¼”ç¤ºRAGç³»ç»Ÿæ ¸å¿ƒåŠŸèƒ½"""
    print("ğŸ” RAGä¸å‘é‡æ•°æ®å¿«é€Ÿæ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºç¤ºä¾‹æ–‡æ¡£
    documents = [
        {
            "id": "python_basics",
            "content": "Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œè¯­æ³•ç®€æ´ï¼ŒåŠŸèƒ½å¼ºå¤§ï¼Œå¹¿æ³›ç”¨äºæ•°æ®ç§‘å­¦å’Œäººå·¥æ™ºèƒ½å¼€å‘ã€‚",
            "metadata": {"category": "ç¼–ç¨‹è¯­è¨€", "level": "åŸºç¡€"}
        },
        {
            "id": "ml_intro", 
            "content": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒåˆ†æ”¯ï¼Œé€šè¿‡ç®—æ³•è®©è®¡ç®—æœºä»æ•°æ®ä¸­è‡ªåŠ¨å­¦ä¹ æ¨¡å¼å’Œè§„å¾‹ã€‚",
            "metadata": {"category": "äººå·¥æ™ºèƒ½", "level": "ä¸­çº§"}
        },
        {
            "id": "deep_learning",
            "content": "æ·±åº¦å­¦ä¹ ä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¨¡æ‹Ÿäººè„‘å¤„ç†ä¿¡æ¯ï¼Œåœ¨å›¾åƒè¯†åˆ«å’Œè‡ªç„¶è¯­è¨€å¤„ç†ä¸­è¡¨ç°å‡ºè‰²ã€‚",
            "metadata": {"category": "äººå·¥æ™ºèƒ½", "level": "é«˜çº§"}
        },
        {
            "id": "data_science",
            "content": "æ•°æ®ç§‘å­¦ç»“åˆç»Ÿè®¡å­¦ã€è®¡ç®—æœºç§‘å­¦å’Œé¢†åŸŸçŸ¥è¯†ï¼Œä»æµ·é‡æ•°æ®ä¸­æå–æœ‰ä»·å€¼çš„æ´å¯Ÿå’ŒçŸ¥è¯†ã€‚",
            "metadata": {"category": "æ•°æ®ç§‘å­¦", "level": "ä¸­çº§"}
        }
    ]
    
    # åˆ›å»ºRAGç³»ç»Ÿ
    print("\nğŸ“š åˆ›å»ºRAGç³»ç»Ÿå¹¶æ·»åŠ æ–‡æ¡£...")
    rag = RAGSystem(vector_dim=64, similarity_threshold=0.05)
    rag.add_documents(documents)
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    stats = rag.get_statistics()
    print(f"\nğŸ“Š ç³»ç»Ÿç»Ÿè®¡: {stats}")
    
    # æµ‹è¯•æŸ¥è¯¢
    test_queries = [
        "Pythonç¼–ç¨‹è¯­è¨€ç‰¹ç‚¹",
        "æœºå™¨å­¦ä¹ ç®—æ³•åŸç†", 
        "ç¥ç»ç½‘ç»œæ·±åº¦å­¦ä¹ ",
        "æ•°æ®åˆ†æç§‘å­¦æ–¹æ³•"
    ]
    
    print("\nğŸ” æµ‹è¯•æŸ¥è¯¢ç»“æœ:")
    print("-" * 50)
    
    for query in test_queries:
        print(f"\næŸ¥è¯¢: '{query}'")
        results = rag.search(query, top_k=2)
        
        if results:
            for i, result in enumerate(results, 1):
                doc = result.document
                print(f"  {i}. [{doc.id}] ç›¸ä¼¼åº¦: {result.similarity:.4f}")
                print(f"     ç±»åˆ«: {doc.metadata.get('category', 'N/A')}")
                print(f"     å†…å®¹: {doc.content[:60]}...")
        else:
            print("  æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
    
    # æ¼”ç¤ºå‘é‡æ“ä½œ
    print(f"\nğŸ§® å‘é‡æ“ä½œæ¼”ç¤º:")
    print("-" * 30)
    
    # åˆ›å»ºç®€å•çš„å‘é‡åŒ–å™¨
    vectorizer = TFIDFVectorizer(max_features=20)
    texts = [doc["content"] for doc in documents]
    vectors = vectorizer.fit_transform(texts)
    
    print(f"æ–‡æ¡£æ•°é‡: {len(texts)}")
    print(f"å‘é‡ç»´åº¦: {len(vectors[0])}")
    print(f"è¯æ±‡è¡¨å¤§å°: {vectorizer.tokenizer.vocab_size}")
    
    # è®¡ç®—æ–‡æ¡£é—´ç›¸ä¼¼åº¦
    print(f"\nğŸ“ æ–‡æ¡£ç›¸ä¼¼åº¦çŸ©é˜µ:")
    for i in range(len(vectors)):
        for j in range(len(vectors)):
            if i <= j:
                sim = VectorSimilarity.cosine_similarity(vectors[i], vectors[j])
                doc_i = documents[i]["id"]
                doc_j = documents[j]["id"]
                print(f"  {doc_i} <-> {doc_j}: {sim:.3f}")
    
    print(f"\nâœ… æ¼”ç¤ºå®Œæˆï¼")

if __name__ == "__main__":
    quick_demo()