import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Tuple, List, Dict, Any
import math
import json
import time
from dataclasses import dataclass

@dataclass
class GPT3Config:
    """GPT-3 æ¨¡å‹é…ç½®"""
    vocab_size: int = 50257
    n_positions: int = 2048  # GPT-3 æ”¯æŒæ›´é•¿çš„ä¸Šä¸‹æ–‡
    n_embd: int = 12288      # GPT-3-175B çš„éšè—å±‚ç»´åº¦
    n_layer: int = 96        # GPT-3-175B çš„å±‚æ•°
    n_head: int = 96         # GPT-3-175B çš„æ³¨æ„åŠ›å¤´æ•°
    n_inner: Optional[int] = None  # 4 * n_embd
    activation_function: str = "gelu_new"
    resid_pdrop: float = 0.1
    embd_pdrop: float = 0.1
    attn_pdrop: float = 0.1
    layer_norm_epsilon: float = 1e-5
    initializer_range: float = 0.02
    use_cache: bool = True
    # GPT-3 ç‰¹æœ‰é…ç½®
    sparse_attention: bool = False  # ç¨€ç–æ³¨æ„åŠ›ï¼ˆç”¨äºè¶…å¤§æ¨¡å‹ï¼‰
    gradient_checkpointing: bool = True  # æ¢¯åº¦æ£€æŸ¥ç‚¹ä»¥èŠ‚çœå†…å­˜
    parallel_attention: bool = True  # å¹¶è¡Œæ³¨æ„åŠ›å’ŒMLPè®¡ç®—
    rotary_pct: float = 0.25  # æ—‹è½¬ä½ç½®ç¼–ç æ¯”ä¾‹
    
    def __post_init__(self):
        if self.n_inner is None:
            self.n_inner = 4 * self.n_embd

class GPT3RotaryEmbedding(nn.Module):
    """æ—‹è½¬ä½ç½®ç¼–ç  (RoPE) - GPT-3 çš„æ”¹è¿›ä¹‹ä¸€"""
    
    def __init__(self, dim: int, max_seq_len: int = 2048, base: float = 10000.0):
        super().__init__()
        self.dim = dim
        self.max_seq_len = max_seq_len
        self.base = base
        
        # é¢„è®¡ç®—é¢‘ç‡
        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
        self.register_buffer('inv_freq', inv_freq)
        
    def forward(self, seq_len: int, device: torch.device):
        """ç”Ÿæˆæ—‹è½¬ä½ç½®ç¼–ç """
        t = torch.arange(seq_len, device=device, dtype=self.inv_freq.dtype)
        freqs = torch.einsum('i,j->ij', t, self.inv_freq)
        emb = torch.cat((freqs, freqs), dim=-1)
        return emb.cos(), emb.sin()

def rotate_half(x):
    """å°†å¼ é‡çš„ååŠéƒ¨åˆ†ç§»åˆ°å‰é¢å¹¶å–è´Ÿå·"""
    x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]
    return torch.cat((-x2, x1), dim=-1)

def apply_rotary_pos_emb(q, k, cos, sin):
    """åº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç """
    cos = cos.unsqueeze(0).unsqueeze(0)  # (1, 1, seq_len, dim)
    sin = sin.unsqueeze(0).unsqueeze(0)
    
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed

class GPT3SparseAttention(nn.Module):
    """GPT-3 ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ï¼ˆæ¨¡æ‹Ÿï¼‰"""
    
    def __init__(self, config: GPT3Config, layer_idx: int = 0):
        super().__init__()
        self.config = config
        self.layer_idx = layer_idx
        
        max_positions = config.n_positions
        self.register_buffer(
            "bias",
            torch.tril(torch.ones((max_positions, max_positions), dtype=torch.bool)).view(
                1, 1, max_positions, max_positions
            ),
            persistent=False,
        )
        
        self.n_head = config.n_head
        self.n_embd = config.n_embd
        self.head_dim = self.n_embd // self.n_head
        
        if self.head_dim * self.n_head != self.n_embd:
            raise ValueError(f"n_embd ({self.n_embd}) å¿…é¡»èƒ½è¢« n_head ({self.n_head}) æ•´é™¤")
        
        # æŸ¥è¯¢ã€é”®ã€å€¼æŠ•å½±
        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=True)
        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=True)
        
        self.attn_dropout = nn.Dropout(config.attn_pdrop)
        self.resid_dropout = nn.Dropout(config.resid_pdrop)
        
        # æ—‹è½¬ä½ç½®ç¼–ç 
        rotary_dim = int(self.head_dim * config.rotary_pct)
        self.rotary_emb = GPT3RotaryEmbedding(rotary_dim, config.n_positions)
        self.rotary_dim = rotary_dim
        
    def _split_heads(self, tensor, num_heads, attn_head_size):
        """å°†éšè—çŠ¶æ€åˆ†å‰²ä¸ºå¤šä¸ªæ³¨æ„åŠ›å¤´"""
        new_shape = tensor.size()[:-1] + (num_heads, attn_head_size)
        tensor = tensor.view(new_shape)
        return tensor.permute(0, 2, 1, 3)  # (batch, head, seq_length, head_features)
    
    def _merge_heads(self, tensor, num_heads, attn_head_size):
        """åˆå¹¶å¤šä¸ªæ³¨æ„åŠ›å¤´"""
        tensor = tensor.permute(0, 2, 1, 3).contiguous()
        new_shape = tensor.size()[:-2] + (num_heads * attn_head_size,)
        return tensor.view(new_shape)
    
    def _sparse_attn_mask(self, seq_len: int):
        """åˆ›å»ºç¨€ç–æ³¨æ„åŠ›æ©ç ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        if not self.config.sparse_attention:
            return None
            
        # ç®€å•çš„å¸¦çŠ¶ç¨€ç–æ³¨æ„åŠ›æ¨¡å¼
        mask = torch.ones(seq_len, seq_len, dtype=torch.bool)
        band_width = min(64, seq_len // 4)  # å±€éƒ¨æ³¨æ„åŠ›å¸¦å®½
        
        # åªä¿ç•™å¯¹è§’çº¿é™„è¿‘çš„æ³¨æ„åŠ›
        for i in range(seq_len):
            start = max(0, i - band_width)
            end = min(seq_len, i + band_width + 1)
            mask[i, :start] = False
            mask[i, end:] = False
            
        return mask
    
    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        past_key_value: Optional[Tuple[torch.Tensor]] = None,
        use_cache: bool = False,
    ):
        batch_size, seq_len = hidden_states.shape[:2]
        
        # è®¡ç®—æŸ¥è¯¢ã€é”®ã€å€¼
        qkv = self.c_attn(hidden_states)
        q, k, v = qkv.split(self.n_embd, dim=2)
        
        # åˆ†å‰²æ³¨æ„åŠ›å¤´
        q = self._split_heads(q, self.n_head, self.head_dim)
        k = self._split_heads(k, self.n_head, self.head_dim)
        v = self._split_heads(v, self.n_head, self.head_dim)
        
        # åº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç 
        if self.rotary_dim > 0:
            cos, sin = self.rotary_emb(seq_len, hidden_states.device)
            q_rot = q[..., :self.rotary_dim]
            q_pass = q[..., self.rotary_dim:]
            k_rot = k[..., :self.rotary_dim]
            k_pass = k[..., self.rotary_dim:]
            
            q_rot, k_rot = apply_rotary_pos_emb(q_rot, k_rot, cos, sin)
            q = torch.cat([q_rot, q_pass], dim=-1)
            k = torch.cat([k_rot, k_pass], dim=-1)
        
        # ä½¿ç”¨ç¼“å­˜
        if past_key_value is not None:
            past_key, past_value = past_key_value
            k = torch.cat([past_key, k], dim=-2)
            v = torch.cat([past_value, v], dim=-2)
        
        if use_cache:
            present = (k, v)
        else:
            present = None
            
        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
        attn_weights = torch.matmul(q, k.transpose(-1, -2))
        attn_weights = attn_weights / math.sqrt(self.head_dim)
        
        # åº”ç”¨å› æœæ©ç 
        causal_mask = self.bias[:, :, :seq_len, :k.size(-2)]
        attn_weights = torch.where(causal_mask, attn_weights, torch.finfo(attn_weights.dtype).min)
        
        # åº”ç”¨ç¨€ç–æ³¨æ„åŠ›æ©ç 
        sparse_mask = self._sparse_attn_mask(k.size(-2))
        if sparse_mask is not None:
            sparse_mask = sparse_mask.to(attn_weights.device)
            attn_weights = torch.where(sparse_mask, attn_weights, torch.finfo(attn_weights.dtype).min)
        
        # åº”ç”¨attention mask
        if attention_mask is not None:
            attn_weights = attn_weights + attention_mask
        
        # Softmaxå’Œdropout
        attn_weights = F.softmax(attn_weights, dim=-1)
        attn_weights = self.attn_dropout(attn_weights)
        
        # åº”ç”¨æ³¨æ„åŠ›æƒé‡
        attn_output = torch.matmul(attn_weights, v)
        attn_output = self._merge_heads(attn_output, self.n_head, self.head_dim)
        
        # è¾“å‡ºæŠ•å½±
        attn_output = self.c_proj(attn_output)
        attn_output = self.resid_dropout(attn_output)
        
        return attn_output, present

class GPT3MLP(nn.Module):
    """GPT-3 å‰é¦ˆç½‘ç»œ"""
    
    def __init__(self, config: GPT3Config):
        super().__init__()
        self.c_fc = nn.Linear(config.n_embd, config.n_inner)
        self.c_proj = nn.Linear(config.n_inner, config.n_embd)
        self.dropout = nn.Dropout(config.resid_pdrop)
        
        # æ¿€æ´»å‡½æ•°
        if config.activation_function == "gelu_new":
            self.act = lambda x: 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))
        elif config.activation_function == "gelu":
            self.act = F.gelu
        elif config.activation_function == "relu":
            self.act = F.relu
        elif config.activation_function == "swish":
            self.act = lambda x: x * torch.sigmoid(x)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¿€æ´»å‡½æ•°: {config.activation_function}")
    
    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        hidden_states = self.c_fc(hidden_states)
        hidden_states = self.act(hidden_states)
        hidden_states = self.c_proj(hidden_states)
        hidden_states = self.dropout(hidden_states)
        return hidden_states

class GPT3Block(nn.Module):
    """GPT-3 Transformerå—ï¼ˆæ”¯æŒå¹¶è¡Œè®¡ç®—ï¼‰"""
    
    def __init__(self, config: GPT3Config, layer_idx: int):
        super().__init__()
        self.config = config
        self.layer_idx = layer_idx
        
        # Pre-LayerNorm æ¶æ„
        self.ln_1 = nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)
        self.attn = GPT3SparseAttention(config, layer_idx)
        self.ln_2 = nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)
        self.mlp = GPT3MLP(config)
        
        # å¹¶è¡Œæ³¨æ„åŠ›å’ŒMLPï¼ˆGPT-3çš„ä¼˜åŒ–ï¼‰
        self.parallel_attention = config.parallel_attention
    
    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        past_key_value: Optional[Tuple[torch.Tensor]] = None,
        use_cache: bool = False,
    ):
        if self.parallel_attention:
            # å¹¶è¡Œè®¡ç®—æ³¨æ„åŠ›å’ŒMLP
            ln_1_output = self.ln_1(hidden_states)
            attn_output, present = self.attn(
                ln_1_output,
                attention_mask=attention_mask,
                past_key_value=past_key_value,
                use_cache=use_cache,
            )
            
            ln_2_output = self.ln_2(hidden_states)
            mlp_output = self.mlp(ln_2_output)
            
            # æ®‹å·®è¿æ¥
            hidden_states = hidden_states + attn_output + mlp_output
        else:
            # ä¸²è¡Œè®¡ç®—ï¼ˆä¼ ç»Ÿæ–¹å¼ï¼‰
            residual = hidden_states
            hidden_states = self.ln_1(hidden_states)
            attn_output, present = self.attn(
                hidden_states,
                attention_mask=attention_mask,
                past_key_value=past_key_value,
                use_cache=use_cache,
            )
            hidden_states = residual + attn_output
            
            residual = hidden_states
            hidden_states = self.ln_2(hidden_states)
            mlp_output = self.mlp(hidden_states)
            hidden_states = residual + mlp_output
        
        return hidden_states, present

class GPT3Model(nn.Module):
    """GPT-3æ¨¡å‹ä¸»ä½“"""
    
    def __init__(self, config: GPT3Config):
        super().__init__()
        self.config = config
        
        # è¯åµŒå…¥å’Œä½ç½®åµŒå…¥
        self.wte = nn.Embedding(config.vocab_size, config.n_embd)
        self.wpe = nn.Embedding(config.n_positions, config.n_embd)
        
        # åµŒå…¥dropout
        self.drop = nn.Dropout(config.embd_pdrop)
        
        # Transformerå±‚
        self.h = nn.ModuleList([GPT3Block(config, layer_idx=i) for i in range(config.n_layer)])
        
        # æœ€ç»ˆå±‚å½’ä¸€åŒ–
        self.ln_f = nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)
        
        # æ¨¡å‹å¹¶è¡Œè®¾ç½®
        self.model_parallel = False
        self.device_map = None
        self.gradient_checkpointing = config.gradient_checkpointing
        
        # åˆå§‹åŒ–æƒé‡
        self.post_init()
        
        print(f"GPT-3æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼Œå‚æ•°é‡: {self.get_num_params():,}")
    
    def get_num_params(self, non_embedding: bool = True) -> int:
        """è®¡ç®—æ¨¡å‹å‚æ•°æ•°é‡"""
        n_params = sum(p.numel() for p in self.parameters())
        if non_embedding:
            n_params -= self.wpe.weight.numel()
            n_params -= self.wte.weight.numel()
        return n_params
    
    def post_init(self):
        """åˆå§‹åŒ–æƒé‡"""
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        """åˆå§‹åŒ–æ¨¡å‹æƒé‡"""
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)
        elif isinstance(module, nn.LayerNorm):
            torch.nn.init.zeros_(module.bias)
            torch.nn.init.ones_(module.weight)
    
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,
        attention_mask: Optional[torch.FloatTensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
    ) -> Dict[str, torch.Tensor]:
        
        use_cache = use_cache if use_cache is not None else self.config.use_cache
        
        batch_size, seq_length = input_ids.shape
        
        if position_ids is None:
            device = input_ids.device
            past_length = 0 if past_key_values is None else past_key_values[0][0].size(-2)
            position_ids = torch.arange(past_length, seq_length + past_length, dtype=torch.long, device=device)
            position_ids = position_ids.unsqueeze(0).expand(batch_size, -1)
        
        # åµŒå…¥
        inputs_embeds = self.wte(input_ids)
        position_embeds = self.wpe(position_ids)
        hidden_states = inputs_embeds + position_embeds
        hidden_states = self.drop(hidden_states)
        
        # æ³¨æ„åŠ›æ©ç 
        if attention_mask is not None:
            attention_mask = attention_mask.view(batch_size, -1)
            attention_mask = attention_mask[:, None, None, :]
            attention_mask = attention_mask.to(dtype=hidden_states.dtype)
            attention_mask = (1.0 - attention_mask) * torch.finfo(hidden_states.dtype).min
        
        # Transformerå±‚
        presents = () if use_cache else None
        for i, (block, past_key_value) in enumerate(zip(self.h, past_key_values or [None] * len(self.h))):
            if self.gradient_checkpointing and self.training:
                # ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹èŠ‚çœå†…å­˜
                def create_custom_forward(module):
                    def custom_forward(*inputs):
                        return module(*inputs, use_cache=use_cache)
                    return custom_forward
                
                layer_outputs = torch.utils.checkpoint.checkpoint(
                    create_custom_forward(block),
                    hidden_states,
                    attention_mask,
                    past_key_value,
                )
            else:
                layer_outputs = block(
                    hidden_states,
                    attention_mask=attention_mask,
                    past_key_value=past_key_value,
                    use_cache=use_cache,
                )
            
            hidden_states = layer_outputs[0]
            
            if use_cache:
                presents = presents + (layer_outputs[1],)
        
        # æœ€ç»ˆå½’ä¸€åŒ–
        hidden_states = self.ln_f(hidden_states)
        
        return {
            "last_hidden_state": hidden_states,
            "past_key_values": presents,
        }

class GPT3LMHeadModel(nn.Module):
    """å¸¦æœ‰è¯­è¨€æ¨¡å‹å¤´çš„GPT-3æ¨¡å‹"""
    
    def __init__(self, config: GPT3Config):
        super().__init__()
        self.config = config
        self.transformer = GPT3Model(config)
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
        
        # æƒé‡å…±äº«ï¼ˆå¯é€‰ï¼‰
        # self.lm_head.weight = self.transformer.wte.weight
        
        self.model_parallel = False
        self.device_map = None
        
        self.post_init()
    
    def post_init(self):
        """åˆå§‹åŒ–æƒé‡å¹¶åº”ç”¨æœ€ç»ˆå¤„ç†"""
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        """åˆå§‹åŒ–æ¨¡å‹æƒé‡"""
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)
        elif isinstance(module, nn.LayerNorm):
            torch.nn.init.zeros_(module.bias)
            torch.nn.init.ones_(module.weight)
    
    def get_output_embeddings(self):
        return self.lm_head
    
    def set_output_embeddings(self, new_embeddings):
        self.lm_head = new_embeddings
    
    def prepare_inputs_for_generation(self, input_ids, past_key_values=None, **kwargs):
        """ä¸ºç”Ÿæˆå‡†å¤‡è¾“å…¥"""
        attention_mask = kwargs.get("attention_mask", None)
        position_ids = kwargs.get("position_ids", None)
        
        # åªä¼ é€’æœ€åä¸€ä¸ªtokenï¼ˆå¦‚æœå·²ç»æœ‰past_key_valuesï¼‰
        if past_key_values:
            input_ids = input_ids[:, -1:]
            if attention_mask is not None:
                attention_mask = attention_mask[:, -1:]
        
        if attention_mask is not None and position_ids is None:
            position_ids = attention_mask.long().cumsum(-1) - 1
            position_ids.masked_fill_(attention_mask == 0, 1)
            if past_key_values:
                position_ids = position_ids[:, -1:]
        
        return {
            "input_ids": input_ids,
            "past_key_values": past_key_values,
            "use_cache": kwargs.get("use_cache", True),
            "position_ids": position_ids,
            "attention_mask": attention_mask,
        }
    
    def forward(
        self,
        input_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,
        attention_mask: Optional[torch.FloatTensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        labels: Optional[torch.LongTensor] = None,
        use_cache: Optional[bool] = None,
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[Tuple[torch.Tensor]]]]:
        
        transformer_outputs = self.transformer(
            input_ids=input_ids,
            past_key_values=past_key_values,
            attention_mask=attention_mask,
            position_ids=position_ids,
            use_cache=use_cache,
        )
        
        hidden_states = transformer_outputs["last_hidden_state"]
        logits = self.lm_head(hidden_states)
        
        loss = None
        if labels is not None:
            # ä¸‹ä¸€ä¸ªtokené¢„æµ‹çš„ç§»ä½æŸå¤±
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()
            loss = F.cross_entropy(
                shift_logits.view(-1, shift_logits.size(-1)),
                shift_labels.view(-1),
                ignore_index=-100,
            )
        
        return logits, loss, transformer_outputs["past_key_values"]

class GPT3InContextLearning:
    """GPT-3 ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆFew-shot Learningï¼‰"""
    
    def __init__(self, model: GPT3LMHeadModel, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self.model.eval()
    
    def few_shot_prompt(
        self, 
        task_description: str,
        examples: List[Tuple[str, str]],
        query: str,
        max_examples: int = 5
    ) -> str:
        """æ„å»ºfew-shotå­¦ä¹ æç¤º"""
        prompt_parts = [task_description, "\n\n"]
        
        # æ·»åŠ ç¤ºä¾‹
        for i, (input_text, output_text) in enumerate(examples[:max_examples]):
            prompt_parts.append(f"ç¤ºä¾‹ {i+1}:\n")
            prompt_parts.append(f"è¾“å…¥: {input_text}\n")
            prompt_parts.append(f"è¾“å‡º: {output_text}\n\n")
        
        # æ·»åŠ æŸ¥è¯¢
        prompt_parts.append(f"æŸ¥è¯¢:\nè¾“å…¥: {query}\nè¾“å‡º:")
        
        return "".join(prompt_parts)
    
    @torch.no_grad()
    def generate_with_context(
        self,
        prompt: str,
        max_new_tokens: int = 100,
        temperature: float = 0.7,
        top_k: int = 50,
        top_p: float = 0.9,
        do_sample: bool = True,
    ) -> str:
        """åŸºäºä¸Šä¸‹æ–‡çš„æ–‡æœ¬ç”Ÿæˆ"""
        input_ids = torch.tensor([self.tokenizer.encode(prompt, add_special_tokens=True)], dtype=torch.long)
        attention_mask = torch.ones_like(input_ids)
        
        past_key_values = None
        generated = input_ids.clone()
        
        for _ in range(max_new_tokens):
            model_inputs = self.model.prepare_inputs_for_generation(
                input_ids=generated,
                past_key_values=past_key_values,
                attention_mask=attention_mask,
                use_cache=True,
            )
            
            logits, _, past_key_values = self.model(**model_inputs)
            next_logits = logits[:, -1, :] / max(temperature, 1e-6)
            
            # Top-kå’ŒTop-pé‡‡æ ·
            if top_k > 0:
                top_k_vals, top_k_idx = torch.topk(next_logits, k=min(top_k, next_logits.size(-1)))
                filtered = torch.full_like(next_logits, float('-inf'))
                filtered.scatter_(dim=-1, index=top_k_idx, src=top_k_vals)
                next_logits = filtered
            
            if 0.0 < top_p < 1.0:
                sorted_logits, sorted_indices = torch.sort(next_logits, descending=True)
                probs = F.softmax(sorted_logits, dim=-1)
                cumprobs = torch.cumsum(probs, dim=-1)
                cutoff = cumprobs > top_p
                cutoff[..., 1:] = cutoff[..., :-1].clone()
                cutoff[..., 0] = False
                sorted_logits[cutoff] = float('-inf')
                next_logits = torch.full_like(next_logits, float('-inf'))
                next_logits.scatter_(dim=-1, index=sorted_indices, src=sorted_logits)
            
            if do_sample:
                probs = F.softmax(next_logits, dim=-1)
                next_id = torch.multinomial(probs, num_samples=1)
            else:
                next_id = torch.argmax(next_logits, dim=-1, keepdim=True)
            
            generated = torch.cat([generated, next_id], dim=1)
            attention_mask = torch.ones_like(generated)
            
            # EOSç»ˆæ­¢
            if next_id.item() == self.tokenizer.eos_token_id:
                break
        
        return self.tokenizer.decode(generated[0].tolist())

# é‡ç”¨GPT-2çš„ç®€å•åˆ†è¯å™¨
class GPT3SimpleTokenizer:
    """ç®€å•å­—ç¬¦çº§åˆ†è¯å™¨ï¼ˆç”¨äºæ¼”ç¤ºï¼‰"""
    def __init__(self):
        self.vocab = {}
        self.inverse_vocab = {}
        self.pad_token = "<PAD>"
        self.unk_token = "<UNK>"
        self.bos_token = "<BOS>"
        self.eos_token = "<EOS>"
        for tok in [self.pad_token, self.unk_token, self.bos_token, self.eos_token]:
            idx = len(self.vocab)
            self.vocab[tok] = idx
            self.inverse_vocab[idx] = tok

    def train(self, texts: List[str]):
        chars = set()
        for t in texts:
            for ch in t:
                chars.add(ch)
        for ch in sorted(chars):
            if ch not in self.vocab:
                idx = len(self.vocab)
                self.vocab[ch] = idx
                self.inverse_vocab[idx] = ch
        return self

    def encode(self, text: str, add_special_tokens: bool = True) -> List[int]:
        ids = []
        if add_special_tokens:
            ids.append(self.vocab[self.bos_token])
        for ch in text:
            ids.append(self.vocab.get(ch, self.vocab[self.unk_token]))
        if add_special_tokens:
            ids.append(self.vocab[self.eos_token])
        return ids

     def decode(self, ids: List[int]) -> str:
        out = []
        for i in ids:
            tok = self.inverse_vocab.get(i, self.unk_token)
            if tok not in [self.pad_token, self.bos_token, self.eos_token]:
                out.append(tok)
        return ''.join(out)

    @property
    def vocab_size(self):
        return len(self.vocab)

    @property
    def pad_token_id(self):
        return self.vocab[self.pad_token]

    @property
    def unk_token_id(self):
        return self.vocab[self.unk_token]

    @property
    def bos_token_id(self):
        return self.vocab[self.bos_token]

    @property
    def eos_token_id(self):
        return self.vocab[self.eos_token]

def create_small_gpt3_config():
    """åˆ›å»ºé€‚åˆæ¼”ç¤ºçš„å°å‹GPT-3é…ç½®"""
    return GPT3Config(
        vocab_size=1000,      # å°è¯æ±‡è¡¨
        n_positions=512,      # è¾ƒçŸ­çš„ä¸Šä¸‹æ–‡
        n_embd=256,          # è¾ƒå°çš„åµŒå…¥ç»´åº¦
        n_layer=6,           # è¾ƒå°‘çš„å±‚æ•°
        n_head=8,            # è¾ƒå°‘çš„æ³¨æ„åŠ›å¤´
        n_inner=1024,        # è¾ƒå°çš„MLPç»´åº¦
        sparse_attention=False,
        gradient_checkpointing=False,
        parallel_attention=True,
        rotary_pct=0.25,
    )

def gpt3_training_demo():
    """GPT-3è®­ç»ƒæ¼”ç¤º"""
    print("=== GPT-3 è®­ç»ƒæ¼”ç¤º ===\n")
    
    # åˆ›å»ºå°å‹é…ç½®ç”¨äºæ¼”ç¤º
    config = create_small_gpt3_config()
    
    # å‡†å¤‡è®­ç»ƒæ•°æ®
    texts = [
        "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯",
        "æœºå™¨å­¦ä¹ æ˜¯å®ç°äººå·¥æ™ºèƒ½çš„é‡è¦æ–¹æ³•",
        "æ·±åº¦å­¦ä¹ åŸºäºç¥ç»ç½‘ç»œæ¨¡å‹",
        "Transformeræ˜¯ç°ä»£è‡ªç„¶è¯­è¨€å¤„ç†çš„åŸºç¡€",
        "GPTæ˜¯åŸºäºTransformerçš„ç”Ÿæˆå¼é¢„è®­ç»ƒæ¨¡å‹",
        "æ³¨æ„åŠ›æœºåˆ¶è®©æ¨¡å‹èƒ½å¤Ÿå…³æ³¨é‡è¦ä¿¡æ¯",
        "è‡ªç„¶è¯­è¨€å¤„ç†åŒ…æ‹¬æ–‡æœ¬ç†è§£å’Œç”Ÿæˆ",
        "é¢„è®­ç»ƒæ¨¡å‹åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚"
    ]
    
    # è®­ç»ƒåˆ†è¯å™¨
    tokenizer = GPT3SimpleTokenizer()
    tokenizer.train(texts)
    print(f"è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")
    
    # æ›´æ–°é…ç½®ä¸­çš„è¯æ±‡è¡¨å¤§å°
    config.vocab_size = tokenizer.vocab_size
    
    # åˆ›å»ºæ¨¡å‹
    model = GPT3LMHeadModel(config)
    print(f"æ¨¡å‹å‚æ•°é‡: {model.transformer.get_num_params():,}")
    
    # å‡†å¤‡è®­ç»ƒæ•°æ®
    train_data = []
    for text in texts:
        tokens = tokenizer.encode(text)
        if len(tokens) <= config.n_positions:
            train_data.append(tokens)
    
    # ç®€å•çš„è®­ç»ƒå¾ªç¯
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
    model.train()
    
    print("\nå¼€å§‹è®­ç»ƒ...")
    for epoch in range(10):
        total_loss = 0
        for tokens in train_data:
            input_ids = torch.tensor([tokens[:-1]], dtype=torch.long)
            labels = torch.tensor([tokens[1:]], dtype=torch.long)
            
            optimizer.zero_grad()
            logits, loss, _ = model(input_ids=input_ids, labels=labels)
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
        
        avg_loss = total_loss / len(train_data)
        if epoch % 2 == 0:
            print(f"Epoch {epoch + 1}: å¹³å‡æŸå¤± = {avg_loss:.4f}")
    
    print("è®­ç»ƒå®Œæˆ!\n")
    return model, tokenizer

def gpt3_inference_demo(model, tokenizer):
    """GPT-3æ¨ç†æ¼”ç¤º"""
    print("=== GPT-3 æ¨ç†æ¼”ç¤º ===\n")
    
    model.eval()
    
    # ç®€å•æ–‡æœ¬ç”Ÿæˆ
    def generate_text(prompt, max_length=50):
        input_ids = torch.tensor([tokenizer.encode(prompt, add_special_tokens=True)], dtype=torch.long)
        
        with torch.no_grad():
            for _ in range(max_length):
                logits, _, _ = model(input_ids=input_ids)
                next_logits = logits[0, -1, :]
                
                # ä½¿ç”¨æ¸©åº¦é‡‡æ ·
                next_logits = next_logits / 0.8
                probs = F.softmax(next_logits, dim=-1)
                next_id = torch.multinomial(probs, num_samples=1)
                
                input_ids = torch.cat([input_ids, next_id.unsqueeze(0)], dim=1)
                
                if next_id.item() == tokenizer.eos_token_id:
                    break
        
        return tokenizer.decode(input_ids[0].tolist())
    
    # æµ‹è¯•ç”Ÿæˆ
    prompts = [
        "äººå·¥æ™ºèƒ½",
        "æœºå™¨å­¦ä¹ ",
        "æ·±åº¦å­¦ä¹ ",
        "è‡ªç„¶è¯­è¨€"
    ]
    
    for prompt in prompts:
        generated = generate_text(prompt, max_length=30)
        print(f"è¾“å…¥: {prompt}")
        print(f"ç”Ÿæˆ: {generated}")
        print("-" * 50)

def gpt3_few_shot_demo():
    """GPT-3 Few-shotå­¦ä¹ æ¼”ç¤º"""
    print("=== GPT-3 Few-shotå­¦ä¹ æ¼”ç¤º ===\n")
    
    # åˆ›å»ºè¾ƒå¤§çš„é…ç½®ç”¨äºfew-shotå­¦ä¹ 
    config = GPT3Config(
        vocab_size=2000,
        n_positions=1024,
        n_embd=512,
        n_layer=8,
        n_head=8,
        n_inner=2048,
        sparse_attention=False,
        gradient_checkpointing=False,
        parallel_attention=True,
        rotary_pct=0.25,
    )
    
    # åˆ›å»ºæ‰©å±•çš„è®­ç»ƒæ•°æ®é›†
    extended_texts = [
        "è¿™æ˜¯ä¸€ä¸ªç§¯æçš„è¯„è®ºï¼šè¿™éƒ¨ç”µå½±çœŸçš„å¾ˆæ£’ï¼",
        "è¿™æ˜¯ä¸€ä¸ªæ¶ˆæçš„è¯„è®ºï¼šè¿™éƒ¨ç”µå½±å¤ªç³Ÿç³•äº†ã€‚",
        "è¿™æ˜¯ä¸€ä¸ªä¸­æ€§çš„è¯„è®ºï¼šè¿™éƒ¨ç”µå½±è¿˜å¯ä»¥ã€‚",
        "è¿™æ˜¯ä¸€ä¸ªç§¯æçš„è¯„è®ºï¼šæˆ‘éå¸¸å–œæ¬¢è¿™æœ¬ä¹¦ã€‚",
        "è¿™æ˜¯ä¸€ä¸ªæ¶ˆæçš„è¯„è®ºï¼šè¿™æœ¬ä¹¦è®©æˆ‘å¤±æœ›ã€‚",
        "ç¿»è¯‘æˆè‹±æ–‡ï¼šä½ å¥½ -> Hello",
        "ç¿»è¯‘æˆè‹±æ–‡ï¼šè°¢è°¢ -> Thank you",
        "ç¿»è¯‘æˆè‹±æ–‡ï¼šå†è§ -> Goodbye",
        "é—®ç­”ï¼šåŒ—äº¬æ˜¯ä¸­å›½çš„é¦–éƒ½å—ï¼Ÿæ˜¯çš„ã€‚",
        "é—®ç­”ï¼šåœ°çƒæ˜¯å¹³çš„å—ï¼Ÿä¸æ˜¯çš„ã€‚",
        "åˆ†ç±»ï¼šè‹¹æœæ˜¯æ°´æœã€‚æ ‡ç­¾ï¼šæ°´æœ",
        "åˆ†ç±»ï¼šæ±½è½¦æ˜¯äº¤é€šå·¥å…·ã€‚æ ‡ç­¾ï¼šäº¤é€šå·¥å…·",
    ]
    
    # è®­ç»ƒæ‰©å±•çš„åˆ†è¯å™¨
    tokenizer = GPT3SimpleTokenizer()
    tokenizer.train(extended_texts)
    config.vocab_size = tokenizer.vocab_size
    
    # åˆ›å»ºæ¨¡å‹
    model = GPT3LMHeadModel(config)
    print(f"Few-shotæ¨¡å‹å‚æ•°é‡: {model.transformer.get_num_params():,}")
    
    # åˆ›å»ºä¸Šä¸‹æ–‡å­¦ä¹ å™¨
    icl = GPT3InContextLearning(model, tokenizer)
    
    # æƒ…æ„Ÿåˆ†æç¤ºä¾‹
    sentiment_examples = [
        ("è¿™éƒ¨ç”µå½±å¾ˆå¥½çœ‹", "ç§¯æ"),
        ("æˆ‘ä¸å–œæ¬¢è¿™ä¸ªäº§å“", "æ¶ˆæ"),
        ("è¿˜ä¸é”™å§", "ä¸­æ€§"),
    ]
    
    prompt = icl.few_shot_prompt(
        task_description="æƒ…æ„Ÿåˆ†æä»»åŠ¡ï¼šåˆ¤æ–­æ–‡æœ¬çš„æƒ…æ„Ÿå€¾å‘",
        examples=sentiment_examples,
        query="è¿™ä¸ªæœåŠ¡è´¨é‡å¾ˆé«˜"
    )
    
    print("Few-shotæç¤ºç¤ºä¾‹:")
    print(prompt)
    print("\n" + "="*60 + "\n")

def gpt3_architecture_analysis():
    """GPT-3æ¶æ„åˆ†æ"""
    print("=== GPT-3 æ¶æ„åˆ†æ ===\n")
    
    # ä¸åŒè§„æ¨¡çš„GPT-3é…ç½®
    configs = {
        "GPT-3 Small (125M)": GPT3Config(
            n_embd=768, n_layer=12, n_head=12, n_positions=2048
        ),
        "GPT-3 Medium (350M)": GPT3Config(
            n_embd=1024, n_layer=24, n_head=16, n_positions=2048
        ),
        "GPT-3 Large (760M)": GPT3Config(
            n_embd=1280, n_layer=36, n_head=20, n_positions=2048
        ),
        "GPT-3 XL (1.3B)": GPT3Config(
            n_embd=2048, n_layer=24, n_head=32, n_positions=2048
        ),
        "GPT-3 2.7B": GPT3Config(
            n_embd=2560, n_layer=32, n_head=32, n_positions=2048
        ),
        "GPT-3 6.7B": GPT3Config(
            n_embd=4096, n_layer=32, n_head=32, n_positions=2048
        ),
        "GPT-3 13B": GPT3Config(
            n_embd=5140, n_layer=40, n_head=40, n_positions=2048
        ),
        "GPT-3 175B": GPT3Config(
            n_embd=12288, n_layer=96, n_head=96, n_positions=2048
        ),
    }
    
    print("GPT-3ä¸åŒè§„æ¨¡æ¨¡å‹é…ç½®:")
    print(f"{'æ¨¡å‹':<20} {'å±‚æ•°':<6} {'éšè—ç»´åº¦':<8} {'æ³¨æ„åŠ›å¤´':<8} {'å‚æ•°é‡':<15}")
    print("-" * 65)
    
    for name, config in configs.items():
        # ä¼°ç®—å‚æ•°é‡ï¼ˆç®€åŒ–è®¡ç®—ï¼‰
        embed_params = config.vocab_size * config.n_embd
        pos_embed_params = config.n_positions * config.n_embd
        layer_params = config.n_layer * (
            # æ³¨æ„åŠ›å±‚
            4 * config.n_embd * config.n_embd +  # QKV + output projection
            # MLPå±‚
            config.n_embd * config.n_inner + config.n_inner * config.n_embd +
            # LayerNorm
            4 * config.n_embd
        )
        total_params = embed_params + pos_embed_params + layer_params
        
        print(f"{name:<20} {config.n_layer:<6} {config.n_embd:<8} {config.n_head:<8} {total_params/1e6:.1f}M")
    
    print("\nGPT-3å…³é”®åˆ›æ–°:")
    print("1. æå¤§çš„æ¨¡å‹è§„æ¨¡ (175Bå‚æ•°)")
    print("2. é«˜è´¨é‡çš„è®­ç»ƒæ•°æ® (çº¦45TBæ–‡æœ¬)")
    print("3. å¼ºå¤§çš„Few-shotå­¦ä¹ èƒ½åŠ›")
    print("4. ä¸Šä¸‹æ–‡å­¦ä¹  (In-Context Learning)")
    print("5. ç¨€ç–æ³¨æ„åŠ›æœºåˆ¶ä¼˜åŒ–")
    print("6. æ¨¡å‹å¹¶è¡Œå’Œæ¢¯åº¦æ£€æŸ¥ç‚¹")
    print("7. æ—‹è½¬ä½ç½®ç¼–ç  (RoPE)")
    print("8. å¹¶è¡Œæ³¨æ„åŠ›å’ŒMLPè®¡ç®—")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ GPT-3 æ¨¡å‹å®ç°ä¸æ¼”ç¤º")
    print("=" * 60)
    
    try:
        # 1. æ¶æ„åˆ†æ
        gpt3_architecture_analysis()
        print("\n" + "="*60 + "\n")
        
        # 2. è®­ç»ƒæ¼”ç¤º
        model, tokenizer = gpt3_training_demo()
        print("="*60 + "\n")
        
        # 3. æ¨ç†æ¼”ç¤º
        gpt3_inference_demo(model, tokenizer)
        print("="*60 + "\n")
        
        # 4. Few-shotå­¦ä¹ æ¼”ç¤º
        gpt3_few_shot_demo()
        
        print("âœ… GPT-3æ¼”ç¤ºå®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ è¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()