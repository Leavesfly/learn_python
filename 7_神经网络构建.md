# ç¥ç»ç½‘ç»œæ„å»º

## ğŸ“‹ ç›®å½•

- [nn.Module è¯¦è§£](#nnmodule-è¯¦è§£)
- [å¸¸ç”¨å±‚ä»‹ç»](#å¸¸ç”¨å±‚ä»‹ç»)
- [æ„å»ºç¥ç»ç½‘ç»œ](#æ„å»ºç¥ç»ç½‘ç»œ)
- [æ¨¡å‹å®¹å™¨](#æ¨¡å‹å®¹å™¨)
- [å‚æ•°ç®¡ç†](#å‚æ•°ç®¡ç†)
- [æ¨¡å‹ä¿å­˜ä¸åŠ è½½](#æ¨¡å‹ä¿å­˜ä¸åŠ è½½)
- [å®æˆ˜ç¤ºä¾‹](#å®æˆ˜ç¤ºä¾‹)

---

## ğŸ—ï¸ nn.Module è¯¦è§£

`nn.Module` æ˜¯ PyTorch ä¸­**æ‰€æœ‰ç¥ç»ç½‘ç»œæ¨¡å—çš„åŸºç±»**ã€‚

### åŸºæœ¬ç»“æ„

```python
import torch
import torch.nn as nn

class MyModel(nn.Module):
    def __init__(self):
        # å¿…é¡»è°ƒç”¨çˆ¶ç±»æ„é€ å‡½æ•°
        super(MyModel, self).__init__()
        # æˆ–ä½¿ç”¨æ›´ç®€æ´çš„å†™æ³•
        # super().__init__()
        
        # å®šä¹‰å±‚
        self.linear1 = nn.Linear(10, 20)
        self.linear2 = nn.Linear(20, 1)
    
    def forward(self, x):
        # å®šä¹‰å‰å‘ä¼ æ’­
        x = self.linear1(x)
        x = torch.relu(x)
        x = self.linear2(x)
        return x

# åˆ›å»ºæ¨¡å‹å®ä¾‹
model = MyModel()

# å‰å‘ä¼ æ’­ï¼ˆè‡ªåŠ¨è°ƒç”¨ forwardï¼‰
x = torch.randn(32, 10)
output = model(x)  # ç­‰ä»·äº model.forward(x)

print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
```

### nn.Module çš„æ ¸å¿ƒåŠŸèƒ½

```python
import torch
import torch.nn as nn

class Net(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 16, 3)
        self.fc1 = nn.Linear(16 * 30 * 30, 10)
    
    def forward(self, x):
        x = torch.relu(self.conv1(x))
        x = x.view(x.size(0), -1)
        x = self.fc1(x)
        return x

model = Net()

# 1. è®¿é—®å‚æ•°
print("=== å‚æ•°è®¿é—® ===")
for name, param in model.named_parameters():
    print(f"{name}: {param.shape}")

# 2. è®¿é—®å­æ¨¡å—
print("\n=== å­æ¨¡å— ===")
for name, module in model.named_modules():
    print(f"{name}: {module.__class__.__name__}")

# 3. æ¨¡å¼åˆ‡æ¢
model.train()  # è®­ç»ƒæ¨¡å¼
model.eval()   # è¯„ä¼°æ¨¡å¼

# 4. è®¾å¤‡è½¬ç§»
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# 5. å‚æ•°åˆå§‹åŒ–
def init_weights(m):
    if isinstance(m, nn.Linear):
        nn.init.xavier_uniform_(m.weight)
        nn.init.zeros_(m.bias)

model.apply(init_weights)
```

---

## ğŸ§± å¸¸ç”¨å±‚ä»‹ç»

### 1. å…¨è¿æ¥å±‚ï¼ˆLinearï¼‰

```python
import torch
import torch.nn as nn

# nn.Linear(in_features, out_features, bias=True)
linear = nn.Linear(10, 5)

# è¾“å…¥: (batch_size, in_features)
x = torch.randn(32, 10)
output = linear(x)
print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")  # (32, 5)

# æŸ¥çœ‹å‚æ•°
print(f"æƒé‡å½¢çŠ¶: {linear.weight.shape}")  # (5, 10)
print(f"åç½®å½¢çŠ¶: {linear.bias.shape}")    # (5,)

# æ•°å­¦å…¬å¼: y = xW^T + b
```

### 2. å·ç§¯å±‚ï¼ˆConvolutionalï¼‰

```python
import torch
import torch.nn as nn

# 2D å·ç§¯
# nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0)
conv = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)

# è¾“å…¥: (batch_size, channels, height, width)
x = torch.randn(32, 3, 64, 64)
output = conv(x)
print(f"å·ç§¯è¾“å‡º: {output.shape}")  # (32, 16, 64, 64)

# 1D å·ç§¯ï¼ˆç”¨äºåºåˆ—ï¼‰
conv1d = nn.Conv1d(in_channels=10, out_channels=20, kernel_size=3)
x_seq = torch.randn(32, 10, 100)  # (batch, channels, length)
output_seq = conv1d(x_seq)
print(f"1Då·ç§¯è¾“å‡º: {output_seq.shape}")  # (32, 20, 98)

# 3D å·ç§¯ï¼ˆç”¨äºè§†é¢‘ï¼‰
conv3d = nn.Conv3d(in_channels=3, out_channels=16, kernel_size=3)
x_video = torch.randn(16, 3, 10, 64, 64)  # (batch, channels, depth, height, width)
output_video = conv3d(x_video)
print(f"3Då·ç§¯è¾“å‡º: {output_video.shape}")
```

### 3. æ± åŒ–å±‚ï¼ˆPoolingï¼‰

```python
import torch
import torch.nn as nn

# æœ€å¤§æ± åŒ–
maxpool = nn.MaxPool2d(kernel_size=2, stride=2)
x = torch.randn(32, 16, 64, 64)
output = maxpool(x)
print(f"æœ€å¤§æ± åŒ–: {output.shape}")  # (32, 16, 32, 32)

# å¹³å‡æ± åŒ–
avgpool = nn.AvgPool2d(kernel_size=2, stride=2)
output = avgpool(x)
print(f"å¹³å‡æ± åŒ–: {output.shape}")  # (32, 16, 32, 32)

# è‡ªé€‚åº”æ± åŒ–ï¼ˆè¾“å‡ºå›ºå®šå¤§å°ï¼‰
adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))
x = torch.randn(32, 512, 7, 7)
output = adaptive_pool(x)
print(f"è‡ªé€‚åº”æ± åŒ–: {output.shape}")  # (32, 512, 1, 1)
```

### 4. æ¿€æ´»å‡½æ•°

```python
import torch
import torch.nn as nn

x = torch.randn(10)

# ReLU
relu = nn.ReLU()
print(f"ReLU: {relu(x)}")

# LeakyReLU
leaky_relu = nn.LeakyReLU(negative_slope=0.01)
print(f"LeakyReLU: {leaky_relu(x)}")

# Sigmoid
sigmoid = nn.Sigmoid()
print(f"Sigmoid: {sigmoid(x)}")

# Tanh
tanh = nn.Tanh()
print(f"Tanh: {tanh(x)}")

# Softmax
softmax = nn.Softmax(dim=0)
print(f"Softmax: {softmax(x)}")
print(f"Softmaxæ±‚å’Œ: {softmax(x).sum()}")  # åº”è¯¥ä¸º 1

# GELU (ç”¨äº Transformer)
gelu = nn.GELU()
print(f"GELU: {gelu(x)}")
```

### 5. å½’ä¸€åŒ–å±‚

```python
import torch
import torch.nn as nn

# Batch Normalization
# è¾“å…¥: (batch, channels, height, width)
bn = nn.BatchNorm2d(num_features=16)
x = torch.randn(32, 16, 64, 64)
output = bn(x)
print(f"BatchNormè¾“å‡º: {output.shape}")

# Layer Normalization
ln = nn.LayerNorm(normalized_shape=[16, 64, 64])
output = ln(x)
print(f"LayerNormè¾“å‡º: {output.shape}")

# Instance Normalization
in_norm = nn.InstanceNorm2d(num_features=16)
output = in_norm(x)
print(f"InstanceNormè¾“å‡º: {output.shape}")

# Group Normalization
gn = nn.GroupNorm(num_groups=4, num_channels=16)
output = gn(x)
print(f"GroupNormè¾“å‡º: {output.shape}")
```

### 6. Dropout

```python
import torch
import torch.nn as nn

# Dropout: è®­ç»ƒæ—¶éšæœºä¸¢å¼ƒï¼Œæ¨ç†æ—¶ä¸ä¸¢å¼ƒ
dropout = nn.Dropout(p=0.5)

x = torch.ones(10)

# è®­ç»ƒæ¨¡å¼
dropout.train()
output_train = dropout(x)
print(f"è®­ç»ƒæ¨¡å¼: {output_train}")  # çº¦ä¸€åŠä¸º0

# è¯„ä¼°æ¨¡å¼
dropout.eval()
output_eval = dropout(x)
print(f"è¯„ä¼°æ¨¡å¼: {output_eval}")  # å…¨ä¸º1
```

### 7. å¾ªç¯å±‚ï¼ˆRNN, LSTM, GRUï¼‰

```python
import torch
import torch.nn as nn

# LSTM
# (input_size, hidden_size, num_layers)
lstm = nn.LSTM(input_size=10, hidden_size=20, num_layers=2, batch_first=True)

# è¾“å…¥: (batch, seq_len, input_size)
x = torch.randn(32, 100, 10)
output, (h_n, c_n) = lstm(x)

print(f"LSTMè¾“å‡º: {output.shape}")      # (32, 100, 20)
print(f"æœ€åéšè—çŠ¶æ€: {h_n.shape}")     # (2, 32, 20)
print(f"æœ€åç»†èƒçŠ¶æ€: {c_n.shape}")     # (2, 32, 20)

# GRU
gru = nn.GRU(input_size=10, hidden_size=20, num_layers=2, batch_first=True)
output, h_n = gru(x)
print(f"GRUè¾“å‡º: {output.shape}")
```

### 8. Transformer å±‚

```python
import torch
import torch.nn as nn

# å¤šå¤´æ³¨æ„åŠ›
multihead_attn = nn.MultiheadAttention(embed_dim=512, num_heads=8, batch_first=True)

# è¾“å…¥: (batch, seq_len, embed_dim)
query = torch.randn(32, 100, 512)
key = torch.randn(32, 100, 512)
value = torch.randn(32, 100, 512)

attn_output, attn_weights = multihead_attn(query, key, value)
print(f"æ³¨æ„åŠ›è¾“å‡º: {attn_output.shape}")

# Transformer Encoder Layer
encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8, batch_first=True)
src = torch.randn(32, 100, 512)
output = encoder_layer(src)
print(f"ç¼–ç å™¨è¾“å‡º: {output.shape}")
```

---

## ğŸ”¨ æ„å»ºç¥ç»ç½‘ç»œ

### 1. ç®€å•å‰é¦ˆç½‘ç»œ

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class SimpleMLP(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super().__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, output_size)
        self.dropout = nn.Dropout(0.2)
    
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = F.relu(self.fc2(x))
        x = self.dropout(x)
        x = self.fc3(x)
        return x

model = SimpleMLP(784, 256, 10)
x = torch.randn(32, 784)
output = model(x)
print(f"MLPè¾“å‡º: {output.shape}")
```

### 2. CNN ç½‘ç»œ

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class SimpleCNN(nn.Module):
    def __init__(self, num_classes=10):
        super().__init__()
        # å·ç§¯å±‚
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(32)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(64)
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.bn3 = nn.BatchNorm2d(128)
        
        # æ± åŒ–å±‚
        self.pool = nn.MaxPool2d(2, 2)
        
        # å…¨è¿æ¥å±‚
        self.fc1 = nn.Linear(128 * 4 * 4, 512)
        self.fc2 = nn.Linear(512, num_classes)
        
        self.dropout = nn.Dropout(0.5)
    
    def forward(self, x):
        # å·ç§¯å—1
        x = self.pool(F.relu(self.bn1(self.conv1(x))))  # 32x32 -> 16x16
        # å·ç§¯å—2
        x = self.pool(F.relu(self.bn2(self.conv2(x))))  # 16x16 -> 8x8
        # å·ç§¯å—3
        x = self.pool(F.relu(self.bn3(self.conv3(x))))  # 8x8 -> 4x4
        
        # å±•å¹³
        x = x.view(x.size(0), -1)  # æˆ– x.flatten(1)
        
        # å…¨è¿æ¥
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        
        return x

model = SimpleCNN(num_classes=10)
x = torch.randn(32, 3, 32, 32)
output = model(x)
print(f"CNNè¾“å‡º: {output.shape}")
```

### 3. ResNet æ®‹å·®å—

```python
import torch
import torch.nn as nn

class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride, 1)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1)
        self.bn2 = nn.BatchNorm2d(out_channels)
        
        # å¿«æ·è¿æ¥
        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, 1, stride),
                nn.BatchNorm2d(out_channels)
            )
    
    def forward(self, x):
        out = torch.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += self.shortcut(x)  # æ®‹å·®è¿æ¥
        out = torch.relu(out)
        return out

# ä½¿ç”¨æ®‹å·®å—
block = ResidualBlock(64, 128, stride=2)
x = torch.randn(32, 64, 32, 32)
output = block(x)
print(f"æ®‹å·®å—è¾“å‡º: {output.shape}")
```

---

## ğŸ“¦ æ¨¡å‹å®¹å™¨

### 1. nn.Sequential

æŒ‰é¡ºåºç»„ç»‡å±‚ï¼š

```python
import torch
import torch.nn as nn

# æ–¹å¼1: ä½ç½®å‚æ•°
model = nn.Sequential(
    nn.Linear(784, 256),
    nn.ReLU(),
    nn.Dropout(0.2),
    nn.Linear(256, 128),
    nn.ReLU(),
    nn.Dropout(0.2),
    nn.Linear(128, 10)
)

# æ–¹å¼2: OrderedDictï¼ˆå¯ä»¥å‘½åï¼‰
from collections import OrderedDict
model = nn.Sequential(OrderedDict([
    ('fc1', nn.Linear(784, 256)),
    ('relu1', nn.ReLU()),
    ('dropout1', nn.Dropout(0.2)),
    ('fc2', nn.Linear(256, 128)),
    ('relu2', nn.ReLU()),
    ('dropout2', nn.Dropout(0.2)),
    ('fc3', nn.Linear(128, 10))
]))

x = torch.randn(32, 784)
output = model(x)
print(f"Sequentialè¾“å‡º: {output.shape}")

# è®¿é—®å­æ¨¡å—
print(model[0])  # ç¬¬ä¸€å±‚
print(model.fc1)  # æŒ‰åç§°è®¿é—®
```

### 2. nn.ModuleList

å­˜å‚¨å­æ¨¡å—çš„åˆ—è¡¨ï¼š

```python
import torch
import torch.nn as nn

class MyModel(nn.Module):
    def __init__(self):
        super().__init__()
        # ModuleList: ä¼šè‡ªåŠ¨æ³¨å†Œå‚æ•°
        self.layers = nn.ModuleList([
            nn.Linear(10, 20),
            nn.Linear(20, 20),
            nn.Linear(20, 10)
        ])
    
    def forward(self, x):
        for layer in self.layers:
            x = torch.relu(layer(x))
        return x

model = MyModel()
print(f"å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters())}")
```

### 3. nn.ModuleDict

å­˜å‚¨å­æ¨¡å—çš„å­—å…¸ï¼š

```python
import torch
import torch.nn as nn

class MyModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.layers = nn.ModuleDict({
            'linear1': nn.Linear(10, 20),
            'linear2': nn.Linear(20, 10),
            'activation': nn.ReLU()
        })
    
    def forward(self, x):
        x = self.layers['linear1'](x)
        x = self.layers['activation'](x)
        x = self.layers['linear2'](x)
        return x

model = MyModel()
print(model.layers['linear1'])
```

---

## ğŸ›ï¸ å‚æ•°ç®¡ç†

### 1. è®¿é—®å‚æ•°

```python
import torch
import torch.nn as nn

model = nn.Sequential(
    nn.Linear(10, 20),
    nn.ReLU(),
    nn.Linear(20, 5)
)

# æ‰€æœ‰å‚æ•°
for name, param in model.named_parameters():
    print(f"{name}: {param.shape}")

# ç‰¹å®šå±‚çš„å‚æ•°
print(f"ç¬¬ä¸€å±‚æƒé‡: {model[0].weight.shape}")
print(f"ç¬¬ä¸€å±‚åç½®: {model[0].bias.shape}")

# å‚æ•°æ€»æ•°
total_params = sum(p.numel() for p in model.parameters())
print(f"æ€»å‚æ•°æ•°: {total_params}")

# å¯è®­ç»ƒå‚æ•°
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params}")
```

### 2. å†»ç»“å‚æ•°

```python
import torch
import torch.nn as nn

model = nn.Sequential(
    nn.Linear(10, 20),
    nn.ReLU(),
    nn.Linear(20, 5)
)

# å†»ç»“ç¬¬ä¸€å±‚
for param in model[0].parameters():
    param.requires_grad = False

# éªŒè¯
for name, param in model.named_parameters():
    print(f"{name}: requires_grad={param.requires_grad}")

# å†»ç»“æ•´ä¸ªæ¨¡å‹
for param in model.parameters():
    param.requires_grad = False

# æˆ–ä½¿ç”¨ eval() æ¨¡å¼ï¼ˆå½±å“ Dropout å’Œ BNï¼‰
model.eval()
```

### 3. å‚æ•°åˆå§‹åŒ–

```python
import torch
import torch.nn as nn

class MyModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear1 = nn.Linear(10, 20)
        self.linear2 = nn.Linear(20, 10)
        
        # åˆå§‹åŒ–
        self._initialize_weights()
    
    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                nn.init.zeros_(m.bias)
    
    def forward(self, x):
        x = torch.relu(self.linear1(x))
        x = self.linear2(x)
        return x

# æˆ–ä½¿ç”¨ apply
def init_weights(m):
    if isinstance(m, nn.Linear):
        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
        if m.bias is not None:
            nn.init.constant_(m.bias, 0)

model = MyModel()
model.apply(init_weights)
```

---

## ğŸ’¾ æ¨¡å‹ä¿å­˜ä¸åŠ è½½

### 1. ä¿å­˜å’ŒåŠ è½½æ•´ä¸ªæ¨¡å‹

```python
import torch
import torch.nn as nn

# å®šä¹‰æ¨¡å‹
model = nn.Sequential(
    nn.Linear(10, 20),
    nn.ReLU(),
    nn.Linear(20, 5)
)

# ä¿å­˜æ•´ä¸ªæ¨¡å‹
torch.save(model, 'model_complete.pth')

# åŠ è½½æ•´ä¸ªæ¨¡å‹
loaded_model = torch.load('model_complete.pth')
loaded_model.eval()
```

### 2. ä¿å­˜å’ŒåŠ è½½æ¨¡å‹å‚æ•°ï¼ˆæ¨èï¼‰

```python
import torch
import torch.nn as nn

# ä¿å­˜æ¨¡å‹å‚æ•°
torch.save(model.state_dict(), 'model_params.pth')

# åŠ è½½æ¨¡å‹å‚æ•°
model = nn.Sequential(
    nn.Linear(10, 20),
    nn.ReLU(),
    nn.Linear(20, 5)
)
model.load_state_dict(torch.load('model_params.pth'))
model.eval()
```

### 3. ä¿å­˜æ£€æŸ¥ç‚¹

```python
import torch
import torch.nn as nn
import torch.optim as optim

model = nn.Linear(10, 5)
optimizer = optim.Adam(model.parameters())
epoch = 10
loss = 0.5

# ä¿å­˜æ£€æŸ¥ç‚¹
checkpoint = {
    'epoch': epoch,
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'loss': loss,
}
torch.save(checkpoint, 'checkpoint.pth')

# åŠ è½½æ£€æŸ¥ç‚¹
checkpoint = torch.load('checkpoint.pth')
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
epoch = checkpoint['epoch']
loss = checkpoint['loss']

print(f"ä» epoch {epoch} æ¢å¤è®­ç»ƒ")
```

---

## ğŸ’¡ å®æˆ˜ç¤ºä¾‹

### å®Œæ•´çš„ MNIST åˆ†ç±»å™¨

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class MNISTClassifier(nn.Module):
    def __init__(self):
        super().__init__()
        # å·ç§¯å±‚
        self.conv1 = nn.Conv2d(1, 32, 3, 1)
        self.conv2 = nn.Conv2d(32, 64, 3, 1)
        
        # Dropout
        self.dropout1 = nn.Dropout(0.25)
        self.dropout2 = nn.Dropout(0.5)
        
        # å…¨è¿æ¥å±‚
        self.fc1 = nn.Linear(9216, 128)
        self.fc2 = nn.Linear(128, 10)
    
    def forward(self, x):
        # å·ç§¯ + æ± åŒ–
        x = self.conv1(x)
        x = F.relu(x)
        x = self.conv2(x)
        x = F.relu(x)
        x = F.max_pool2d(x, 2)
        x = self.dropout1(x)
        
        # å±•å¹³
        x = torch.flatten(x, 1)
        
        # å…¨è¿æ¥
        x = self.fc1(x)
        x = F.relu(x)
        x = self.dropout2(x)
        x = self.fc2(x)
        
        # Softmax (å¯é€‰ï¼ŒCrossEntropyLoss ä¼šè‡ªåŠ¨åŒ…å«)
        output = F.log_softmax(x, dim=1)
        return output

# åˆ›å»ºæ¨¡å‹
model = MNISTClassifier()

# æ¨¡æ‹Ÿè¾“å…¥
x = torch.randn(32, 1, 28, 28)
output = model(x)

print(f"æ¨¡å‹æ¶æ„:\n{model}")
print(f"\nè¾“å‡ºå½¢çŠ¶: {output.shape}")
print(f"å‚æ•°æ€»æ•°: {sum(p.numel() for p in model.parameters())}")
```

---

## ğŸ“ æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **nn.Module**: æ‰€æœ‰æ¨¡å‹çš„åŸºç±»
2. **å¸¸ç”¨å±‚**: Linear, Conv2d, BatchNorm, Dropout ç­‰
3. **æ¨¡å‹å®¹å™¨**: Sequential, ModuleList, ModuleDict
4. **å‚æ•°ç®¡ç†**: è®¿é—®ã€å†»ç»“ã€åˆå§‹åŒ–
5. **æ¨¡å‹ä¿å­˜**: state_dictï¼ˆæ¨èï¼‰æˆ–æ•´ä¸ªæ¨¡å‹

### æœ€ä½³å®è·µ

- âœ… ä½¿ç”¨ `super().__init__()` åˆå§‹åŒ–
- âœ… åœ¨ `__init__` ä¸­å®šä¹‰å±‚ï¼Œåœ¨ `forward` ä¸­ä½¿ç”¨
- âœ… ä½¿ç”¨ `state_dict` ä¿å­˜æ¨¡å‹
- âœ… é€‚å½“ä½¿ç”¨ `train()` å’Œ `eval()` æ¨¡å¼
- âœ… åˆç†åˆå§‹åŒ–å‚æ•°

---

**ä¸‹ä¸€æ­¥**: [è®­ç»ƒä¸ä¼˜åŒ–æŠ€å·§](./7_è®­ç»ƒä¸ä¼˜åŒ–æŠ€å·§.md)

*æœ€åæ›´æ–°: 2025-10-17*
