# -*- coding: utf-8 -*-
"""
RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰ä¸å‘é‡æ•°æ®ç®€å•å®ç°
æ¼”ç¤ºå‘é‡åŒ–ã€ç›¸ä¼¼åº¦è®¡ç®—ã€æ£€ç´¢å’Œç”Ÿæˆçš„å®Œæ•´æµç¨‹
"""

import json
import math
import re
import time
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from collections import defaultdict, Counter
from datetime import datetime
import sqlite3


@dataclass
class Document:
    """æ–‡æ¡£ç»“æ„"""
    id: str
    content: str
    metadata: Dict[str, Any]
    embedding: Optional[List[float]] = None
    created_at: Optional[datetime] = None
    
    def __post_init__(self):
        if self.created_at is None:
            self.created_at = datetime.now()


@dataclass
class QueryResult:
    """æŸ¥è¯¢ç»“æœç»“æ„"""
    document: Document
    similarity: float
    rank: int


class SimpleTokenizer:
    """ç®€å•çš„åˆ†è¯å™¨"""
    
    def __init__(self):
        self.vocab = {}
        self.vocab_size = 0
    
    def tokenize(self, text: str) -> List[str]:
        """åˆ†è¯"""
        # ç®€å•çš„åŸºäºæ­£åˆ™è¡¨è¾¾å¼çš„åˆ†è¯
        text = text.lower()
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·ï¼Œä¿ç•™ä¸­è‹±æ–‡å­—ç¬¦
        text = re.sub(r'[^\w\s\u4e00-\u9fff]', ' ', text)
        tokens = text.split()
        
        # å¯¹ä¸­æ–‡è¿›è¡Œå­—ç¬¦çº§åˆ†è¯
        result = []
        for token in tokens:
            if re.search(r'[\u4e00-\u9fff]', token):
                # ä¸­æ–‡å­—ç¬¦ï¼ŒæŒ‰å­—ç¬¦åˆ†è¯
                result.extend(list(token))
            else:
                # è‹±æ–‡å•è¯
                result.append(token)
        
        return [t for t in result if t.strip()]
    
    def build_vocab(self, texts: List[str]):
        """æ„å»ºè¯æ±‡è¡¨"""
        word_counts = Counter()
        
        for text in texts:
            tokens = self.tokenize(text)
            word_counts.update(tokens)
        
        # æŒ‰é¢‘ç‡æ’åºï¼Œæ„å»ºè¯æ±‡è¡¨
        self.vocab = {word: idx for idx, (word, _) in enumerate(word_counts.most_common())}
        self.vocab_size = len(self.vocab)
        
        print(f"æ„å»ºè¯æ±‡è¡¨å®Œæˆï¼Œå…± {self.vocab_size} ä¸ªè¯æ±‡")
    
    def tokens_to_ids(self, tokens: List[str]) -> List[int]:
        """å°†è¯æ±‡è½¬æ¢ä¸ºID"""
        return [self.vocab.get(token, 0) for token in tokens]


class TFIDFVectorizer:
    """TF-IDFå‘é‡åŒ–å™¨"""
    
    def __init__(self, max_features: int = 1000):
        self.max_features = max_features
        self.tokenizer = SimpleTokenizer()
        self.idf_scores = {}
        self.feature_names = []
    
    def fit(self, documents: List[str]):
        """è®­ç»ƒTF-IDFæ¨¡å‹"""
        print("å¼€å§‹è®­ç»ƒTF-IDFæ¨¡å‹...")
        
        # æ„å»ºè¯æ±‡è¡¨
        self.tokenizer.build_vocab(documents)
        
        # è®¡ç®—æ–‡æ¡£é¢‘ç‡
        doc_frequencies = defaultdict(int)
        total_docs = len(documents)
        
        for doc in documents:
            tokens = set(self.tokenizer.tokenize(doc))  # ä½¿ç”¨setå»é‡
            for token in tokens:
                doc_frequencies[token] += 1
        
        # è®¡ç®—IDFåˆ†æ•°
        for token, df in doc_frequencies.items():
            self.idf_scores[token] = math.log(total_docs / df)
        
        # é€‰æ‹©å‰max_featuresä¸ªæœ€é‡è¦çš„ç‰¹å¾
        sorted_features = sorted(self.idf_scores.items(), key=lambda x: x[1], reverse=True)
        self.feature_names = [token for token, _ in sorted_features[:self.max_features]]
        
        print(f"TF-IDFæ¨¡å‹è®­ç»ƒå®Œæˆï¼Œç‰¹å¾ç»´åº¦: {len(self.feature_names)}")
    
    def transform(self, text: str) -> List[float]:
        """å°†æ–‡æœ¬è½¬æ¢ä¸ºTF-IDFå‘é‡"""
        if not self.feature_names:
            return [0.0] * self.max_features
        
        tokens = self.tokenizer.tokenize(text)
        token_counts = Counter(tokens)
        total_tokens = len(tokens)
        
        # è®¡ç®—TF-IDFå‘é‡
        vector = []
        for feature in self.feature_names:
            tf = token_counts.get(feature, 0) / total_tokens if total_tokens > 0 else 0
            idf = self.idf_scores.get(feature, 0)
            tfidf = tf * idf
            vector.append(tfidf)
        
        return vector
    
    def fit_transform(self, documents: List[str]) -> List[List[float]]:
        """è®­ç»ƒå¹¶è½¬æ¢æ–‡æ¡£"""
        self.fit(documents)
        return [self.transform(doc) for doc in documents]


class VectorSimilarity:
    """å‘é‡ç›¸ä¼¼åº¦è®¡ç®—"""
    
    @staticmethod
    def cosine_similarity(vec1: List[float], vec2: List[float]) -> float:
        """ä½™å¼¦ç›¸ä¼¼åº¦"""
        if len(vec1) != len(vec2):
            return 0.0
        
        dot_product = sum(a * b for a, b in zip(vec1, vec2))
        norm1 = math.sqrt(sum(x * x for x in vec1))
        norm2 = math.sqrt(sum(x * x for x in vec2))
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return dot_product / (norm1 * norm2)
    
    @staticmethod
    def euclidean_distance(vec1: List[float], vec2: List[float]) -> float:
        """æ¬§æ°è·ç¦»"""
        if len(vec1) != len(vec2):
            return float('inf')
        
        return math.sqrt(sum((a - b) ** 2 for a, b in zip(vec1, vec2)))
    
    @staticmethod
    def manhattan_distance(vec1: List[float], vec2: List[float]) -> float:
        """æ›¼å“ˆé¡¿è·ç¦»"""
        if len(vec1) != len(vec2):
            return float('inf')
        
        return sum(abs(a - b) for a, b in zip(vec1, vec2))


class VectorDatabase:
    """å‘é‡æ•°æ®åº“"""
    
    def __init__(self, db_path: str = ":memory:"):
        self.db_path = db_path
        self.conn = sqlite3.connect(db_path, check_same_thread=False)
        self._init_database()
    
    def _init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        self.conn.execute("""
            CREATE TABLE IF NOT EXISTS documents (
                id TEXT PRIMARY KEY,
                content TEXT NOT NULL,
                metadata TEXT,
                embedding TEXT,
                created_at REAL
            )
        """)
        self.conn.commit()
    
    def add_document(self, document: Document):
        """æ·»åŠ æ–‡æ¡£"""
        self.conn.execute("""
            INSERT OR REPLACE INTO documents 
            (id, content, metadata, embedding, created_at)
            VALUES (?, ?, ?, ?, ?)
        """, (
            document.id,
            document.content,
            json.dumps(document.metadata),
            json.dumps(document.embedding) if document.embedding else None,
            (document.created_at or datetime.now()).timestamp()
        ))
        self.conn.commit()
    
    def get_document(self, doc_id: str) -> Optional[Document]:
        """è·å–æ–‡æ¡£"""
        cursor = self.conn.execute(
            "SELECT * FROM documents WHERE id = ?", (doc_id,)
        )
        row = cursor.fetchone()
        
        if row:
            return Document(
                id=row[0],
                content=row[1],
                metadata=json.loads(row[2]) if row[2] else {},
                embedding=json.loads(row[3]) if row[3] else None,
                created_at=datetime.fromtimestamp(row[4])
            )
        return None
    
    def get_all_documents(self) -> List[Document]:
        """è·å–æ‰€æœ‰æ–‡æ¡£"""
        cursor = self.conn.execute("SELECT * FROM documents")
        documents = []
        
        for row in cursor.fetchall():
            documents.append(Document(
                id=row[0],
                content=row[1],
                metadata=json.loads(row[2]) if row[2] else {},
                embedding=json.loads(row[3]) if row[3] else None,
                created_at=datetime.fromtimestamp(row[4])
            ))
        
        return documents
    
    def delete_document(self, doc_id: str) -> bool:
        """åˆ é™¤æ–‡æ¡£"""
        cursor = self.conn.execute("DELETE FROM documents WHERE id = ?", (doc_id,))
        self.conn.commit()
        return cursor.rowcount > 0
    
    def count_documents(self) -> int:
        """æ–‡æ¡£æ€»æ•°"""
        cursor = self.conn.execute("SELECT COUNT(*) FROM documents")
        return cursor.fetchone()[0]


class RAGSystem:
    """RAGæ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿ"""
    
    def __init__(self, vector_dim: int = 512, similarity_threshold: float = 0.1):
        self.vector_dim = vector_dim
        self.similarity_threshold = similarity_threshold
        
        # æ ¸å¿ƒç»„ä»¶
        self.vectorizer = TFIDFVectorizer(max_features=vector_dim)
        self.vector_db = VectorDatabase()
        self.similarity_calculator = VectorSimilarity()
        
        # çŠ¶æ€
        self.is_trained = False
        self.documents_count = 0
    
    def add_documents(self, documents: List[Dict[str, Any]]):
        """æ‰¹é‡æ·»åŠ æ–‡æ¡£"""
        print(f"æ­£åœ¨æ·»åŠ  {len(documents)} ä¸ªæ–‡æ¡£...")
        
        # å‡†å¤‡æ–‡æ¡£å†…å®¹ç”¨äºè®­ç»ƒå‘é‡åŒ–å™¨
        contents = [doc['content'] for doc in documents]
        
        # è®­ç»ƒå‘é‡åŒ–å™¨ï¼ˆå¦‚æœè¿˜æœªè®­ç»ƒï¼‰
        if not self.is_trained:
            self.vectorizer.fit(contents)
            self.is_trained = True
        
        # å‘é‡åŒ–æ–‡æ¡£å¹¶å­˜å‚¨
        for i, doc_data in enumerate(documents):
            doc_id = doc_data.get('id', f"doc_{int(time.time())}_{i}")
            content = doc_data['content']
            metadata = doc_data.get('metadata', {})
            
            # è®¡ç®—æ–‡æ¡£å‘é‡
            embedding = self.vectorizer.transform(content)
            
            # åˆ›å»ºæ–‡æ¡£å¯¹è±¡
            document = Document(
                id=doc_id,
                content=content,
                metadata=metadata,
                embedding=embedding
            )
            
            # å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
            self.vector_db.add_document(document)
            
            if (i + 1) % 10 == 0:  # æ¯10ä¸ªæ–‡æ¡£æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                print(f"å·²å¤„ç† {i + 1}/{len(documents)} ä¸ªæ–‡æ¡£")
        
        self.documents_count = self.vector_db.count_documents()
        print(f"æ–‡æ¡£æ·»åŠ å®Œæˆï¼å½“å‰å…±æœ‰ {self.documents_count} ä¸ªæ–‡æ¡£")
    
    def search(self, query: str, top_k: int = 5, similarity_method: str = 'cosine') -> List[QueryResult]:
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        if not self.is_trained:
            print("RAGç³»ç»Ÿå°šæœªè®­ç»ƒï¼Œè¯·å…ˆæ·»åŠ æ–‡æ¡£")
            return []
        
        print(f"æ£€ç´¢æŸ¥è¯¢: '{query}'")
        
        # å‘é‡åŒ–æŸ¥è¯¢
        query_embedding = self.vectorizer.transform(query)
        
        # è·å–æ‰€æœ‰æ–‡æ¡£
        all_documents = self.vector_db.get_all_documents()
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = []
        for doc in all_documents:
            if doc.embedding:
                if similarity_method == 'cosine':
                    similarity = self.similarity_calculator.cosine_similarity(
                        query_embedding, doc.embedding
                    )
                elif similarity_method == 'euclidean':
                    distance = self.similarity_calculator.euclidean_distance(
                        query_embedding, doc.embedding
                    )
                    similarity = 1 / (1 + distance)  # è½¬æ¢ä¸ºç›¸ä¼¼åº¦
                else:
                    similarity = self.similarity_calculator.cosine_similarity(
                        query_embedding, doc.embedding
                    )
                
                if similarity >= self.similarity_threshold:
                    similarities.append((doc, similarity))
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        similarities.sort(key=lambda x: x[1], reverse=True)
        
        # æ„å»ºç»“æœ
        results = []
        for rank, (doc, similarity) in enumerate(similarities[:top_k]):
            results.append(QueryResult(
                document=doc,
                similarity=similarity,
                rank=rank + 1
            ))
        
        print(f"æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£")
        return results
    
    def generate_context(self, query: str, max_context_length: int = 1000) -> str:
        """ä¸ºæŸ¥è¯¢ç”Ÿæˆä¸Šä¸‹æ–‡"""
        search_results = self.search(query, top_k=5)
        
        if not search_results:
            return "æœªæ‰¾åˆ°ç›¸å…³å†…å®¹ã€‚"
        
        context_parts = []
        current_length = 0
        
        for result in search_results:
            doc = result.document
            content = doc.content
            
            # æ·»åŠ æ–‡æ¡£ä¿¡æ¯å¤´
            doc_header = f"[æ–‡æ¡£ {doc.id}, ç›¸ä¼¼åº¦: {result.similarity:.3f}]\n"
            
            if current_length + len(doc_header) + len(content) <= max_context_length:
                context_parts.append(doc_header + content)
                current_length += len(doc_header) + len(content)
            else:
                # æˆªæ–­å†…å®¹ä»¥é€‚åº”é•¿åº¦é™åˆ¶
                remaining_space = max_context_length - current_length - len(doc_header)
                if remaining_space > 50:  # ç¡®ä¿æœ‰è¶³å¤Ÿç©ºé—´æ˜¾ç¤ºæœ‰æ„ä¹‰çš„å†…å®¹
                    truncated_content = content[:remaining_space - 3] + "..."
                    context_parts.append(doc_header + truncated_content)
                break
        
        return "\n\n".join(context_parts)
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
        return {
            "documents_count": self.documents_count,
            "vector_dimension": self.vector_dim,
            "is_trained": self.is_trained,
            "vocabulary_size": self.vectorizer.tokenizer.vocab_size if self.is_trained else 0,
            "similarity_threshold": self.similarity_threshold
        }


def create_sample_documents() -> List[Dict[str, Any]]:
    """åˆ›å»ºç¤ºä¾‹æ–‡æ¡£"""
    return [
        {
            "id": "python_intro",
            "content": "Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œç”±Guido van Rossumäº1991å¹´åˆ›å»ºã€‚å®ƒå…·æœ‰ç®€æ´çš„è¯­æ³•å’Œå¼ºå¤§çš„åŠŸèƒ½ï¼Œå¹¿æ³›ç”¨äºWebå¼€å‘ã€æ•°æ®ç§‘å­¦ã€äººå·¥æ™ºèƒ½ç­‰é¢†åŸŸã€‚Pythonçš„è®¾è®¡å“²å­¦å¼ºè°ƒä»£ç çš„å¯è¯»æ€§å’Œç®€æ´æ€§ã€‚",
            "metadata": {"category": "ç¼–ç¨‹è¯­è¨€", "difficulty": "å…¥é—¨"}
        },
        {
            "id": "machine_learning_basics",
            "content": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿåœ¨ä¸è¢«æ˜ç¡®ç¼–ç¨‹çš„æƒ…å†µä¸‹å­¦ä¹ ã€‚æœºå™¨å­¦ä¹ ç®—æ³•æ„å»ºæ•°å­¦æ¨¡å‹ï¼ŒåŸºäºè®­ç»ƒæ•°æ®è¿›è¡Œé¢„æµ‹æˆ–å†³ç­–ã€‚å¸¸è§çš„æœºå™¨å­¦ä¹ ç±»å‹åŒ…æ‹¬ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ã€‚",
            "metadata": {"category": "äººå·¥æ™ºèƒ½", "difficulty": "ä¸­çº§"}
        },
        {
            "id": "deep_learning_intro",
            "content": "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é›†ï¼Œå®ƒåŸºäºäººå·¥ç¥ç»ç½‘ç»œï¼Œç‰¹åˆ«æ˜¯æ·±åº¦ç¥ç»ç½‘ç»œã€‚æ·±åº¦å­¦ä¹ åœ¨å›¾åƒè¯†åˆ«ã€è¯­éŸ³è¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰ä»»åŠ¡ä¸­å–å¾—äº†çªç ´æ€§è¿›å±•ã€‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰æ˜¯å¸¸ç”¨çš„æ·±åº¦å­¦ä¹ æ¶æ„ã€‚",
            "metadata": {"category": "äººå·¥æ™ºèƒ½", "difficulty": "é«˜çº§"}
        },
        {
            "id": "data_science_overview",
            "content": "æ•°æ®ç§‘å­¦æ˜¯ä¸€ä¸ªè·¨å­¦ç§‘é¢†åŸŸï¼Œç»“åˆäº†ç»Ÿè®¡å­¦ã€è®¡ç®—æœºç§‘å­¦å’Œé¢†åŸŸçŸ¥è¯†æ¥ä»æ•°æ®ä¸­æå–æ´å¯Ÿã€‚æ•°æ®ç§‘å­¦å®¶ä½¿ç”¨å„ç§å·¥å…·å’ŒæŠ€æœ¯ï¼ŒåŒ…æ‹¬æ•°æ®æŒ–æ˜ã€æœºå™¨å­¦ä¹ ã€å¯è§†åŒ–ç­‰ï¼Œæ¥åˆ†æå¤æ‚çš„æ•°æ®é›†å¹¶è§£å†³ä¸šåŠ¡é—®é¢˜ã€‚",
            "metadata": {"category": "æ•°æ®ç§‘å­¦", "difficulty": "ä¸­çº§"}
        },
        {
            "id": "web_development_python",
            "content": "Pythonåœ¨Webå¼€å‘ä¸­éå¸¸æµè¡Œï¼Œæœ‰è®¸å¤šå¼ºå¤§çš„æ¡†æ¶å¯ä¾›é€‰æ‹©ã€‚Djangoæ˜¯ä¸€ä¸ªé«˜çº§Webæ¡†æ¶ï¼Œæä¾›äº†å®Œæ•´çš„è§£å†³æ–¹æ¡ˆã€‚Flaskæ˜¯ä¸€ä¸ªè½»é‡çº§æ¡†æ¶ï¼Œæ›´é€‚åˆå°å‹é¡¹ç›®ã€‚FastAPIæ˜¯ä¸€ä¸ªç°ä»£æ¡†æ¶ï¼Œä¸“ä¸ºæ„å»ºAPIè€Œè®¾è®¡ï¼Œæ”¯æŒå¼‚æ­¥ç¼–ç¨‹ã€‚",
            "metadata": {"category": "Webå¼€å‘", "difficulty": "ä¸­çº§"}
        },
        {
            "id": "natural_language_processing",
            "content": "è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œä¸“æ³¨äºä½¿è®¡ç®—æœºç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚NLPæŠ€æœ¯åŒ…æ‹¬æ–‡æœ¬åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æã€æœºå™¨ç¿»è¯‘ã€é—®ç­”ç³»ç»Ÿç­‰ã€‚è¿‘å¹´æ¥ï¼ŒåŸºäºTransformerçš„å¤§è¯­è¨€æ¨¡å‹å¦‚GPTã€BERTç­‰åœ¨NLPä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚",
            "metadata": {"category": "è‡ªç„¶è¯­è¨€å¤„ç†", "difficulty": "é«˜çº§"}
        },
        {
            "id": "database_fundamentals",
            "content": "æ•°æ®åº“æ˜¯å­˜å‚¨å’Œç®¡ç†æ•°æ®çš„ç³»ç»Ÿã€‚å…³ç³»å‹æ•°æ®åº“ä½¿ç”¨SQLè¯­è¨€è¿›è¡ŒæŸ¥è¯¢ï¼Œå¦‚MySQLã€PostgreSQLç­‰ã€‚NoSQLæ•°æ®åº“é€‚åˆå¤„ç†éç»“æ„åŒ–æ•°æ®ï¼Œå¦‚MongoDBã€Redisç­‰ã€‚æ•°æ®åº“è®¾è®¡éœ€è¦è€ƒè™‘æ•°æ®æ¨¡å‹ã€ç´¢å¼•ã€äº‹åŠ¡å¤„ç†ç­‰æ–¹é¢ã€‚",
            "metadata": {"category": "æ•°æ®åº“", "difficulty": "ä¸­çº§"}
        },
        {
            "id": "cloud_computing_intro",
            "content": "äº‘è®¡ç®—æ˜¯é€šè¿‡äº’è”ç½‘æä¾›è®¡ç®—æœåŠ¡çš„æ¨¡å¼ï¼ŒåŒ…æ‹¬æœåŠ¡å™¨ã€å­˜å‚¨ã€æ•°æ®åº“ã€ç½‘ç»œã€è½¯ä»¶ç­‰ã€‚ä¸»è¦çš„äº‘æœåŠ¡æ¨¡å‹æœ‰IaaSï¼ˆåŸºç¡€è®¾æ–½å³æœåŠ¡ï¼‰ã€PaaSï¼ˆå¹³å°å³æœåŠ¡ï¼‰å’ŒSaaSï¼ˆè½¯ä»¶å³æœåŠ¡ï¼‰ã€‚AWSã€Azureã€Google Cloudæ˜¯ä¸»è¦çš„äº‘æœåŠ¡æä¾›å•†ã€‚",
            "metadata": {"category": "äº‘è®¡ç®—", "difficulty": "ä¸­çº§"}
        },
        {
            "id": "software_engineering_practices",
            "content": "è½¯ä»¶å·¥ç¨‹æ˜¯ä¸€é—¨å…³äºå¦‚ä½•ç³»ç»ŸåŒ–ã€è§„èŒƒåŒ–ã€å¯é‡åŒ–åœ°å¼€å‘è½¯ä»¶çš„å­¦ç§‘ã€‚è‰¯å¥½çš„è½¯ä»¶å·¥ç¨‹å®è·µåŒ…æ‹¬ç‰ˆæœ¬æ§åˆ¶ã€ä»£ç å®¡æŸ¥ã€å•å…ƒæµ‹è¯•ã€æŒç»­é›†æˆã€æ•æ·å¼€å‘ç­‰ã€‚è¿™äº›å®è·µæœ‰åŠ©äºæé«˜è½¯ä»¶è´¨é‡ã€é™ä½ç»´æŠ¤æˆæœ¬ã€æå‡å›¢é˜Ÿåä½œæ•ˆç‡ã€‚",
            "metadata": {"category": "è½¯ä»¶å·¥ç¨‹", "difficulty": "ä¸­çº§"}
        },
        {
            "id": "cybersecurity_basics",
            "content": "ç½‘ç»œå®‰å…¨æ˜¯ä¿æŠ¤è®¡ç®—æœºç³»ç»Ÿå’Œç½‘ç»œå…å—æ•°å­—æ”»å‡»çš„å®è·µã€‚å¸¸è§çš„å®‰å…¨å¨èƒåŒ…æ‹¬æ¶æ„è½¯ä»¶ã€é’“é±¼æ”»å‡»ã€æ•°æ®æ³„éœ²ç­‰ã€‚ç½‘ç»œå®‰å…¨æªæ–½åŒ…æ‹¬é˜²ç«å¢™ã€åŠ å¯†ã€èº«ä»½éªŒè¯ã€è®¿é—®æ§åˆ¶ç­‰ã€‚å®‰å…¨å¼€å‘ç”Ÿå‘½å‘¨æœŸï¼ˆSDLCï¼‰å°†å®‰å…¨è€ƒè™‘èå…¥è½¯ä»¶å¼€å‘çš„å„ä¸ªé˜¶æ®µã€‚",
            "metadata": {"category": "ç½‘ç»œå®‰å…¨", "difficulty": "é«˜çº§"}
        }
    ]


def demo_rag_system():
    """æ¼”ç¤ºRAGç³»ç»Ÿ"""
    print("=" * 60)
    print("ğŸ” RAGæ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿæ¼”ç¤º")
    print("=" * 60)
    
    # åˆ›å»ºRAGç³»ç»Ÿ
    rag = RAGSystem(vector_dim=256, similarity_threshold=0.05)
    
    # åˆ›å»ºç¤ºä¾‹æ–‡æ¡£
    print("\nğŸ“š å‡†å¤‡ç¤ºä¾‹æ–‡æ¡£...")
    documents = create_sample_documents()
    
    # æ·»åŠ æ–‡æ¡£åˆ°RAGç³»ç»Ÿ
    rag.add_documents(documents)
    
    # æ˜¾ç¤ºç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯
    stats = rag.get_statistics()
    print(f"\nğŸ“Š ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯:")
    for key, value in stats.items():
        print(f"  {key}: {value}")
    
    print("\n" + "=" * 40)
    print("ğŸ” å¼€å§‹æ£€ç´¢æ¼”ç¤º")
    print("=" * 40)
    
    # æµ‹è¯•æŸ¥è¯¢
    test_queries = [
        "Pythonç¼–ç¨‹è¯­è¨€çš„ç‰¹ç‚¹",
        "æœºå™¨å­¦ä¹ ç®—æ³•",
        "æ·±åº¦å­¦ä¹ ç¥ç»ç½‘ç»œ",
        "Webå¼€å‘æ¡†æ¶",
        "æ•°æ®åº“ç®¡ç†ç³»ç»Ÿ",
        "äº‘è®¡ç®—æœåŠ¡æ¨¡å‹",
        "ç½‘ç»œå®‰å…¨é˜²æŠ¤"
    ]
    
    for query in test_queries:
        print(f"\nğŸ” æŸ¥è¯¢: '{query}'")
        print("-" * 50)
        
        # æ‰§è¡Œæ£€ç´¢
        results = rag.search(query, top_k=3)
        
        if results:
            for result in results:
                doc = result.document
                print(f"ğŸ“„ æ–‡æ¡£ID: {doc.id}")
                print(f"ğŸ“Š ç›¸ä¼¼åº¦: {result.similarity:.4f}")
                print(f"ğŸ“ å†…å®¹: {doc.content[:100]}...")
                print(f"ğŸ·ï¸  ç±»åˆ«: {doc.metadata.get('category', 'N/A')}")
                print()
        else:
            print("âŒ æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
        
        # ç”Ÿæˆä¸Šä¸‹æ–‡
        context = rag.generate_context(query, max_context_length=300)
        print(f"ğŸ“‹ ç”Ÿæˆçš„ä¸Šä¸‹æ–‡:\n{context[:200]}...\n")
    
    # äº¤äº’å¼æŸ¥è¯¢
    print("\n" + "=" * 40)
    print("ğŸ’¬ äº¤äº’å¼æŸ¥è¯¢æ¨¡å¼")
    print("=" * 40)
    print("è¾“å…¥æŸ¥è¯¢å†…å®¹ï¼ˆè¾“å…¥ 'quit' é€€å‡ºï¼‰:")
    
    while True:
        try:
            user_query = input("\nğŸ” è¯·è¾“å…¥æŸ¥è¯¢: ").strip()
            
            if user_query.lower() in ['quit', 'exit', 'é€€å‡º', 'q']:
                print("ğŸ‘‹ é€€å‡ºæŸ¥è¯¢æ¨¡å¼")
                break
            
            if not user_query:
                continue
            
            # æ‰§è¡Œæ£€ç´¢
            results = rag.search(user_query, top_k=3)
            
            if results:
                print(f"\nğŸ“‹ æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£:")
                for i, result in enumerate(results, 1):
                    doc = result.document
                    print(f"\n{i}. ğŸ“„ {doc.id}")
                    print(f"   ğŸ“Š ç›¸ä¼¼åº¦: {result.similarity:.4f}")
                    print(f"   ğŸ·ï¸  ç±»åˆ«: {doc.metadata.get('category', 'N/A')}")
                    print(f"   ğŸ“ å†…å®¹: {doc.content}")
                
                # ç”Ÿæˆä¸Šä¸‹æ–‡ç”¨äºå›ç­”
                context = rag.generate_context(user_query)
                print(f"\nğŸ“‹ åŸºäºæ£€ç´¢ç»“æœçš„ä¸Šä¸‹æ–‡:")
                print(context)
            else:
                print("âŒ æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œè¯·å°è¯•å…¶ä»–å…³é”®è¯")
                
        except KeyboardInterrupt:
            print("\nğŸ‘‹ ç¨‹åºè¢«ä¸­æ–­ï¼Œé€€å‡º")
            break
        except Exception as e:
            print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")


def demo_vector_operations():
    """æ¼”ç¤ºå‘é‡æ“ä½œ"""
    print("\n" + "=" * 50)
    print("ğŸ§® å‘é‡æ“ä½œæ¼”ç¤º")
    print("=" * 50)
    
    # åˆ›å»ºç¤ºä¾‹æ–‡æœ¬
    texts = [
        "äººå·¥æ™ºèƒ½æ˜¯ä¸€é—¨è®¡ç®—æœºç§‘å­¦",
        "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„åˆ†æ”¯",
        "æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œ",
        "Pythonæ˜¯æµè¡Œçš„ç¼–ç¨‹è¯­è¨€",
        "æ•°æ®ç§‘å­¦åˆ†æå¤§é‡æ•°æ®"
    ]
    
    # åˆ›å»ºå‘é‡åŒ–å™¨
    vectorizer = TFIDFVectorizer(max_features=20)
    
    # è®­ç»ƒå¹¶è½¬æ¢
    print("\nğŸ“Š è®­ç»ƒTF-IDFå‘é‡åŒ–å™¨...")
    vectors = vectorizer.fit_transform(texts)
    
    print(f"è¯æ±‡è¡¨å¤§å°: {len(vectorizer.feature_names)}")
    print(f"å‘é‡ç»´åº¦: {len(vectors[0])}")
    
    # æ˜¾ç¤ºç‰¹å¾è¯æ±‡
    print(f"\nğŸ”¤ ç‰¹å¾è¯æ±‡: {vectorizer.feature_names[:10]}...")
    
    # æ˜¾ç¤ºå‘é‡
    print(f"\nğŸ“ˆ æ–‡æœ¬å‘é‡:")
    for i, (text, vector) in enumerate(zip(texts, vectors)):
        non_zero_features = [(vectorizer.feature_names[j], vector[j]) 
                           for j in range(len(vector)) if vector[j] > 0]
        print(f"{i+1}. '{text}'")
        print(f"   éé›¶ç‰¹å¾: {non_zero_features[:3]}...")
    
    # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
    print(f"\nğŸ“ ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µ:")
    print("    ", end="")
    for i in range(len(texts)):
        print(f"{i+1:6}", end="")
    print()
    
    for i in range(len(vectors)):
        print(f"{i+1:2}: ", end="")
        for j in range(len(vectors)):
            similarity = VectorSimilarity.cosine_similarity(vectors[i], vectors[j])
            print(f"{similarity:5.3f} ", end="")
        print()
    
    # æŸ¥è¯¢ç›¸ä¼¼åº¦
    print(f"\nğŸ” æŸ¥è¯¢ç›¸ä¼¼åº¦æµ‹è¯•:")
    query_text = "æœºå™¨å­¦ä¹ ç®—æ³•"
    query_vector = vectorizer.transform(query_text)
    
    print(f"æŸ¥è¯¢: '{query_text}'")
    similarities = []
    for i, (text, vector) in enumerate(zip(texts, vectors)):
        similarity = VectorSimilarity.cosine_similarity(query_vector, vector)
        similarities.append((i, text, similarity))
    
    # æŒ‰ç›¸ä¼¼åº¦æ’åº
    similarities.sort(key=lambda x: x[2], reverse=True)
    
    print("ç›¸ä¼¼åº¦æ’åºç»“æœ:")
    for rank, (idx, text, sim) in enumerate(similarities, 1):
        print(f"{rank}. {sim:.4f} - '{text}'")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ RAGä¸å‘é‡æ•°æ®å®ç°æ¼”ç¤º")
    print("é€‰æ‹©æ¼”ç¤ºæ¨¡å¼:")
    print("1. RAGæ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿæ¼”ç¤º")
    print("2. å‘é‡æ“ä½œåŸºç¡€æ¼”ç¤º")
    print("3. å®Œæ•´æ¼”ç¤ºï¼ˆåŒ…å«ä¸¤ä¸ªéƒ¨åˆ†ï¼‰")
    
    try:
        choice = input("\nè¯·é€‰æ‹© (1-3): ").strip()
        
        if choice == '1':
            demo_rag_system()
        elif choice == '2':
            demo_vector_operations()
        elif choice == '3':
            demo_vector_operations()
            demo_rag_system()
        else:
            print("æ— æ•ˆé€‰æ‹©ï¼Œè¿è¡Œå®Œæ•´æ¼”ç¤º")
            demo_vector_operations()
            demo_rag_system()
            
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç¨‹åºè¢«ä¸­æ–­ï¼Œå†è§ï¼")
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")


if __name__ == "__main__":
    main()