# -*- coding: utf-8 -*-
"""
æ·±åº¦ç ”ç©¶Agentç³»ç»Ÿ (DeepResearch Agent)
åŸºäºLLMé©±åŠ¨çš„æ™ºèƒ½ç ”ç©¶åŠ©æ‰‹ï¼Œå…·å¤‡å¤šé˜¶æ®µæ¨ç†ã€çŸ¥è¯†å›¾è°±æ„å»ºã€è‡ªé€‚åº”å­¦ä¹ ç­‰èƒ½åŠ›

ä¸»è¦ç‰¹æ€§:
1. å¤šé˜¶æ®µç ”ç©¶ç®¡é“ - é—®é¢˜åˆ†æã€ä¿¡æ¯æ”¶é›†ã€æ·±åº¦åˆ†æã€ç»¼åˆæ¨ç†
2. çŸ¥è¯†å›¾è°±æ„å»º - åŠ¨æ€æ„å»ºå’Œæ›´æ–°é¢†åŸŸçŸ¥è¯†å›¾è°±
3. è‡ªé€‚åº”æ¨ç†æœºåˆ¶ - æ ¹æ®é—®é¢˜å¤æ‚åº¦é€‰æ‹©åˆé€‚çš„æ¨ç†ç­–ç•¥
4. æŒç»­å­¦ä¹ èƒ½åŠ› - ä»ç ”ç©¶è¿‡ç¨‹ä¸­å­¦ä¹ å’Œæ”¹è¿›
5. å¤šæ¨¡æ€ä¿¡æ¯èåˆ - æ•´åˆæ–‡æœ¬ã€æ•°æ®ã€ç»“æ„åŒ–çŸ¥è¯†
"""

import json
import time
import random
import hashlib
import re
import math
from typing import Dict, List, Any, Optional, Union, Callable, Tuple, Set
from dataclasses import dataclass, field
from abc import ABC, abstractmethod
from datetime import datetime, timedelta
from collections import defaultdict, deque
from enum import Enum
import sqlite3


class ResearchPhase(Enum):
    """ç ”ç©¶é˜¶æ®µæšä¸¾"""
    PROBLEM_ANALYSIS = "problem_analysis"
    INFORMATION_GATHERING = "information_gathering"
    DEEP_ANALYSIS = "deep_analysis"
    SYNTHESIS = "synthesis"
    VALIDATION = "validation"
    CONCLUSION = "conclusion"


class ReasoningMode(Enum):
    """æ¨ç†æ¨¡å¼æšä¸¾"""
    QUICK = "quick"           # å¿«é€Ÿæ¨ç†
    THOROUGH = "thorough"     # å½»åº•æ¨ç†
    CREATIVE = "creative"     # åˆ›æ„æ¨ç†
    ANALYTICAL = "analytical" # åˆ†ææ¨ç†
    SYSTEMATIC = "systematic" # ç³»ç»Ÿæ¨ç†


@dataclass
class ResearchQuery:
    """ç ”ç©¶æŸ¥è¯¢ç»“æ„"""
    query: str
    domain: str = "general"
    complexity: int = 1  # 1-5å¤æ‚åº¦ç­‰çº§
    urgency: int = 1     # 1-5ç´§æ€¥ç¨‹åº¦
    depth_required: int = 3  # 1-5æ·±åº¦è¦æ±‚
    timestamp: datetime = field(default_factory=datetime.now)
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class ResearchStep:
    """ç ”ç©¶æ­¥éª¤è®°å½•"""
    phase: ResearchPhase
    step_type: str  # "thought", "action", "observation", "insight"
    content: str
    confidence: float = 0.0
    sources: List[str] = field(default_factory=list)
    timestamp: datetime = field(default_factory=datetime.now)
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class KnowledgeNode:
    """çŸ¥è¯†å›¾è°±èŠ‚ç‚¹"""
    id: str
    content: str
    node_type: str  # "concept", "fact", "relation", "hypothesis"
    domain: str
    confidence: float = 0.0
    connections: Set[str] = field(default_factory=set)
    evidence: List[str] = field(default_factory=list)
    timestamp: datetime = field(default_factory=datetime.now)


@dataclass
class ResearchInsight:
    """ç ”ç©¶æ´å¯Ÿ"""
    content: str
    insight_type: str  # "pattern", "contradiction", "gap", "connection"
    confidence: float
    supporting_evidence: List[str] = field(default_factory=list)
    implications: List[str] = field(default_factory=list)
    timestamp: datetime = field(default_factory=datetime.now)


class IntelligentReasoner:
    """æ™ºèƒ½æ¨ç†å™¨ - è‡ªé€‚åº”é€‰æ‹©æ¨ç†ç­–ç•¥"""
    
    def __init__(self):
        self.reasoning_patterns = {
            ReasoningMode.QUICK: self._quick_reasoning,
            ReasoningMode.THOROUGH: self._thorough_reasoning, 
            ReasoningMode.CREATIVE: self._creative_reasoning,
            ReasoningMode.ANALYTICAL: self._analytical_reasoning,
            ReasoningMode.SYSTEMATIC: self._systematic_reasoning
        }
        self.performance_history = defaultdict(list)
    
    def select_reasoning_mode(self, query: ResearchQuery, context: Dict[str, Any]) -> ReasoningMode:
        """æ™ºèƒ½é€‰æ‹©æ¨ç†æ¨¡å¼"""
        # åŸºäºæŸ¥è¯¢ç‰¹å¾é€‰æ‹©æ¨ç†æ¨¡å¼
        if query.complexity <= 2 and query.urgency >= 4:
            return ReasoningMode.QUICK
        elif query.depth_required >= 4:
            return ReasoningMode.THOROUGH
        elif "åˆ›æ–°" in query.query or "æ–°" in query.query:
            return ReasoningMode.CREATIVE
        elif "åˆ†æ" in query.query or "æ¯”è¾ƒ" in query.query:
            return ReasoningMode.ANALYTICAL
        else:
            return ReasoningMode.SYSTEMATIC
    
    def reason(self, query: ResearchQuery, context: Dict[str, Any], mode: Optional[ReasoningMode] = None) -> List[str]:
        """æ‰§è¡Œæ¨ç†"""
        if mode is None:
            mode = self.select_reasoning_mode(query, context)
        
        return self.reasoning_patterns[mode](query, context)
    
    def _quick_reasoning(self, query: ResearchQuery, context: Dict[str, Any]) -> List[str]:
        """å¿«é€Ÿæ¨ç†æ¨¡å¼"""
        reasoning_steps = [
            f"å¿«é€Ÿåˆ†æé—®é¢˜: {query.query}",
            "è¯†åˆ«æ ¸å¿ƒå…³é”®è¯å’Œæ¦‚å¿µ",
            "è°ƒç”¨å·²æœ‰çŸ¥è¯†è¿›è¡Œç›´æ¥åŒ¹é…",
            "ç”Ÿæˆåˆæ­¥ç­”æ¡ˆ"
        ]
        return reasoning_steps
    
    def _thorough_reasoning(self, query: ResearchQuery, context: Dict[str, Any]) -> List[str]:
        """å½»åº•æ¨ç†æ¨¡å¼"""
        reasoning_steps = [
            f"æ·±å…¥åˆ†æé—®é¢˜çš„å¤šä¸ªç»´åº¦: {query.query}",
            "åˆ†è§£é—®é¢˜ä¸ºå­é—®é¢˜",
            "ç³»ç»Ÿæ€§æ”¶é›†ç›¸å…³ä¿¡æ¯",
            "å¤šè§’åº¦åˆ†ææ¯ä¸ªå­é—®é¢˜",
            "ç»¼åˆåˆ†æç»“æœ",
            "éªŒè¯æ¨ç†é€»è¾‘",
            "å½¢æˆå…¨é¢ç»“è®º"
        ]
        return reasoning_steps
    
    def _creative_reasoning(self, query: ResearchQuery, context: Dict[str, Any]) -> List[str]:
        """åˆ›æ„æ¨ç†æ¨¡å¼"""
        reasoning_steps = [
            f"ä»åˆ›æ–°è§’åº¦é‡æ–°å®¡è§†é—®é¢˜: {query.query}",
            "å¯»æ‰¾éä¼ ç»Ÿçš„æ€è€ƒè§’åº¦",
            "è”æƒ³ç›¸å…³ä½†ä¸ç›´æ¥çš„é¢†åŸŸçŸ¥è¯†",
            "ç”Ÿæˆå¤šä¸ªå‡è®¾æ€§æ–¹æ¡ˆ",
            "è¯„ä¼°åˆ›æ–°æ–¹æ¡ˆçš„å¯è¡Œæ€§",
            "æ•´åˆæœ€æœ‰æ½œåŠ›çš„åˆ›æ–°æƒ³æ³•"
        ]
        return reasoning_steps
    
    def _analytical_reasoning(self, query: ResearchQuery, context: Dict[str, Any]) -> List[str]:
        """åˆ†ææ¨ç†æ¨¡å¼"""
        reasoning_steps = [
            f"ç³»ç»Ÿåˆ†æé—®é¢˜ç»“æ„: {query.query}",
            "è¯†åˆ«å˜é‡å’Œå½±å“å› ç´ ",
            "å»ºç«‹å› æœå…³ç³»æ¨¡å‹",
            "é‡åŒ–åˆ†æå„å› ç´ æƒé‡",
            "å¯¹æ¯”ä¸åŒæ–¹æ¡ˆçš„ä¼˜åŠ£",
            "å¾—å‡ºåŸºäºæ•°æ®çš„ç»“è®º"
        ]
        return reasoning_steps
    
    def _systematic_reasoning(self, query: ResearchQuery, context: Dict[str, Any]) -> List[str]:
        """ç³»ç»Ÿæ¨ç†æ¨¡å¼"""
        reasoning_steps = [
            f"ç³»ç»Ÿæ€§åœ°æ¢³ç†é—®é¢˜: {query.query}",
            "æ„å»ºé—®é¢˜çš„æ¦‚å¿µæ¡†æ¶",
            "æŒ‰é€»è¾‘é¡ºåºæ”¶é›†ä¿¡æ¯",
            "å»ºç«‹çŸ¥è¯†ç»“æ„å›¾",
            "è¿›è¡Œç»“æ„åŒ–åˆ†æ",
            "å½¢æˆç³»ç»Ÿæ€§ç»“è®º"
        ]
        return reasoning_steps


class KnowledgeGraph:
    """åŠ¨æ€çŸ¥è¯†å›¾è°±"""
    
    def __init__(self):
        self.nodes: Dict[str, KnowledgeNode] = {}
        self.connections: Dict[str, Dict[str, float]] = defaultdict(dict)  # è¿æ¥æƒé‡
        self.domains: Dict[str, Set[str]] = defaultdict(set)
        self.update_history = []
    
    def add_node(self, node: KnowledgeNode) -> str:
        """æ·»åŠ çŸ¥è¯†èŠ‚ç‚¹"""
        self.nodes[node.id] = node
        self.domains[node.domain].add(node.id)
        
        # è‡ªåŠ¨å‘ç°è¿æ¥
        self._discover_connections(node)
        
        self.update_history.append({
            "action": "add_node",
            "node_id": node.id,
            "timestamp": datetime.now()
        })
        
        return node.id
    
    def add_connection(self, node1_id: str, node2_id: str, weight: float = 1.0, relation_type: str = "related"):
        """æ·»åŠ èŠ‚ç‚¹è¿æ¥"""
        if node1_id in self.nodes and node2_id in self.nodes:
            self.connections[node1_id][node2_id] = weight
            self.connections[node2_id][node1_id] = weight
            
            # æ›´æ–°èŠ‚ç‚¹è¿æ¥é›†åˆ
            self.nodes[node1_id].connections.add(node2_id)
            self.nodes[node2_id].connections.add(node1_id)
    
    def _discover_connections(self, new_node: KnowledgeNode):
        """è‡ªåŠ¨å‘ç°èŠ‚ç‚¹è¿æ¥"""
        for existing_id, existing_node in self.nodes.items():
            if existing_id != new_node.id:
                # è®¡ç®—ç›¸ä¼¼åº¦
                similarity = self._calculate_similarity(new_node.content, existing_node.content)
                
                if similarity > 0.3:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                    self.add_connection(new_node.id, existing_id, similarity)
    
    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦"""
        words1 = set(re.findall(r'\w+', text1.lower()))
        words2 = set(re.findall(r'\w+', text2.lower()))
        
        if not words1 or not words2:
            return 0.0
        
        intersection = len(words1 & words2)
        union = len(words1 | words2)
        
        return intersection / union if union > 0 else 0.0
    
    def get_related_nodes(self, node_id: str, max_distance: int = 2) -> List[KnowledgeNode]:
        """è·å–ç›¸å…³èŠ‚ç‚¹"""
        if node_id not in self.nodes:
            return []
        
        related = []
        visited = set()
        queue = [(node_id, 0)]
        
        while queue:
            current_id, distance = queue.pop(0)
            
            if current_id in visited or distance > max_distance:
                continue
            
            visited.add(current_id)
            
            if distance > 0:  # ä¸åŒ…æ‹¬è‡ªå·±
                related.append(self.nodes[current_id])
            
            # æ·»åŠ é‚»å±…èŠ‚ç‚¹
            for neighbor_id in self.nodes[current_id].connections:
                if neighbor_id not in visited:
                    queue.append((neighbor_id, distance + 1))
        
        # æŒ‰è¿æ¥æƒé‡æ’åº
        related.sort(key=lambda node: self.connections.get(node_id, {}).get(node.id, 0), reverse=True)
        
        return related
    
    def search_nodes(self, query: str, domain: Optional[str] = None) -> List[KnowledgeNode]:
        """æœç´¢ç›¸å…³èŠ‚ç‚¹"""
        query_words = set(re.findall(r'\w+', query.lower()))
        results = []
        
        for node in self.nodes.values():
            if domain and node.domain != domain:
                continue
            
            node_words = set(re.findall(r'\w+', node.content.lower()))
            similarity = len(query_words & node_words) / len(query_words | node_words) if query_words | node_words else 0
            
            if similarity > 0.1:  # æœ€ä½ç›¸ä¼¼åº¦é˜ˆå€¼
                results.append((node, similarity))
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        results.sort(key=lambda x: x[1], reverse=True)
        return [node for node, _ in results[:10]]  # è¿”å›å‰10ä¸ªç»“æœ
    
    def get_domain_overview(self, domain: str) -> Dict[str, Any]:
        """è·å–é¢†åŸŸæ¦‚è§ˆ"""
        if domain not in self.domains:
            return {}
        
        node_ids = self.domains[domain]
        nodes = [self.nodes[nid] for nid in node_ids]
        
        # ç»Ÿè®¡ä¿¡æ¯
        node_types = defaultdict(int)
        avg_confidence = 0
        
        for node in nodes:
            node_types[node.node_type] += 1
            avg_confidence += node.confidence
        
        avg_confidence /= len(nodes) if nodes else 1
        
        # æ‰¾åˆ°ä¸­å¿ƒèŠ‚ç‚¹ï¼ˆè¿æ¥æœ€å¤šçš„èŠ‚ç‚¹ï¼‰
        central_nodes = sorted(nodes, key=lambda n: len(n.connections), reverse=True)[:3]
        
        return {
            "domain": domain,
            "total_nodes": len(nodes),
            "node_types": dict(node_types),
            "average_confidence": avg_confidence,
            "central_nodes": [{"id": n.id, "content": n.content[:100]} for n in central_nodes],
            "last_updated": max([n.timestamp for n in nodes]) if nodes else None
        }


class ResearchPipeline:
    """å¤šé˜¶æ®µç ”ç©¶ç®¡é“"""
    
    def __init__(self, knowledge_graph: KnowledgeGraph, reasoner: IntelligentReasoner):
        self.knowledge_graph = knowledge_graph
        self.reasoner = reasoner
        self.research_tools = self._initialize_tools()
        self.phase_handlers = {
            ResearchPhase.PROBLEM_ANALYSIS: self._analyze_problem,
            ResearchPhase.INFORMATION_GATHERING: self._gather_information,
            ResearchPhase.DEEP_ANALYSIS: self._deep_analysis,
            ResearchPhase.SYNTHESIS: self._synthesize,
            ResearchPhase.VALIDATION: self._validate,
            ResearchPhase.CONCLUSION: self._conclude
        }
    
    def _initialize_tools(self) -> Dict[str, Callable]:
        """åˆå§‹åŒ–ç ”ç©¶å·¥å…·"""
        return {
            "web_search": self._web_search_tool,
            "literature_search": self._literature_search_tool,
            "data_analysis": self._data_analysis_tool,
            "expert_knowledge": self._expert_knowledge_tool,
            "trend_analysis": self._trend_analysis_tool
        }
    
    def execute_research(self, query: ResearchQuery) -> Dict[str, Any]:
        """æ‰§è¡Œå®Œæ•´ç ”ç©¶æµç¨‹"""
        research_context = {
            "query": query,
            "steps": [],
            "insights": [],
            "knowledge_used": [],
            "tools_called": [],
            "confidence_scores": []
        }
        
        # æŒ‰é˜¶æ®µæ‰§è¡Œç ”ç©¶
        for phase in ResearchPhase:
            phase_result = self.phase_handlers[phase](query, research_context)
            research_context["steps"].extend(phase_result["steps"])
            research_context["insights"].extend(phase_result.get("insights", []))
            research_context["tools_called"].extend(phase_result.get("tools_called", []))
            
            # æ›´æ–°çŸ¥è¯†å›¾è°±
            if "new_knowledge" in phase_result:
                for knowledge in phase_result["new_knowledge"]:
                    self._add_to_knowledge_graph(knowledge, query.domain)
        
        return research_context
    
    def _analyze_problem(self, query: ResearchQuery, context: Dict[str, Any]) -> Dict[str, Any]:
        """é—®é¢˜åˆ†æé˜¶æ®µ"""
        steps = []
        insights = []
        
        # åˆ†æé—®é¢˜å¤æ‚åº¦
        complexity_factors = self._assess_complexity(query.query)
        steps.append(ResearchStep(
            phase=ResearchPhase.PROBLEM_ANALYSIS,
            step_type="analysis",
            content=f"é—®é¢˜å¤æ‚åº¦è¯„ä¼°: {complexity_factors}",
            confidence=0.8
        ))
        
        # è¯†åˆ«å…³é”®æ¦‚å¿µ
        key_concepts = self._extract_key_concepts(query.query)
        steps.append(ResearchStep(
            phase=ResearchPhase.PROBLEM_ANALYSIS,
            step_type="insight",
            content=f"è¯†åˆ«å…³é”®æ¦‚å¿µ: {key_concepts}",
            confidence=0.9
        ))
        
        # ç¡®å®šç ”ç©¶èŒƒå›´
        scope = self._determine_research_scope(query, key_concepts)
        steps.append(ResearchStep(
            phase=ResearchPhase.PROBLEM_ANALYSIS,
            step_type="planning",
            content=f"ç¡®å®šç ”ç©¶èŒƒå›´: {scope}",
            confidence=0.7
        ))
        
        return {
            "steps": steps,
            "insights": insights,
            "key_concepts": key_concepts,
            "scope": scope
        }
    
    def _gather_information(self, query: ResearchQuery, context: Dict[str, Any]) -> Dict[str, Any]:
        """ä¿¡æ¯æ”¶é›†é˜¶æ®µ"""
        steps = []
        tools_called = []
        new_knowledge = []
        
        # ä»çŸ¥è¯†å›¾è°±æœç´¢ç›¸å…³ä¿¡æ¯
        related_nodes = self.knowledge_graph.search_nodes(query.query, query.domain)
        steps.append(ResearchStep(
            phase=ResearchPhase.INFORMATION_GATHERING,
            step_type="action",
            content=f"ä»çŸ¥è¯†å›¾è°±æ£€ç´¢åˆ° {len(related_nodes)} ä¸ªç›¸å…³èŠ‚ç‚¹",
            confidence=0.8
        ))
        
        # æ¨¡æ‹Ÿä½¿ç”¨å„ç§å·¥å…·æ”¶é›†ä¿¡æ¯
        information_sources = ["web_search", "literature_search", "expert_knowledge"]
        
        for tool_name in information_sources:
            if tool_name in self.research_tools:
                tool_result = self.research_tools[tool_name](query.query)
                steps.append(ResearchStep(
                    phase=ResearchPhase.INFORMATION_GATHERING,
                    step_type="action",
                    content=f"ä½¿ç”¨{tool_name}å·¥å…·: {tool_result}",
                    confidence=0.7
                ))
                tools_called.append(tool_name)
                
                # å°†å·¥å…·ç»“æœè½¬æ¢ä¸ºçŸ¥è¯†
                new_knowledge.append({
                    "content": tool_result,
                    "source": tool_name,
                    "confidence": 0.7
                })
        
        return {
            "steps": steps,
            "tools_called": tools_called,
            "new_knowledge": new_knowledge
        }
    
    def _deep_analysis(self, query: ResearchQuery, context: Dict[str, Any]) -> Dict[str, Any]:
        """æ·±åº¦åˆ†æé˜¶æ®µ"""
        steps = []
        insights = []
        
        # ä½¿ç”¨æ™ºèƒ½æ¨ç†å™¨è¿›è¡Œæ·±åº¦åˆ†æ
        reasoning_steps = self.reasoner.reason(query, context)
        
        for i, reasoning_step in enumerate(reasoning_steps):
            steps.append(ResearchStep(
                phase=ResearchPhase.DEEP_ANALYSIS,
                step_type="thought",
                content=reasoning_step,
                confidence=0.8 - i * 0.05  # ç½®ä¿¡åº¦éšæ¨ç†æ·±åº¦é€’å‡
            ))
        
        # è¯†åˆ«æ¨¡å¼å’Œå…³è”
        patterns = self._identify_patterns(context)
        if patterns:
            insights.append(ResearchInsight(
                content=f"å‘ç°å…³é”®æ¨¡å¼: {patterns}",
                insight_type="pattern",
                confidence=0.7
            ))
        
        # å‘ç°çŸ¥è¯†ç¼ºå£
        gaps = self._identify_knowledge_gaps(query, context)
        if gaps:
            insights.append(ResearchInsight(
                content=f"è¯†åˆ«çŸ¥è¯†ç¼ºå£: {gaps}",
                insight_type="gap",
                confidence=0.6
            ))
        
        return {
            "steps": steps,
            "insights": insights
        }
    
    def _synthesize(self, query: ResearchQuery, context: Dict[str, Any]) -> Dict[str, Any]:
        """ç»¼åˆé˜¶æ®µ"""
        steps = []
        insights = []
        
        # æ•´åˆæ‰€æœ‰ä¿¡æ¯
        synthesis_content = self._integrate_information(context)
        steps.append(ResearchStep(
            phase=ResearchPhase.SYNTHESIS,
            step_type="synthesis",
            content=synthesis_content,
            confidence=0.8
        ))
        
        # ç”Ÿæˆæ–°çš„æ´å¯Ÿ
        new_insights = self._generate_insights(context)
        insights.extend(new_insights)
        
        return {
            "steps": steps,
            "insights": insights
        }
    
    def _validate(self, query: ResearchQuery, context: Dict[str, Any]) -> Dict[str, Any]:
        """éªŒè¯é˜¶æ®µ"""
        steps = []
        
        # é€»è¾‘ä¸€è‡´æ€§æ£€æŸ¥
        consistency_score = self._check_logical_consistency(context)
        steps.append(ResearchStep(
            phase=ResearchPhase.VALIDATION,
            step_type="validation",
            content=f"é€»è¾‘ä¸€è‡´æ€§è¯„åˆ†: {consistency_score:.2f}",
            confidence=consistency_score
        ))
        
        # è¯æ®æ”¯æŒåº¦æ£€æŸ¥
        evidence_score = self._assess_evidence_support(context)
        steps.append(ResearchStep(
            phase=ResearchPhase.VALIDATION,
            step_type="validation", 
            content=f"è¯æ®æ”¯æŒåº¦è¯„åˆ†: {evidence_score:.2f}",
            confidence=evidence_score
        ))
        
        return {
            "steps": steps
        }
    
    def _conclude(self, query: ResearchQuery, context: Dict[str, Any]) -> Dict[str, Any]:
        """ç»“è®ºé˜¶æ®µ"""
        steps = []
        
        # ç”Ÿæˆæœ€ç»ˆç»“è®º
        conclusion = self._generate_conclusion(query, context)
        steps.append(ResearchStep(
            phase=ResearchPhase.CONCLUSION,
            step_type="conclusion",
            content=conclusion,
            confidence=0.9
        ))
        
        # æå‡ºåç»­ç ”ç©¶æ–¹å‘
        future_directions = self._suggest_future_research(query, context)
        steps.append(ResearchStep(
            phase=ResearchPhase.CONCLUSION,
            step_type="suggestion",
            content=f"åç»­ç ”ç©¶å»ºè®®: {future_directions}",
            confidence=0.7
        ))
        
        return {
            "steps": steps
        }
    
    # å·¥å…·å®ç°
    def _web_search_tool(self, query: str) -> str:
        """æ¨¡æ‹Ÿç½‘ç»œæœç´¢å·¥å…·"""
        search_results = {
            "äººå·¥æ™ºèƒ½": "AIæŠ€æœ¯æ­£åœ¨å¿«é€Ÿå‘å±•ï¼Œåœ¨å„ä¸ªé¢†åŸŸéƒ½æœ‰å¹¿æ³›åº”ç”¨",
            "æœºå™¨å­¦ä¹ ": "æœºå™¨å­¦ä¹ æ˜¯AIçš„æ ¸å¿ƒæŠ€æœ¯ï¼ŒåŒ…æ‹¬ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ ç­‰",
            "æ·±åº¦å­¦ä¹ ": "æ·±åº¦å­¦ä¹ åŸºäºç¥ç»ç½‘ç»œï¼Œåœ¨å›¾åƒè¯†åˆ«å’Œè‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸè¡¨ç°å‡ºè‰²",
            "python": "Pythonæ˜¯æœ€å—æ¬¢è¿çš„ç¼–ç¨‹è¯­è¨€ä¹‹ä¸€ï¼Œç‰¹åˆ«é€‚åˆæ•°æ®ç§‘å­¦å’ŒAIå¼€å‘"
        }
        
        for key in search_results:
            if key in query.lower():
                return f"ç½‘ç»œæœç´¢ç»“æœ: {search_results[key]}"
        
        return f"ç½‘ç»œæœç´¢ç»“æœ: å…³äº'{query}'çš„æœ€æ–°ä¿¡æ¯å’Œè§‚ç‚¹"
    
    def _literature_search_tool(self, query: str) -> str:
        """æ¨¡æ‹Ÿæ–‡çŒ®æœç´¢å·¥å…·"""
        return f"æ–‡çŒ®æœç´¢ç»“æœ: æ‰¾åˆ°ä¸'{query}'ç›¸å…³çš„å­¦æœ¯è®ºæ–‡å’Œç ”ç©¶æŠ¥å‘Š"
    
    def _data_analysis_tool(self, query: str) -> str:
        """æ¨¡æ‹Ÿæ•°æ®åˆ†æå·¥å…·"""
        return f"æ•°æ®åˆ†æç»“æœ: å¯¹'{query}'ç›¸å…³æ•°æ®è¿›è¡Œç»Ÿè®¡åˆ†æï¼Œå‘ç°é‡è¦è¶‹åŠ¿"
    
    def _expert_knowledge_tool(self, query: str) -> str:
        """æ¨¡æ‹Ÿä¸“å®¶çŸ¥è¯†å·¥å…·"""
        return f"ä¸“å®¶çŸ¥è¯†: é¢†åŸŸä¸“å®¶å¯¹'{query}'çš„ä¸“ä¸šè§è§£å’Œç»éªŒåˆ†äº«"
    
    def _trend_analysis_tool(self, query: str) -> str:
        """æ¨¡æ‹Ÿè¶‹åŠ¿åˆ†æå·¥å…·"""
        return f"è¶‹åŠ¿åˆ†æ: '{query}'é¢†åŸŸçš„å‘å±•è¶‹åŠ¿å’Œæœªæ¥é¢„æµ‹"
    
    # åˆ†ææ–¹æ³•
    def _assess_complexity(self, query: str) -> Dict[str, Any]:
        """è¯„ä¼°é—®é¢˜å¤æ‚åº¦"""
        factors = {
            "query_length": len(query.split()),
            "question_types": len(re.findall(r'[?ï¼Ÿ]', query)),
            "technical_terms": len(re.findall(r'[A-Z][a-z]+|æŠ€æœ¯|ç®—æ³•|æ¨¡å‹', query)),
            "complexity_score": min(5, (len(query.split()) + len(re.findall(r'[?ï¼Ÿ]', query)) * 2) / 10)
        }
        return factors
    
    def _extract_key_concepts(self, query: str) -> List[str]:
        """æå–å…³é”®æ¦‚å¿µ"""
        # ç®€å•çš„å…³é”®è¯æå–
        words = re.findall(r'\w+', query)
        # è¿‡æ»¤å¸¸è§è¯æ±‡ï¼Œä¿ç•™é‡è¦æ¦‚å¿µ
        important_words = [w for w in words if len(w) > 3 and w not in ['ä»€ä¹ˆ', 'å¦‚ä½•', 'ä¸ºä»€ä¹ˆ', 'æ€ä¹ˆ']]
        return important_words[:5]  # è¿”å›å‰5ä¸ªé‡è¦è¯æ±‡
    
    def _determine_research_scope(self, query: ResearchQuery, key_concepts: List[str]) -> Dict[str, Any]:
        """ç¡®å®šç ”ç©¶èŒƒå›´"""
        return {
            "primary_domain": query.domain,
            "key_concepts": key_concepts,
            "depth_level": query.depth_required,
            "estimated_time": query.complexity * 10,  # åˆ†é’Ÿ
            "required_tools": ["web_search", "literature_search"]
        }
    
    def _identify_patterns(self, context: Dict[str, Any]) -> List[str]:
        """è¯†åˆ«æ¨¡å¼"""
        # æ¨¡æ‹Ÿæ¨¡å¼è¯†åˆ«
        patterns = [
            "æŠ€æœ¯å‘å±•å‘ˆæŒ‡æ•°å¢é•¿è¶‹åŠ¿",
            "è·¨é¢†åŸŸåº”ç”¨è¶Šæ¥è¶Šæ™®é",
            "å¼€æºç¤¾åŒºæ¨åŠ¨åˆ›æ–°"
        ]
        return random.sample(patterns, random.randint(1, 2))
    
    def _identify_knowledge_gaps(self, query: ResearchQuery, context: Dict[str, Any]) -> List[str]:
        """è¯†åˆ«çŸ¥è¯†ç¼ºå£"""
        gaps = [
            "ç¼ºä¹æœ€æ–°çš„å®è¯ç ”ç©¶æ•°æ®",
            "éœ€è¦æ›´å¤šçš„æ¡ˆä¾‹ç ”ç©¶",
            "ç†è®ºä¸å®è·µä¹‹é—´çš„æ¡¥æ¢å¾…å»ºç«‹"
        ]
        return random.sample(gaps, random.randint(1, 2))
    
    def _integrate_information(self, context: Dict[str, Any]) -> str:
        """æ•´åˆä¿¡æ¯"""
        step_count = len(context["steps"])
        insight_count = len(context["insights"])
        tool_count = len(set(context["tools_called"]))
        
        return f"æ•´åˆäº† {step_count} ä¸ªç ”ç©¶æ­¥éª¤ï¼Œå‘ç° {insight_count} ä¸ªå…³é”®æ´å¯Ÿï¼Œä½¿ç”¨äº† {tool_count} ç§å·¥å…·"
    
    def _generate_insights(self, context: Dict[str, Any]) -> List[ResearchInsight]:
        """ç”Ÿæˆæ´å¯Ÿ"""
        insights = []
        
        # åŸºäºç ”ç©¶æ­¥éª¤ç”Ÿæˆæ´å¯Ÿ
        if len(context["steps"]) > 10:
            insights.append(ResearchInsight(
                content="ç ”ç©¶è¿‡ç¨‹æ­ç¤ºäº†é—®é¢˜çš„å¤šå±‚æ¬¡ç»“æ„",
                insight_type="connection",
                confidence=0.8
            ))
        
        if len(context["tools_called"]) > 2:
            insights.append(ResearchInsight(
                content="å¤šå·¥å…·èåˆæä¾›äº†æ›´å…¨é¢çš„è§†è§’",
                insight_type="pattern",
                confidence=0.7
            ))
        
        return insights
    
    def _check_logical_consistency(self, context: Dict[str, Any]) -> float:
        """æ£€æŸ¥é€»è¾‘ä¸€è‡´æ€§"""
        # æ¨¡æ‹Ÿé€»è¾‘ä¸€è‡´æ€§æ£€æŸ¥
        return random.uniform(0.7, 0.95)
    
    def _assess_evidence_support(self, context: Dict[str, Any]) -> float:
        """è¯„ä¼°è¯æ®æ”¯æŒåº¦"""
        # åŸºäºå·¥å…·è°ƒç”¨æ•°é‡å’Œæ´å¯Ÿæ•°é‡è¯„ä¼°
        tool_score = min(1.0, len(set(context["tools_called"])) / 3)
        insight_score = min(1.0, len(context["insights"]) / 5)
        return (tool_score + insight_score) / 2
    
    def _generate_conclusion(self, query: ResearchQuery, context: Dict[str, Any]) -> str:
        """ç”Ÿæˆç»“è®º"""
        key_findings = len(context["insights"])
        confidence = sum(step.confidence for step in context["steps"]) / len(context["steps"])
        
        return f"åŸºäºæ·±åº¦ç ”ç©¶åˆ†æï¼Œå¯¹é—®é¢˜'{query.query}'çš„ç ”ç©¶å‘ç°äº† {key_findings} ä¸ªå…³é”®æ´å¯Ÿï¼Œæ•´ä½“ç½®ä¿¡åº¦ä¸º {confidence:.2f}ã€‚ç ”ç©¶è¡¨æ˜è¯¥é—®é¢˜å…·æœ‰å¤šç»´åº¦ç‰¹å¾ï¼Œéœ€è¦ç»¼åˆè€ƒè™‘å¤šä¸ªå› ç´ ã€‚"
    
    def _suggest_future_research(self, query: ResearchQuery, context: Dict[str, Any]) -> str:
        """å»ºè®®åç»­ç ”ç©¶æ–¹å‘"""
        suggestions = [
            "æ·±å…¥ç ”ç©¶å…·ä½“åº”ç”¨åœºæ™¯",
            "æ‰©å¤§æ ·æœ¬è§„æ¨¡è¿›è¡ŒéªŒè¯",
            "æ¢ç´¢è·¨é¢†åŸŸçš„å…³è”æ€§",
            "å¼€å‘æ›´ç²¾ç¡®çš„è¯„ä¼°æ–¹æ³•"
        ]
        return "; ".join(random.sample(suggestions, 2))
    
    def _add_to_knowledge_graph(self, knowledge: Dict[str, Any], domain: str):
        """æ·»åŠ çŸ¥è¯†åˆ°å›¾è°±"""
        node_id = hashlib.md5(knowledge["content"].encode()).hexdigest()[:16]
        
        node = KnowledgeNode(
            id=node_id,
            content=knowledge["content"],
            node_type="fact",
            domain=domain,
            confidence=knowledge.get("confidence", 0.5),
            evidence=[knowledge.get("source", "unknown")]
        )
        
        self.knowledge_graph.add_node(node)


class DeepResearchAgent:
    """æ·±åº¦ç ”ç©¶Agent - ä¸»ç±»"""
    
    def __init__(self, name: str = "DeepResearch Agent", domain: str = "general"):
        self.name = name
        self.domain = domain
        
        # åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
        self.knowledge_graph = KnowledgeGraph()
        self.reasoner = IntelligentReasoner()
        self.pipeline = ResearchPipeline(self.knowledge_graph, self.reasoner)
        
        # ç ”ç©¶å†å²å’ŒçŠ¶æ€
        self.research_history = []
        self.performance_metrics = {
            "total_research_count": 0,
            "avg_confidence": 0.0,
            "domain_expertise": defaultdict(float),
            "reasoning_mode_usage": defaultdict(int)
        }
        
        # å­¦ä¹ å’Œé€‚åº”å‚æ•°
        self.learning_rate = 0.1
        self.confidence_threshold = 0.7
        self.max_research_depth = 5
    
    def research(self, query: str, domain: Optional[str] = None, 
                complexity: int = 3, depth_required: int = 3, urgency: int = 2) -> Dict[str, Any]:
        """æ‰§è¡Œæ·±åº¦ç ”ç©¶"""
        
        # æ„å»ºç ”ç©¶æŸ¥è¯¢
        research_query = ResearchQuery(
            query=query,
            domain=domain or self.domain,
            complexity=complexity,
            depth_required=depth_required,
            urgency=urgency
        )
        
        print(f"ğŸ” å¼€å§‹æ·±åº¦ç ”ç©¶: {query}")
        print(f"ğŸ¨ ç ”ç©¶é…ç½®: å¤æ‚åº¦={complexity}, æ·±åº¦={depth_required}, ç´§æ€¥åº¦={urgency}")
        
        # é€‰æ‹©æ¨ç†æ¨¡å¼
        reasoning_mode = self.reasoner.select_reasoning_mode(research_query, {})
        print(f"ğŸ§  é€‰æ‹©æ¨ç†æ¨¡å¼: {reasoning_mode.value}")
        
        # æ‰§è¡Œç ”ç©¶ç®¡é“
        research_result = self.pipeline.execute_research(research_query)
        
        # å¤„ç†ç»“æœ
        final_result = self._process_research_result(research_query, research_result, reasoning_mode)
        
        # æ›´æ–°å­¦ä¹ æŒ‡æ ‡
        self._update_learning_metrics(research_query, research_result, reasoning_mode)
        
        # ä¿å­˜ç ”ç©¶å†å²
        self.research_history.append({
            "query": research_query,
            "result": final_result,
            "timestamp": datetime.now()
        })
        
        return final_result
    
    def _process_research_result(self, query: ResearchQuery, result: Dict[str, Any], 
                                reasoning_mode: ReasoningMode) -> Dict[str, Any]:
        """å¤„ç†ç ”ç©¶ç»“æœ"""
        
        # è®¡ç®—æ€»ä½“ç½®ä¿¡åº¦
        total_confidence = sum(step.confidence for step in result["steps"]) / len(result["steps"])
        
        # ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
        final_answer = self._generate_comprehensive_answer(query, result)
        
        # æå–å…³é”®æ´å¯Ÿ
        key_insights = [insight.content for insight in result["insights"]]
        
        # è¯„ä¼°ç ”ç©¶è´¨é‡
        quality_score = self._assess_research_quality(result)
        
        return {
            "query": query.query,
            "domain": query.domain,
            "reasoning_mode": reasoning_mode.value,
            "final_answer": final_answer,
            "key_insights": key_insights,
            "total_confidence": total_confidence,
            "quality_score": quality_score,
            "research_steps": len(result["steps"]),
            "tools_used": len(set(result["tools_called"])),
            "knowledge_nodes_consulted": len(result["knowledge_used"]),
            "phases_completed": len(ResearchPhase),
            "detailed_steps": [{
                "phase": step.phase.value,
                "type": step.step_type,
                "content": step.content,
                "confidence": step.confidence
            } for step in result["steps"]],
            "timestamp": datetime.now().isoformat()
        }
    
    def _generate_comprehensive_answer(self, query: ResearchQuery, result: Dict[str, Any]) -> str:
        """ç”Ÿæˆç»¼åˆç­”æ¡ˆ"""
        
        # è·å–ç»“è®ºæ­¥éª¤
        conclusion_steps = [step for step in result["steps"] if step.phase == ResearchPhase.CONCLUSION]
        
        if conclusion_steps:
            main_conclusion = conclusion_steps[0].content
        else:
            main_conclusion = f"å¯¹äºé—®é¢˜'{query.query}'ï¼Œç»è¿‡æ·±å…¥ç ”ç©¶åˆ†æï¼Œæˆ‘ä»¬å¾—å‡ºäº†ç»¼åˆæ€§çš„ç ”ç©¶ç»“æœã€‚"
        
        # æ•´åˆå…³é”®æ´å¯Ÿ
        insights_summary = ""
        if result["insights"]:
            insights_list = [f"- {insight.content}" for insight in result["insights"][:3]]
            insights_summary = f"\n\nğŸ’¡ å…³é”®æ´å¯Ÿ:\n" + "\n".join(insights_list)
        
        # æ•´åˆå·¥å…·ä½¿ç”¨ç»“æœ
        tools_summary = ""
        if result["tools_called"]:
            unique_tools = list(set(result["tools_called"]))
            tools_summary = f"\n\nğŸ”§ ä½¿ç”¨å·¥å…·: {', '.join(unique_tools)}"
        
        # ç»¼åˆç­”æ¡ˆ
        comprehensive_answer = f"{main_conclusion}{insights_summary}{tools_summary}"
        
        return comprehensive_answer
    
    def _assess_research_quality(self, result: Dict[str, Any]) -> float:
        """è¯„ä¼°ç ”ç©¶è´¨é‡"""
        
        # å¤šç»´åº¦è¯„ä¼°
        factors = {
            "completeness": min(1.0, len(result["steps"]) / 15),  # å®Œæ•´æ€§
            "depth": min(1.0, len([s for s in result["steps"] if s.step_type == "thought"]) / 8),  # æ€è€ƒæ·±åº¦
            "diversity": min(1.0, len(set(result["tools_called"])) / 3),  # å·¥å…·å¤šæ ·æ€§
            "insights": min(1.0, len(result["insights"]) / 3),  # æ´å¯Ÿæ•°é‡
            "confidence": sum(step.confidence for step in result["steps"]) / len(result["steps"])  # å¹³å‡ç½®ä¿¡åº¦
        }
        
        # åŠ æƒå¹³å‡
        weights = {
            "completeness": 0.2,
            "depth": 0.25,
            "diversity": 0.2,
            "insights": 0.2,
            "confidence": 0.15
        }
        
        quality_score = sum(factors[key] * weights[key] for key in factors)
        return round(quality_score, 3)
    
    def _update_learning_metrics(self, query: ResearchQuery, result: Dict[str, Any], 
                                reasoning_mode: ReasoningMode):
        """æ›´æ–°å­¦ä¹ æŒ‡æ ‡"""
        
        # æ›´æ–°æ€»ä½“æŒ‡æ ‡
        self.performance_metrics["total_research_count"] += 1
        
        # æ›´æ–°å¹³å‡ç½®ä¿¡åº¦
        current_confidence = sum(step.confidence for step in result["steps"]) / len(result["steps"])
        old_avg = self.performance_metrics["avg_confidence"]
        count = self.performance_metrics["total_research_count"]
        self.performance_metrics["avg_confidence"] = (old_avg * (count - 1) + current_confidence) / count
        
        # æ›´æ–°é¢†åŸŸä¸“ä¸šåº¦
        domain = query.domain
        old_expertise = self.performance_metrics["domain_expertise"][domain]
        self.performance_metrics["domain_expertise"][domain] = (
            old_expertise * (1 - self.learning_rate) + current_confidence * self.learning_rate
        )
        
        # æ›´æ–°æ¨ç†æ¨¡å¼ä½¿ç”¨ç»Ÿè®¡
        self.performance_metrics["reasoning_mode_usage"][reasoning_mode.value] += 1
    
    def get_knowledge_overview(self, domain: Optional[str] = None) -> Dict[str, Any]:
        """è·å–çŸ¥è¯†æ¦‚è§ˆ"""
        target_domain = domain or self.domain
        return self.knowledge_graph.get_domain_overview(target_domain)
    
    def get_performance_report(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        return {
            "agent_name": self.name,
            "primary_domain": self.domain,
            "performance_metrics": dict(self.performance_metrics),
            "total_knowledge_nodes": len(self.knowledge_graph.nodes),
            "research_history_count": len(self.research_history),
            "knowledge_domains": list(self.knowledge_graph.domains.keys()),
            "last_research": self.research_history[-1]["timestamp"].isoformat() if self.research_history else None
        }
    
    def add_domain_knowledge(self, content: str, domain: str, node_type: str = "concept", 
                           confidence: float = 0.8) -> str:
        """æ·»åŠ é¢†åŸŸçŸ¥è¯†"""
        
        node_id = hashlib.md5(f"{content}{domain}{datetime.now()}".encode()).hexdigest()[:16]
        
        node = KnowledgeNode(
            id=node_id,
            content=content,
            node_type=node_type,
            domain=domain,
            confidence=confidence
        )
        
        self.knowledge_graph.add_node(node)
        print(f"âœ… å·²æ·»åŠ çŸ¥è¯†èŠ‚ç‚¹: {content[:50]}...")
        
        return node_id
    
    def explore_research_topic(self, topic: str, max_depth: int = 3) -> Dict[str, Any]:
        """æ¢ç´¢æ€§ç ”ç©¶ä¸»é¢˜"""
        
        exploration_results = []
        
        # ç”Ÿæˆå¤šä¸ªç›¸å…³é—®é¢˜
        related_questions = self._generate_related_questions(topic)
        
        for i, question in enumerate(related_questions[:max_depth]):
            print(f"\nğŸ” æ¢ç´¢é—®é¢˜ {i+1}: {question}")
            
            result = self.research(
                query=question,
                complexity=2,  # ä¸­ç­‰å¤æ‚åº¦
                depth_required=2,  # ä¸­ç­‰æ·±åº¦
                urgency=1  # ä½ç´§æ€¥åº¦
            )
            
            exploration_results.append({
                "question": question,
                "result": result
            })
        
        return {
            "topic": topic,
            "exploration_results": exploration_results,
            "total_questions_explored": len(exploration_results),
            "timestamp": datetime.now().isoformat()
        }
    
    def _generate_related_questions(self, topic: str) -> List[str]:
        """ç”Ÿæˆç›¸å…³é—®é¢˜"""
        
        question_templates = [
            f"{topic}çš„æ ¸å¿ƒåŸç†æ˜¯ä»€ä¹ˆï¼Ÿ",
            f"{topic}åœ¨å®é™…ä¸­æœ‰å“ªäº›åº”ç”¨ï¼Ÿ",
            f"{topic}çš„å‘å±•è¶‹åŠ¿å¦‚ä½•ï¼Ÿ",
            f"{topic}é¢ä¸´çš„ä¸»è¦æŒ‘æˆ˜æ˜¯ä»€ä¹ˆï¼Ÿ",
            f"{topic}ä¸å…¶ä»–é¢†åŸŸæœ‰ä»€ä¹ˆå…³è”ï¼Ÿ"
        ]
        
        return question_templates
    
    def collaborative_research(self, main_query: str, perspectives: List[str]) -> Dict[str, Any]:
        """åä½œå¼ç ”ç©¶ - ä»å¤šä¸ªè§†è§’ç ”ç©¶åŒä¸€é—®é¢˜"""
        
        print(f"ğŸ¤ å¼€å§‹åä½œå¼ç ”ç©¶: {main_query}")
        print(f"ğŸ‘ï¸ ç ”ç©¶è§†è§’: {', '.join(perspectives)}")
        
        perspective_results = []
        
        for perspective in perspectives:
            perspective_query = f"ä»{perspective}è§†è§’åˆ†æ: {main_query}"
            
            print(f"\nğŸ” ç ”ç©¶è§†è§’: {perspective}")
            
            result = self.research(
                query=perspective_query,
                complexity=3,
                depth_required=3
            )
            
            perspective_results.append({
                "perspective": perspective,
                "query": perspective_query,
                "result": result
            })
        
        # ç»¼åˆåˆ†æ
        synthesis = self._synthesize_perspectives(main_query, perspective_results)
        
        return {
            "main_query": main_query,
            "perspectives": perspectives,
            "perspective_results": perspective_results,
            "synthesis": synthesis,
            "timestamp": datetime.now().isoformat()
        }
    
    def _synthesize_perspectives(self, main_query: str, perspective_results: List[Dict]) -> Dict[str, Any]:
        """ç»¼åˆå¤šè§†è§’ç ”ç©¶ç»“æœ"""
        
        # æ”¶é›†æ‰€æœ‰æ´å¯Ÿ
        all_insights = []
        for result in perspective_results:
            all_insights.extend(result["result"]["key_insights"])
        
        # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦
        avg_confidence = sum(result["result"]["total_confidence"] for result in perspective_results) / len(perspective_results)
        
        # æ‰¾åˆ°å…±åŒä¸»é¢˜
        common_themes = self._identify_common_themes([result["result"]["final_answer"] for result in perspective_results])
        
        # ç”Ÿæˆç»¼åˆç»“è®º
        synthesis_conclusion = f"é€šè¿‡å¤šè§†è§’åˆ†æ'{main_query}'ï¼Œæˆ‘ä»¬å‘ç°äº†{len(set(all_insights))}ä¸ªç‹¬ç‰¹æ´å¯Ÿã€‚å…±åŒä¸»é¢˜åŒ…æ‹¬ï¼š{', '.join(common_themes)}ã€‚å¤šè§†è§’åˆ†ææä¾›äº†æ›´å…¨é¢å’Œæ·±å…¥çš„ç†è§£ã€‚"
        
        return {
            "synthesis_conclusion": synthesis_conclusion,
            "all_insights": list(set(all_insights)),
            "common_themes": common_themes,
            "average_confidence": avg_confidence,
            "perspectives_count": len(perspective_results)
        }
    
    def _identify_common_themes(self, texts: List[str]) -> List[str]:
        """è¯†åˆ«å…±åŒä¸»é¢˜"""
        
        # ç®€å•çš„å…³é”®è¯æ•°æ®
        word_counts = defaultdict(int)
        
        for text in texts:
            words = re.findall(r'\w+', text.lower())
            for word in words:
                if len(word) > 3:  # åªè€ƒè™‘è¾ƒé•¿çš„è¯
                    word_counts[word] += 1
        
        # æ‰¾åˆ°å‡ºç°é¢‘ç‡é«˜çš„è¯æ±‡
        common_words = [word for word, count in word_counts.items() if count >= len(texts) // 2]
        
        return common_words[:5]  # è¿”å›å‰5ä¸ªå…±åŒä¸»é¢˜


def demo_basic_research():
    """æ¼”ç¤ºåŸºç¡€ç ”ç©¶åŠŸèƒ½"""
    print("\n" + "=" * 80)
    print("ğŸ” DeepResearch Agent - åŸºç¡€ç ”ç©¶æ¼”ç¤º")
    print("=" * 80)
    
    # åˆ›å»ºç ”ç©¶Agent
    agent = DeepResearchAgent(name="æ·±åº¦ç ”ç©¶åŠ©æ‰‹", domain="äººå·¥æ™ºèƒ½")
    
    # æ·»åŠ ä¸€äº›åŸºç¡€çŸ¥è¯†
    knowledge_base = [
        ("äººå·¥æ™ºèƒ½æ˜¯ä½¿æœºå™¨èƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„æŠ€æœ¯", "äººå·¥æ™ºèƒ½", "concept"),
        ("æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä½¿ç³»ç»Ÿèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ ", "äººå·¥æ™ºèƒ½", "concept"),
        ("æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œæ¨¡æ‹Ÿäººè„‘çš„å­¦ä¹ è¿‡ç¨‹", "äººå·¥æ™ºèƒ½", "concept")
    ]
    
    print("\nğŸ“š æ­£åœ¨æ·»åŠ é¢†åŸŸçŸ¥è¯†...")
    for content, domain, node_type in knowledge_base:
        agent.add_domain_knowledge(content, domain, node_type)
    
    # æ¼”ç¤ºåŸºç¡€ç ”ç©¶
    result = agent.research(
        query="ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ",
        complexity=3,
        depth_required=3,
        urgency=2
    )
    
    print(f"\nğŸ† ç ”ç©¶ç»“æœ:")
    print(f"  âœ… ç½®ä¿¡åº¦: {result['total_confidence']:.2f}")
    print(f"  ğŸ“‹ ç ”ç©¶æ­¥éª¤æ•°: {result['research_steps']}")
    print(f"  ğŸ”§ ä½¿ç”¨å·¥å…·æ•°: {result['tools_used']}")
    print(f"  ğŸ’¡ å…³é”®æ´å¯Ÿæ•°: {len(result['key_insights'])}")
    print(f"  ğŸ† è´¨é‡è¯„åˆ†: {result['quality_score']}")
    
    print(f"\nğŸ“œ æœ€ç»ˆç­”æ¡ˆ:")
    print(result['final_answer'])
    
    return agent


def main():
    """ä¸»å‡½æ•° - è¿è¡Œæ¼”ç¤º"""
    print("ğŸ† æ¬¢è¿ä½¿ç”¨ DeepResearch Agent ç³»ç»Ÿ")
    print("ğŸ”¬ è¿™æ˜¯ä¸€ä¸ªåŸºäº LLM é©±åŠ¨çš„æ·±åº¦ç ”ç©¶æ™ºèƒ½ä½“")
    
    print("\né€‰æ‹©æ¼”ç¤ºæ¨¡å¼:")
    print("1. åŸºç¡€ç ”ç©¶åŠŸèƒ½æ¼”ç¤º")
    print("2. é«˜çº§ç ”ç©¶åŠŸèƒ½æ¼”ç¤º")
    print("3. æ¨ç†æ¨¡å¼æµ‹è¯•")
    print("4. äº¤äº’å¼ç ”ç©¦æ¨¡å¼")
    print("0. é€€å‡º")
    
    while True:
        try:
            choice = input("\nè¯·é€‰æ‹© (0-4): ").strip()
            
            if choice == '0':
                print("\nğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ DeepResearch Agent!")
                break
            elif choice == '1':
                demo_basic_research()
            elif choice == '2':
                print("ğŸš§ é«˜çº§åŠŸèƒ½å¾…å®ç°...")
            elif choice == '3':
                print("ğŸš§ æ¨ç†æ¨¡å¼æµ‹è¯•å¾…å®ç°...")
            elif choice == '4':
                print("ğŸš§ äº¤äº’å¼æ¨¡å¼å¾…å®ç°...")
            else:
                print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥ 0-4")
                
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ç¨‹åºè¢«ä¸­æ–­ï¼Œå†è§ï¼")
            break
        except Exception as e:
            print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")


if __name__ == "__main__":
    main()