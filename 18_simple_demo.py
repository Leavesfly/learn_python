"""
ç®€åŒ–ç‰ˆè‡ªè¿›åŒ–Agentæ¼”ç¤º
ç›´æ¥åœ¨æ–‡ä»¶ä¸­å¼•ç”¨ä¸»è¦åŠŸèƒ½æ¥é¿å…å¯¼å…¥é—®é¢˜
"""

import json
import time
import random
from typing import Dict, List, Any, Optional
from dataclasses import dataclass

@dataclass
class SimpleExperience:
    """ç®€åŒ–çš„ç»éªŒè®°å½•"""
    task: str
    action: str
    success: bool
    reward: float
    timestamp: float

class SimpleEvolutionAgent:
    """ç®€åŒ–ç‰ˆè‡ªè¿›åŒ–Agent"""
    
    def __init__(self, name: str = "SimpleAgent"):
        self.name = name
        self.experiences = []
        self.action_success_rates = {}  # åŠ¨ä½œæˆåŠŸç‡
        self.task_action_preferences = {}  # ä»»åŠ¡-åŠ¨ä½œåå¥½
        self.total_tasks = 0
        self.successful_tasks = 0
        self.exploration_rate = 0.3
        
        # å¯ç”¨å·¥å…·
        self.tools = ['search', 'calculate', 'analyze', 'plan']
        
    def choose_action(self, task: str) -> str:
        """é€‰æ‹©åŠ¨ä½œ"""
        # æ¢ç´¢vsåˆ©ç”¨
        if random.random() < self.exploration_rate:
            # æ¢ç´¢ï¼šéšæœºé€‰æ‹©
            return random.choice(self.tools)
        else:
            # åˆ©ç”¨ï¼šåŸºäºå†å²ç»éªŒé€‰æ‹©
            if task in self.task_action_preferences:
                # é€‰æ‹©è¯¥ä»»åŠ¡ç±»å‹æœ€æˆåŠŸçš„åŠ¨ä½œ
                best_action = max(self.task_action_preferences[task].items(), 
                                key=lambda x: x[1])
                return best_action[0]
            else:
                # é€‰æ‹©å…¨å±€æœ€æˆåŠŸçš„åŠ¨ä½œ
                if self.action_success_rates:
                    best_action = max(self.action_success_rates.items(),
                                    key=lambda x: x[1])
                    return best_action[0]
                else:
                    return random.choice(self.tools)
    
    def execute_action(self, action: str, task: str) -> tuple:
        """æ‰§è¡ŒåŠ¨ä½œå¹¶è¿”å›æˆåŠŸä¸å¦"""
        # æ¨¡æ‹Ÿæ‰§è¡Œç»“æœï¼Œä¸åŒåŠ¨ä½œå¯¹ä¸åŒä»»åŠ¡çš„æˆåŠŸç‡ä¸åŒ
        success_probabilities = {
            'search': {
                'æœç´¢': 0.9, 'æŸ¥è¯¢': 0.8, 'ç ”ç©¶': 0.7, 'åˆ†æ': 0.4
            },
            'calculate': {
                'è®¡ç®—': 0.9, 'æ•°å­¦': 0.8, 'ç»Ÿè®¡': 0.7, 'ä¼˜åŒ–': 0.6
            },
            'analyze': {
                'åˆ†æ': 0.9, 'ç ”ç©¶': 0.8, 'è¯„ä¼°': 0.7, 'é¢„æµ‹': 0.6
            },
            'plan': {
                'è§„åˆ’': 0.9, 'è®¾è®¡': 0.8, 'åˆ¶å®š': 0.7, 'åˆ›å»º': 0.6
            }
        }
        
        # åŸºäºä»»åŠ¡å…³é”®è¯ç¡®å®šåŸºç¡€æˆåŠŸç‡
        base_prob = 0.5
        for keyword, prob in success_probabilities.get(action, {}).items():
            if keyword in task:
                base_prob = prob
                break
        
        # æ·»åŠ ä¸€äº›éšæœºæ€§
        actual_prob = base_prob + random.uniform(-0.2, 0.2)
        actual_prob = max(0.1, min(0.95, actual_prob))  # é™åˆ¶åœ¨åˆç†èŒƒå›´
        
        success = random.random() < actual_prob
        reward = 1.0 if success else -0.5
        
        return success, reward
    
    def learn_from_experience(self, experience: SimpleExperience):
        """ä»ç»éªŒä¸­å­¦ä¹ """
        self.experiences.append(experience)
        
        # æ›´æ–°åŠ¨ä½œæˆåŠŸç‡
        action = experience.action
        if action not in self.action_success_rates:
            self.action_success_rates[action] = 0.5
        
        # ä½¿ç”¨æŒ‡æ•°ç§»åŠ¨å¹³å‡æ›´æ–°æˆåŠŸç‡
        alpha = 0.1
        if experience.success:
            self.action_success_rates[action] = (1 - alpha) * self.action_success_rates[action] + alpha * 1.0
        else:
            self.action_success_rates[action] = (1 - alpha) * self.action_success_rates[action] + alpha * 0.0
        
        # æ›´æ–°ä»»åŠ¡-åŠ¨ä½œåå¥½
        task_type = experience.task.split('ï¼š')[0] if 'ï¼š' in experience.task else experience.task[:4]
        if task_type not in self.task_action_preferences:
            self.task_action_preferences[task_type] = {}
        
        if action not in self.task_action_preferences[task_type]:
            self.task_action_preferences[task_type][action] = 0.5
            
        if experience.success:
            self.task_action_preferences[task_type][action] = \
                (1 - alpha) * self.task_action_preferences[task_type][action] + alpha * 1.0
        else:
            self.task_action_preferences[task_type][action] = \
                (1 - alpha) * self.task_action_preferences[task_type][action] + alpha * 0.0
    
    def process_task(self, task: str) -> Dict[str, Any]:
        """å¤„ç†ä»»åŠ¡"""
        # é€‰æ‹©åŠ¨ä½œ
        action = self.choose_action(task)
        
        # æ‰§è¡ŒåŠ¨ä½œ
        success, reward = self.execute_action(action, task)
        
        # åˆ›å»ºç»éªŒ
        experience = SimpleExperience(
            task=task,
            action=action,
            success=success,
            reward=reward,
            timestamp=time.time()
        )
        
        # å­¦ä¹ 
        self.learn_from_experience(experience)
        
        # æ›´æ–°ç»Ÿè®¡
        self.total_tasks += 1
        if success:
            self.successful_tasks += 1
            
        return {
            'task': task,
            'action': action,
            'success': success,
            'reward': reward,
            'success_rate': self.successful_tasks / self.total_tasks
        }
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½æ‘˜è¦"""
        return {
            'total_tasks': self.total_tasks,
            'successful_tasks': self.successful_tasks,
            'success_rate': self.successful_tasks / self.total_tasks if self.total_tasks > 0 else 0,
            'exploration_rate': self.exploration_rate,
            'learned_preferences': self.task_action_preferences,
            'action_success_rates': self.action_success_rates
        }
    
    def evolve(self):
        """è‡ªæˆ‘è¿›åŒ–"""
        # æ ¹æ®è¿‘æœŸè¡¨ç°è°ƒæ•´æ¢ç´¢ç‡
        if len(self.experiences) >= 10:
            recent_success = sum(1 for exp in self.experiences[-10:] if exp.success) / 10
            if recent_success > 0.8:
                self.exploration_rate = max(0.1, self.exploration_rate - 0.05)
            elif recent_success < 0.5:
                self.exploration_rate = min(0.5, self.exploration_rate + 0.05)

def comprehensive_demo():
    """ç»¼åˆæ¼”ç¤º"""
    print("=== ç®€åŒ–ç‰ˆè‡ªè¿›åŒ–Agentç»¼åˆæ¼”ç¤º ===\n")
    
    agent = SimpleEvolutionAgent("å­¦ä¹ åŠ©æ‰‹")
    
    # æµ‹è¯•ä»»åŠ¡åºåˆ— - ä»ç®€å•åˆ°å¤æ‚
    test_tasks = [
        # ç¬¬ä¸€é˜¶æ®µï¼šåŸºç¡€ä»»åŠ¡
        "æœç´¢ï¼šPythonåŸºç¡€æ•™ç¨‹",
        "è®¡ç®—ï¼šæŠ•èµ„æ”¶ç›Šç‡",
        "åˆ†æï¼šç”¨æˆ·è¡Œä¸ºæ•°æ®", 
        "è§„åˆ’ï¼šå­¦ä¹ è·¯çº¿å›¾",
        
        # ç¬¬äºŒé˜¶æ®µï¼šä¸­ç­‰éš¾åº¦
        "æœç´¢ï¼šæœºå™¨å­¦ä¹ ç®—æ³•",
        "è®¡ç®—ï¼šæ¨¡å‹å‡†ç¡®ç‡",
        "åˆ†æï¼šæ•°æ®åˆ†å¸ƒç‰¹å¾",
        "è§„åˆ’ï¼šé¡¹ç›®å¼€å‘è®¡åˆ’",
        
        # ç¬¬ä¸‰é˜¶æ®µï¼šå¤æ‚ä»»åŠ¡
        "ç ”ç©¶ï¼šæ·±åº¦å­¦ä¹ å‰æ²¿",
        "ä¼˜åŒ–ï¼šç¥ç»ç½‘ç»œæ¶æ„",
        "è¯„ä¼°ï¼šç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡",
        "è®¾è®¡ï¼šæ™ºèƒ½æ¨èç³»ç»Ÿ",
        
        # ç¬¬å››é˜¶æ®µï¼šåˆ›æ–°æŒ‘æˆ˜
        "åˆ›å»ºï¼šæ–°å‹ç®—æ³•æ¡†æ¶",
        "é¢„æµ‹ï¼šæŠ€æœ¯å‘å±•è¶‹åŠ¿",
        "åˆ¶å®šï¼šAIä¼¦ç†å‡†åˆ™",
        "æ„å»ºï¼šå¤šæ¨¡æ€å­¦ä¹ ç³»ç»Ÿ"
    ]
    
    print(f"å‡†å¤‡æ‰§è¡Œ {len(test_tasks)} ä¸ªæ¸è¿›å¼ä»»åŠ¡\n")
    
    results = []
    
    # æ‰§è¡Œä»»åŠ¡å¹¶è§‚å¯Ÿå­¦ä¹ è¿‡ç¨‹
    for i, task in enumerate(test_tasks, 1):
        print(f"--- ä»»åŠ¡ {i}: {task} ---")
        
        result = agent.process_task(task)
        results.append(result)
        
        print(f"ğŸ¯ é€‰æ‹©åŠ¨ä½œ: {result['action']}")
        print(f"âœ… æ‰§è¡Œç»“æœ: {'æˆåŠŸ' if result['success'] else 'å¤±è´¥'}")
        print(f"ğŸ“Š å½“å‰æˆåŠŸç‡: {result['success_rate']:.1%}")
        
        # æ¯4ä¸ªä»»åŠ¡æ˜¾ç¤ºä¸€æ¬¡å­¦ä¹ çŠ¶æ€
        if i % 4 == 0:
            print(f"\n--- ç¬¬{i//4}é˜¶æ®µå­¦ä¹ æ€»ç»“ ---")
            performance = agent.get_performance_summary()
            
            print(f"ğŸ¯ é˜¶æ®µæˆåŠŸç‡: {performance['success_rate']:.1%}")
            print(f"ğŸ” å½“å‰æ¢ç´¢ç‡: {performance['exploration_rate']:.2f}")
            
            print("ğŸ§  å­¦åˆ°çš„åŠ¨ä½œåå¥½:")
            for task_type, actions in performance['learned_preferences'].items():
                best_action = max(actions.items(), key=lambda x: x[1])
                print(f"  {task_type}: æœ€ä½³åŠ¨ä½œ {best_action[0]} (æˆåŠŸç‡ {best_action[1]:.1%})")
            
            print("ğŸ› ï¸ å·¥å…·ä½¿ç”¨æ•ˆæœ:")
            for action, success_rate in performance['action_success_rates'].items():
                print(f"  {action}: {success_rate:.1%}")
            
            # è§¦å‘è¿›åŒ–
            agent.evolve()
            print(f"ğŸ”„ æ¢ç´¢ç‡è°ƒæ•´ä¸º: {agent.exploration_rate:.2f}")
            print()
        else:
            print()
    
    # æœ€ç»ˆåˆ†æ
    print("=" * 60)
    print("ğŸ“ æœ€ç»ˆå­¦ä¹ æˆæœåˆ†æ")
    print("=" * 60)
    
    final_performance = agent.get_performance_summary()
    
    print(f"\nğŸ“ˆ æ•´ä½“è¡¨ç°:")
    print(f"  æ€»ä»»åŠ¡æ•°: {final_performance['total_tasks']}")
    print(f"  æˆåŠŸä»»åŠ¡: {final_performance['successful_tasks']}")
    print(f"  æœ€ç»ˆæˆåŠŸç‡: {final_performance['success_rate']:.1%}")
    
    # å­¦ä¹ æ›²çº¿åˆ†æ
    success_rates = [r['success_rate'] for r in results]
    
    print(f"\nğŸ“Š å­¦ä¹ æ›²çº¿åˆ†æ:")
    print(f"  åˆæœŸæˆåŠŸç‡ (å‰4ä»»åŠ¡): {success_rates[3]:.1%}")
    print(f"  ä¸­æœŸæˆåŠŸç‡ (ç¬¬8ä»»åŠ¡): {success_rates[7]:.1%}")
    print(f"  åæœŸæˆåŠŸç‡ (ç¬¬12ä»»åŠ¡): {success_rates[11]:.1%}")
    print(f"  æœ€ç»ˆæˆåŠŸç‡: {success_rates[-1]:.1%}")
    
    improvement = success_rates[-1] - success_rates[3]
    print(f"  æ€»ä½“æ”¹è¿›: {improvement:+.1%}")
    
    if improvement > 0.1:
        print("  ğŸš€ æ˜¾è‘—æ”¹è¿›ï¼Agentå­¦ä¹ æ•ˆæœæ˜æ˜¾")
    elif improvement > 0:
        print("  ğŸ“ˆ ç¨³æ­¥æå‡ï¼Œå­¦ä¹ æ–¹å‘æ­£ç¡®")
    else:
        print("  ğŸ¤” å­¦ä¹ æ•ˆæœæœ‰é™ï¼Œéœ€è¦è°ƒä¼˜")
    
    # ä¸“ä¸šåŒ–åˆ†æ
    print(f"\nğŸ¯ ä»»åŠ¡ä¸“ä¸šåŒ–åˆ†æ:")
    task_types = set()
    for task in test_tasks:
        task_type = task.split('ï¼š')[0] if 'ï¼š' in task else task[:4]
        task_types.add(task_type)
    
    for task_type in task_types:
        if task_type in final_performance['learned_preferences']:
            preferences = final_performance['learned_preferences'][task_type]
            best_action = max(preferences.items(), key=lambda x: x[1])
            print(f"  {task_type}ä»»åŠ¡: ä¸“ç²¾ {best_action[0]} (ç†Ÿç»ƒåº¦ {best_action[1]:.1%})")
    
    # ä¿å­˜å­¦ä¹ ç»“æœ
    timestamp = int(time.time())
    with open(f"learning_results_{timestamp}.json", 'w', encoding='utf-8') as f:
        json.dump({
            'agent_name': agent.name,
            'final_performance': final_performance,
            'task_results': results,
            'learning_timeline': [
                {
                    'task_num': i+1,
                    'task': test_tasks[i],
                    'action': results[i]['action'],
                    'success': results[i]['success'],
                    'cumulative_success_rate': results[i]['success_rate']
                }
                for i in range(len(test_tasks))
            ]
        }, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ å­¦ä¹ ç»“æœå·²ä¿å­˜åˆ°: learning_results_{timestamp}.json")
    
    return agent, results

if __name__ == "__main__":
    agent, results = comprehensive_demo()
    
    print(f"\nğŸ‰ è‡ªè¿›åŒ–Agentæ¼”ç¤ºå®Œæˆï¼")
    print(f"Agentä»éšæœºå†³ç­–è¿›åŒ–ä¸ºä¸“ä¸šåŒ–çš„ä»»åŠ¡å¤„ç†ç³»ç»Ÿã€‚")