#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RAGç³»ç»Ÿæµ‹è¯•è„šæœ¬
éªŒè¯å„ä¸ªç»„ä»¶çš„åŠŸèƒ½æ˜¯å¦æ­£å¸¸
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

def test_imports():
    """æµ‹è¯•å¯¼å…¥"""
    try:
        from 19_rag_vector_demo import (
            SimpleTokenizer, TFIDFVectorizer, VectorSimilarity,
            VectorDatabase, RAGSystem, Document, QueryResult
        )
        print("âœ… æ‰€æœ‰æ¨¡å—å¯¼å…¥æˆåŠŸ")
        return True
    except ImportError as e:
        print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        return False

def test_tokenizer():
    """æµ‹è¯•åˆ†è¯å™¨"""
    try:
        from 19_rag_vector_demo import SimpleTokenizer
        
        tokenizer = SimpleTokenizer()
        
        # æµ‹è¯•ä¸­è‹±æ–‡åˆ†è¯
        text = "Pythonæ˜¯ç¼–ç¨‹è¯­è¨€ machine learning"
        tokens = tokenizer.tokenize(text)
        print(f"åˆ†è¯æµ‹è¯•: '{text}' -> {tokens}")
        
        # æ„å»ºè¯æ±‡è¡¨
        texts = ["Pythonç¼–ç¨‹", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ "]
        tokenizer.build_vocab(texts)
        print(f"è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")
        
        print("âœ… åˆ†è¯å™¨æµ‹è¯•é€šè¿‡")
        return True
    except Exception as e:
        print(f"âŒ åˆ†è¯å™¨æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_vectorizer():
    """æµ‹è¯•å‘é‡åŒ–å™¨"""
    try:
        from 19_rag_vector_demo import TFIDFVectorizer
        
        vectorizer = TFIDFVectorizer(max_features=10)
        
        # æµ‹è¯•æ–‡æ¡£
        docs = [
            "Pythonæ˜¯ç¼–ç¨‹è¯­è¨€",
            "æœºå™¨å­¦ä¹ å¾ˆé‡è¦", 
            "æ·±åº¦å­¦ä¹ ä½¿ç”¨ç¥ç»ç½‘ç»œ"
        ]
        
        # è®­ç»ƒå’Œè½¬æ¢
        vectors = vectorizer.fit_transform(docs)
        print(f"å‘é‡åŒ–æµ‹è¯•: {len(docs)} ä¸ªæ–‡æ¡£ -> {len(vectors)} ä¸ªå‘é‡")
        print(f"å‘é‡ç»´åº¦: {len(vectors[0])}")
        
        print("âœ… å‘é‡åŒ–å™¨æµ‹è¯•é€šè¿‡")
        return True
    except Exception as e:
        print(f"âŒ å‘é‡åŒ–å™¨æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_similarity():
    """æµ‹è¯•ç›¸ä¼¼åº¦è®¡ç®—"""
    try:
        from 19_rag_vector_demo import VectorSimilarity
        
        vec1 = [1.0, 0.0, 0.0]
        vec2 = [0.0, 1.0, 0.0]  
        vec3 = [1.0, 0.0, 0.0]
        
        # æµ‹è¯•ä½™å¼¦ç›¸ä¼¼åº¦
        sim1 = VectorSimilarity.cosine_similarity(vec1, vec2)  # åº”è¯¥æ˜¯0
        sim2 = VectorSimilarity.cosine_similarity(vec1, vec3)  # åº”è¯¥æ˜¯1
        
        print(f"ç›¸ä¼¼åº¦æµ‹è¯•: vec1 ä¸ vec2 = {sim1:.3f}")
        print(f"ç›¸ä¼¼åº¦æµ‹è¯•: vec1 ä¸ vec3 = {sim2:.3f}")
        
        print("âœ… ç›¸ä¼¼åº¦è®¡ç®—æµ‹è¯•é€šè¿‡")
        return True
    except Exception as e:
        print(f"âŒ ç›¸ä¼¼åº¦è®¡ç®—æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_rag_system():
    """æµ‹è¯•RAGç³»ç»Ÿ"""
    try:
        from 19_rag_vector_demo import RAGSystem
        
        # åˆ›å»ºRAGç³»ç»Ÿ
        rag = RAGSystem(vector_dim=32, similarity_threshold=0.0)
        
        # æ·»åŠ æµ‹è¯•æ–‡æ¡£
        documents = [
            {
                "id": "doc1",
                "content": "Pythonæ˜¯ä¸€ç§ç¼–ç¨‹è¯­è¨€",
                "metadata": {"type": "tech"}
            },
            {
                "id": "doc2", 
                "content": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½åˆ†æ”¯",
                "metadata": {"type": "ai"}
            }
        ]
        
        rag.add_documents(documents)
        
        # æµ‹è¯•æ£€ç´¢
        results = rag.search("ç¼–ç¨‹è¯­è¨€", top_k=2)
        print(f"æ£€ç´¢æµ‹è¯•: æ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
        
        if results:
            result = results[0]
            print(f"æœ€ä½³åŒ¹é…: {result.document.id}, ç›¸ä¼¼åº¦: {result.similarity:.3f}")
        
        # æµ‹è¯•ä¸Šä¸‹æ–‡ç”Ÿæˆ
        context = rag.generate_context("ç¼–ç¨‹", max_context_length=200)
        print(f"ä¸Šä¸‹æ–‡ç”Ÿæˆ: {len(context)} å­—ç¬¦")
        
        print("âœ… RAGç³»ç»Ÿæµ‹è¯•é€šè¿‡")
        return True
    except Exception as e:
        print(f"âŒ RAGç³»ç»Ÿæµ‹è¯•å¤±è´¥: {e}")
        return False

def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("ğŸ§ª å¼€å§‹RAGç³»ç»Ÿç»„ä»¶æµ‹è¯•")
    print("=" * 50)
    
    tests = [
        ("å¯¼å…¥æµ‹è¯•", test_imports),
        ("åˆ†è¯å™¨æµ‹è¯•", test_tokenizer),
        ("å‘é‡åŒ–å™¨æµ‹è¯•", test_vectorizer),
        ("ç›¸ä¼¼åº¦è®¡ç®—æµ‹è¯•", test_similarity),
        ("RAGç³»ç»Ÿæµ‹è¯•", test_rag_system)
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        print(f"\nğŸ”¬ {test_name}:")
        if test_func():
            passed += 1
        else:
            print(f"è·³è¿‡åç»­æµ‹è¯•...")
            break
    
    print(f"\nğŸ“Š æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼RAGç³»ç»Ÿè¿è¡Œæ­£å¸¸")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä»£ç ")

if __name__ == "__main__":
    run_all_tests()