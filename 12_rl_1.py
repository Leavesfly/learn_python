"""
å¼ºåŒ–å­¦ä¹ å…¥é—¨æ•™ç¨‹ - åŸºç¡€ç†è®ºä¸Žæ¦‚å¿µ
ä½œè€…: Qoder AI Assistant
æ—¥æœŸ: 2025-09-16

æœ¬æ–‡ä»¶åŒ…å«å¼ºåŒ–å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µã€æœ¯è¯­å’Œç†è®ºåŸºç¡€
"""

import numpy as np
import matplotlib.pyplot as plt
import random
from typing import Dict, List, Tuple, Optional

class RLBasics:
    """
    å¼ºåŒ–å­¦ä¹ åŸºç¡€æ¦‚å¿µæ¼”ç¤ºç±»
    """
    
    def __init__(self):
        self.episodes_data = []
        
    def explain_concepts(self):
        """
        å¼ºåŒ–å­¦ä¹ æ ¸å¿ƒæ¦‚å¿µè§£é‡Š
        """
        concepts = {
            "æ™ºèƒ½ä½“ (Agent)": "åšå†³ç­–çš„å®žä½“ï¼Œå¦‚æ¸¸æˆä¸­çš„çŽ©å®¶ã€æœºå™¨äººç­‰",
            "çŽ¯å¢ƒ (Environment)": "æ™ºèƒ½ä½“æ‰€å¤„çš„ä¸–ç•Œï¼Œæä¾›çŠ¶æ€å’Œå¥–åŠ±",
            "çŠ¶æ€ (State)": "æè¿°çŽ¯å¢ƒå½“å‰æƒ…å†µçš„ä¿¡æ¯",
            "åŠ¨ä½œ (Action)": "æ™ºèƒ½ä½“å¯ä»¥æ‰§è¡Œçš„æ“ä½œ",
            "å¥–åŠ± (Reward)": "çŽ¯å¢ƒå¯¹æ™ºèƒ½ä½“åŠ¨ä½œçš„åé¦ˆä¿¡å·",
            "ç­–ç•¥ (Policy)": "æ™ºèƒ½ä½“é€‰æ‹©åŠ¨ä½œçš„è§„åˆ™ï¼ŒÏ€(a|s)",
            "ä»·å€¼å‡½æ•° (Value Function)": "è¯„ä¼°çŠ¶æ€æˆ–çŠ¶æ€-åŠ¨ä½œå¯¹çš„å¥½å",
            "Qå‡½æ•°": "çŠ¶æ€-åŠ¨ä½œä»·å€¼å‡½æ•°ï¼ŒQ(s,a)",
            "æŽ¢ç´¢vsåˆ©ç”¨": "æŽ¢ç´¢æ–°åŠ¨ä½œ vs åˆ©ç”¨å·²çŸ¥æœ€ä¼˜åŠ¨ä½œçš„æƒè¡¡"
        }
        
        print("ðŸŽ¯ å¼ºåŒ–å­¦ä¹ æ ¸å¿ƒæ¦‚å¿µ")
        print("=" * 50)
        for concept, explanation in concepts.items():
            print(f"ðŸ“Œ {concept}: {explanation}")
        print()
        
    def rl_process_flow(self):
        """
        å¼ºåŒ–å­¦ä¹ è¿‡ç¨‹æµç¨‹å›¾ï¼ˆæ–‡å­—ç‰ˆï¼‰
        """
        flow = """
        ðŸ”„ å¼ºåŒ–å­¦ä¹ äº¤äº’æµç¨‹ï¼š
        
        1. æ™ºèƒ½ä½“è§‚å¯Ÿå½“å‰çŠ¶æ€ s_t
        2. æ ¹æ®ç­–ç•¥Ï€é€‰æ‹©åŠ¨ä½œ a_t
        3. æ‰§è¡ŒåŠ¨ä½œï¼ŒçŽ¯å¢ƒç»™å‡ºæ–°çŠ¶æ€ s_{t+1} å’Œå¥–åŠ± r_t
        4. æ™ºèƒ½ä½“æ›´æ–°ç­–ç•¥/ä»·å€¼å‡½æ•°
        5. é‡å¤æ­¥éª¤1-4
        
        ç›®æ ‡ï¼šæœ€å¤§åŒ–ç´¯ç§¯å¥–åŠ± G_t = Î£ Î³^k * r_{t+k+1}
        å…¶ä¸­ Î³ æ˜¯æŠ˜æ‰£å› å­ (0 â‰¤ Î³ â‰¤ 1)
        """
        print(flow)

class SimpleGridWorld:
    """
    ç®€å•ç½‘æ ¼ä¸–ç•ŒçŽ¯å¢ƒ - ç”¨äºŽæ¼”ç¤ºå¼ºåŒ–å­¦ä¹ åŸºæœ¬æ¦‚å¿µ
    """
    
    def __init__(self, size: int = 4):
        self.size = size
        self.reset()
        self.goal_state = (size-1, size-1)  # å³ä¸‹è§’ä¸ºç›®æ ‡
        self.obstacles = [(1, 1), (2, 2)]   # éšœç¢ç‰©ä½ç½®
        
    def reset(self) -> Tuple[int, int]:
        """é‡ç½®çŽ¯å¢ƒï¼Œè¿”å›žåˆå§‹çŠ¶æ€"""
        self.agent_pos = (0, 0)  # å·¦ä¸Šè§’å¼€å§‹
        return self.agent_pos
    
    def get_valid_actions(self, state: Tuple[int, int]) -> List[int]:
        """èŽ·å–å½“å‰çŠ¶æ€ä¸‹çš„æœ‰æ•ˆåŠ¨ä½œ"""
        valid_actions = []
        x, y = state
        
        # 0: ä¸Š, 1: ä¸‹, 2: å·¦, 3: å³
        if x > 0: valid_actions.append(0)  # ä¸Š
        if x < self.size - 1: valid_actions.append(1)  # ä¸‹
        if y > 0: valid_actions.append(2)  # å·¦
        if y < self.size - 1: valid_actions.append(3)  # å³
        
        return valid_actions
    
    def step(self, action: int) -> Tuple[Tuple[int, int], float, bool]:
        """
        æ‰§è¡ŒåŠ¨ä½œï¼Œè¿”å›žæ–°çŠ¶æ€ã€å¥–åŠ±å’Œæ˜¯å¦ç»“æŸ
        """
        x, y = self.agent_pos
        
        # æ ¹æ®åŠ¨ä½œç§»åŠ¨
        if action == 0 and x > 0:  # ä¸Š
            x -= 1
        elif action == 1 and x < self.size - 1:  # ä¸‹
            x += 1
        elif action == 2 and y > 0:  # å·¦
            y -= 1
        elif action == 3 and y < self.size - 1:  # å³
            y += 1
        
        new_pos = (x, y)
        
        # æ£€æŸ¥æ˜¯å¦æ’žåˆ°éšœç¢ç‰©
        if new_pos in self.obstacles:
            new_pos = self.agent_pos  # ä¸ç§»åŠ¨
            reward = -1.0
        else:
            self.agent_pos = new_pos
            # å¥–åŠ±è®¾è®¡
            if new_pos == self.goal_state:
                reward = 10.0  # åˆ°è¾¾ç›®æ ‡
            else:
                reward = -0.1  # æ¯æ­¥å°æƒ©ç½šï¼Œé¼“åŠ±å¿«é€Ÿåˆ°è¾¾ç›®æ ‡
        
        done = (new_pos == self.goal_state)
        return new_pos, reward, done
    
    def render(self):
        """å¯è§†åŒ–å½“å‰çŠ¶æ€"""
        grid = [['.' for _ in range(self.size)] for _ in range(self.size)]
        
        # æ ‡è®°éšœç¢ç‰©
        for obs in self.obstacles:
            grid[obs[0]][obs[1]] = 'â–ˆ'
        
        # æ ‡è®°ç›®æ ‡
        grid[self.goal_state[0]][self.goal_state[1]] = 'G'
        
        # æ ‡è®°æ™ºèƒ½ä½“
        grid[self.agent_pos[0]][self.agent_pos[1]] = 'A'
        
        print("\nå½“å‰ç½‘æ ¼ä¸–ç•ŒçŠ¶æ€:")
        print("A: æ™ºèƒ½ä½“, G: ç›®æ ‡, â–ˆ: éšœç¢ç‰©, .: ç©ºåœ°")
        for row in grid:
            print(' '.join(row))
        print()

def demo_random_policy():
    """
    æ¼”ç¤ºéšæœºç­–ç•¥åœ¨ç½‘æ ¼ä¸–ç•Œä¸­çš„è¡¨çŽ°
    """
    print("ðŸŽ® éšæœºç­–ç•¥æ¼”ç¤º")
    print("=" * 30)
    
    env = SimpleGridWorld()
    total_episodes = 5
    
    for episode in range(total_episodes):
        state = env.reset()
        total_reward = 0
        steps = 0
        
        print(f"\nç¬¬ {episode + 1} å›žåˆ:")
        env.render()
        
        while steps < 50:  # æœ€å¤§æ­¥æ•°é™åˆ¶
            valid_actions = env.get_valid_actions(state)
            action = random.choice(valid_actions)
            
            action_names = ['ä¸Š', 'ä¸‹', 'å·¦', 'å³']
            print(f"æ­¥éª¤ {steps + 1}: æ‰§è¡ŒåŠ¨ä½œ '{action_names[action]}'")
            
            state, reward, done = env.step(action)
            total_reward += reward
            steps += 1
            
            env.render()
            
            if done:
                print(f"ðŸŽ‰ æˆåŠŸåˆ°è¾¾ç›®æ ‡! æ€»å¥–åŠ±: {total_reward:.1f}, æ­¥æ•°: {steps}")
                break
        
        if not done:
            print(f"âŒ æœªèƒ½åˆ°è¾¾ç›®æ ‡ï¼Œæ€»å¥–åŠ±: {total_reward:.1f}")

def explain_value_functions():
    """
    è§£é‡Šä»·å€¼å‡½æ•°çš„æ¦‚å¿µ
    """
    explanation = """
    ðŸ“Š ä»·å€¼å‡½æ•°è¯¦è§£
    
    1. çŠ¶æ€ä»·å€¼å‡½æ•° V(s):
       - å®šä¹‰: ä»ŽçŠ¶æ€så¼€å§‹ï¼Œéµå¾ªç­–ç•¥Ï€çš„æœŸæœ›ç´¯ç§¯å¥–åŠ±
       - å…¬å¼: V^Ï€(s) = E[G_t | S_t = s]
       - å«ä¹‰: å‘Šè¯‰æˆ‘ä»¬åœ¨æŸä¸ªçŠ¶æ€"æœ‰å¤šå¥½"
    
    2. åŠ¨ä½œä»·å€¼å‡½æ•° Q(s,a):
       - å®šä¹‰: åœ¨çŠ¶æ€sæ‰§è¡ŒåŠ¨ä½œaï¼Œç„¶åŽéµå¾ªç­–ç•¥Ï€çš„æœŸæœ›ç´¯ç§¯å¥–åŠ±
       - å…¬å¼: Q^Ï€(s,a) = E[G_t | S_t = s, A_t = a]
       - å«ä¹‰: å‘Šè¯‰æˆ‘ä»¬åœ¨æŸä¸ªçŠ¶æ€æ‰§è¡ŒæŸä¸ªåŠ¨ä½œ"æœ‰å¤šå¥½"
    
    3. Bellmanæ–¹ç¨‹:
       - V(s) = Î£ Ï€(a|s) * Î£ P(s'|s,a) * [R(s,a,s') + Î³*V(s')]
       - Q(s,a) = Î£ P(s'|s,a) * [R(s,a,s') + Î³*Î£ Ï€(a'|s')*Q(s',a')]
    
    4. æœ€ä¼˜ä»·å€¼å‡½æ•°:
       - V*(s) = max_Ï€ V^Ï€(s)
       - Q*(s,a) = max_Ï€ Q^Ï€(s,a)
    """
    print(explanation)

def main():
    """
    å¼ºåŒ–å­¦ä¹ åŸºç¡€æ•™ç¨‹ä¸»å‡½æ•°
    """
    print("ðŸš€ æ¬¢è¿Žæ¥åˆ°å¼ºåŒ–å­¦ä¹ å…¥é—¨æ•™ç¨‹!")
    print("=" * 50)
    
    # åŸºç¡€æ¦‚å¿µä»‹ç»
    rl_basics = RLBasics()
    rl_basics.explain_concepts()
    rl_basics.rl_process_flow()
    
    # ä»·å€¼å‡½æ•°è§£é‡Š
    explain_value_functions()
    
    # éšæœºç­–ç•¥æ¼”ç¤º
    demo_random_policy()
    
    print("\nðŸ“š å­¦ä¹ å»ºè®®:")
    print("1. ç†è§£æ™ºèƒ½ä½“ä¸ŽçŽ¯å¢ƒçš„äº¤äº’è¿‡ç¨‹")
    print("2. æŽŒæ¡å¥–åŠ±è®¾è®¡çš„é‡è¦æ€§")
    print("3. ç†è§£æŽ¢ç´¢ä¸Žåˆ©ç”¨çš„æƒè¡¡")
    print("4. å­¦ä¹ ä»·å€¼å‡½æ•°çš„æ¦‚å¿µ")
    print("5. ç»§ç»­å­¦ä¹ å…·ä½“ç®—æ³• (Q-Learning, DQNç­‰)")

if __name__ == "__main__":
    main()