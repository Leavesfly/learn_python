"""
Q-Learningç®—æ³•å®ç°
ç»å…¸çš„è¡¨æ ¼å¼å¼ºåŒ–å­¦ä¹ ç®—æ³•

Q-Learningæ˜¯ä¸€ç§off-policyçš„æ—¶åºå·®åˆ†å­¦ä¹ ç®—æ³•
æ ¸å¿ƒæ€æƒ³ï¼šé€šè¿‡è¿­ä»£æ›´æ–°Qè¡¨æ¥å­¦ä¹ æœ€ä¼˜åŠ¨ä½œä»·å€¼å‡½æ•°
"""

import numpy as np
import matplotlib.pyplot as plt
import random
from collections import defaultdict
import pickle

class QLearningAgent:
    """
    Q-Learningæ™ºèƒ½ä½“å®ç°
    """
    
    def __init__(self, 
                 state_size: int,
                 action_size: int, 
                 learning_rate: float = 0.1,
                 discount_factor: float = 0.95,
                 epsilon: float = 1.0,
                 epsilon_decay: float = 0.995,
                 epsilon_min: float = 0.01):
        """
        åˆå§‹åŒ–Q-Learningæ™ºèƒ½ä½“
        
        Args:
            state_size: çŠ¶æ€ç©ºé—´å¤§å°
            action_size: åŠ¨ä½œç©ºé—´å¤§å°
            learning_rate: å­¦ä¹ ç‡ Î±
            discount_factor: æŠ˜æ‰£å› å­ Î³
            epsilon: Îµ-è´ªå©ªç­–ç•¥çš„æ¢ç´¢ç‡
            epsilon_decay: Îµè¡°å‡ç‡
            epsilon_min: Îµçš„æœ€å°å€¼
        """
        self.state_size = state_size
        self.action_size = action_size
        self.lr = learning_rate
        self.gamma = discount_factor
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = epsilon_min
        
        # åˆå§‹åŒ–Qè¡¨
        self.q_table = defaultdict(lambda: np.zeros(action_size))
        
        # è®°å½•è®­ç»ƒè¿‡ç¨‹
        self.training_history = {
            'episode_rewards': [],
            'episode_steps': [],
            'epsilon_history': []
        }
    
    def get_state_key(self, state):
        """å°†çŠ¶æ€è½¬æ¢ä¸ºQè¡¨çš„é”®"""
        if isinstance(state, tuple):
            return state
        elif isinstance(state, np.ndarray):
            return tuple(state)
        else:
            return state
    
    def choose_action(self, state, valid_actions=None):
        """
        Îµ-è´ªå©ªç­–ç•¥é€‰æ‹©åŠ¨ä½œ
        
        Args:
            state: å½“å‰çŠ¶æ€
            valid_actions: æœ‰æ•ˆåŠ¨ä½œåˆ—è¡¨
            
        Returns:
            é€‰æ‹©çš„åŠ¨ä½œ
        """
        state_key = self.get_state_key(state)
        
        if valid_actions is None:
            valid_actions = list(range(self.action_size))
        
        # Îµ-è´ªå©ªç­–ç•¥
        if random.random() < self.epsilon:
            # æ¢ç´¢ï¼šéšæœºé€‰æ‹©åŠ¨ä½œ
            return random.choice(valid_actions)
        else:
            # åˆ©ç”¨ï¼šé€‰æ‹©Qå€¼æœ€å¤§çš„åŠ¨ä½œ
            q_values = self.q_table[state_key]
            valid_q_values = [(action, q_values[action]) for action in valid_actions]
            best_action = max(valid_q_values, key=lambda x: x[1])[0]
            return best_action
    
    def update(self, state, action, reward, next_state, done):
        """
        Q-Learningæ›´æ–°è§„åˆ™
        
        Q(s,a) â† Q(s,a) + Î±[r + Î³*max_a'Q(s',a') - Q(s,a)]
        """
        state_key = self.get_state_key(state)
        next_state_key = self.get_state_key(next_state)
        
        current_q = self.q_table[state_key][action]
        
        if done:
            # ç»ˆæ­¢çŠ¶æ€ï¼Œä¸‹ä¸€ä¸ªçŠ¶æ€çš„ä»·å€¼ä¸º0
            target_q = reward
        else:
            # é€‰æ‹©ä¸‹ä¸€ä¸ªçŠ¶æ€çš„æœ€å¤§Qå€¼
            next_max_q = np.max(self.q_table[next_state_key])
            target_q = reward + self.gamma * next_max_q
        
        # Q-Learningæ›´æ–°å…¬å¼
        self.q_table[state_key][action] = current_q + self.lr * (target_q - current_q)
    
    def decay_epsilon(self):
        """è¡°å‡Îµå€¼"""
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
    
    def save_model(self, filepath: str):
        """ä¿å­˜Qè¡¨"""
        with open(filepath, 'wb') as f:
            pickle.dump(dict(self.q_table), f)
    
    def load_model(self, filepath: str):
        """åŠ è½½Qè¡¨"""
        with open(filepath, 'rb') as f:
            loaded_q_table = pickle.load(f)
            self.q_table = defaultdict(lambda: np.zeros(self.action_size))
            self.q_table.update(loaded_q_table)

class FrozenLakeEnvironment:
    """
    å†°æ¹–ç¯å¢ƒ - ç»å…¸å¼ºåŒ–å­¦ä¹ é—®é¢˜
    
    4x4ç½‘æ ¼ï¼Œæ™ºèƒ½ä½“éœ€è¦ä»èµ·ç‚¹(0,0)åˆ°è¾¾ç›®æ ‡(3,3)
    åœ°é¢æœ‰æ´ï¼Œæ‰è¿›å»æ¸¸æˆç»“æŸ
    """
    
    def __init__(self, size=4, hole_prob=0.1):
        self.size = size
        self.start = (0, 0)
        self.goal = (size-1, size-1)
        
        # éšæœºç”Ÿæˆæ´çš„ä½ç½®
        self.holes = set()
        for i in range(size):
            for j in range(size):
                if (i, j) != self.start and (i, j) != self.goal:
                    if random.random() < hole_prob:
                        self.holes.add((i, j))
        
        # ç¡®ä¿è‡³å°‘æœ‰ä¸€æ¡è·¯å¾„å¯è¾¾
        if len(self.holes) > size * size * 0.3:
            self.holes = set(list(self.holes)[:int(size * size * 0.3)])
        
        self.reset()
    
    def reset(self):
        """é‡ç½®ç¯å¢ƒ"""
        self.agent_pos = self.start
        return self.agent_pos
    
    def get_valid_actions(self, state):
        """è·å–æœ‰æ•ˆåŠ¨ä½œ"""
        valid_actions = []
        x, y = state
        
        # 0: ä¸Š, 1: ä¸‹, 2: å·¦, 3: å³
        if x > 0: valid_actions.append(0)
        if x < self.size - 1: valid_actions.append(1)
        if y > 0: valid_actions.append(2)
        if y < self.size - 1: valid_actions.append(3)
        
        return valid_actions
    
    def step(self, action):
        """æ‰§è¡ŒåŠ¨ä½œ"""
        x, y = self.agent_pos
        
        # ç§»åŠ¨
        if action == 0 and x > 0:  # ä¸Š
            x -= 1
        elif action == 1 and x < self.size - 1:  # ä¸‹
            x += 1
        elif action == 2 and y > 0:  # å·¦
            y -= 1
        elif action == 3 and y < self.size - 1:  # å³
            y += 1
        
        self.agent_pos = (x, y)
        
        # è®¡ç®—å¥–åŠ±
        if self.agent_pos in self.holes:
            reward = -10.0
            done = True
        elif self.agent_pos == self.goal:
            reward = 10.0
            done = True
        else:
            reward = -0.1  # æ¯æ­¥å°æƒ©ç½š
            done = False
        
        return self.agent_pos, reward, done
    
    def render(self):
        """å¯è§†åŒ–ç¯å¢ƒ"""
        grid = [['.' for _ in range(self.size)] for _ in range(self.size)]
        
        # æ ‡è®°æ´
        for hole in self.holes:
            grid[hole[0]][hole[1]] = 'H'
        
        # æ ‡è®°ç›®æ ‡
        grid[self.goal[0]][self.goal[1]] = 'G'
        
        # æ ‡è®°æ™ºèƒ½ä½“
        if self.agent_pos not in self.holes:
            grid[self.agent_pos[0]][self.agent_pos[1]] = 'A'
        
        print("\nå†°æ¹–ç¯å¢ƒ:")
        print("A: æ™ºèƒ½ä½“, G: ç›®æ ‡, H: æ´, .: å®‰å…¨åŒºåŸŸ")
        for row in grid:
            print(' '.join(row))
        print()

def train_q_learning(episodes=1000, render_interval=200):
    """
    è®­ç»ƒQ-Learningæ™ºèƒ½ä½“
    """
    print("ğŸ¯ å¼€å§‹Q-Learningè®­ç»ƒ")
    print("=" * 40)
    
    # åˆ›å»ºç¯å¢ƒå’Œæ™ºèƒ½ä½“
    env = FrozenLakeEnvironment()
    agent = QLearningAgent(
        state_size=16,  # 4x4ç½‘æ ¼
        action_size=4,   # ä¸Šä¸‹å·¦å³
        learning_rate=0.1,
        discount_factor=0.95,
        epsilon=1.0,
        epsilon_decay=0.995,
        epsilon_min=0.01
    )
    
    print("ç¯å¢ƒè®¾ç½®:")
    env.render()
    
    # è®­ç»ƒå¾ªç¯
    for episode in range(episodes):
        state = env.reset()
        total_reward = 0
        steps = 0
        
        while steps < 100:  # æœ€å¤§æ­¥æ•°é™åˆ¶
            # é€‰æ‹©åŠ¨ä½œ
            valid_actions = env.get_valid_actions(state)
            action = agent.choose_action(state, valid_actions)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, done = env.step(action)
            
            # æ›´æ–°Qè¡¨
            agent.update(state, action, reward, next_state, done)
            
            state = next_state
            total_reward += reward
            steps += 1
            
            if done:
                break
        
        # è®°å½•è®­ç»ƒæ•°æ®
        agent.training_history['episode_rewards'].append(total_reward)
        agent.training_history['episode_steps'].append(steps)
        agent.training_history['epsilon_history'].append(agent.epsilon)
        
        # è¡°å‡Îµ
        agent.decay_epsilon()
        
        # å®šæœŸè¾“å‡ºè®­ç»ƒä¿¡æ¯
        if episode % render_interval == 0:
            avg_reward = np.mean(agent.training_history['episode_rewards'][-100:])
            print(f"å›åˆ {episode}: å¹³å‡å¥–åŠ±={avg_reward:.2f}, Îµ={agent.epsilon:.3f}")
    
    return agent, env

def test_trained_agent(agent, env, num_tests=5):
    """
    æµ‹è¯•è®­ç»ƒå¥½çš„æ™ºèƒ½ä½“
    """
    print("\nğŸ§ª æµ‹è¯•è®­ç»ƒå¥½çš„æ™ºèƒ½ä½“")
    print("=" * 30)
    
    # æš‚æ—¶å…³é—­æ¢ç´¢
    original_epsilon = agent.epsilon
    agent.epsilon = 0.0
    
    success_count = 0
    
    for test in range(num_tests):
        state = env.reset()
        total_reward = 0
        steps = 0
        
        print(f"\næµ‹è¯• {test + 1}:")
        env.render()
        
        while steps < 50:
            valid_actions = env.get_valid_actions(state)
            action = agent.choose_action(state, valid_actions)
            
            action_names = ['ä¸Š', 'ä¸‹', 'å·¦', 'å³']
            print(f"æ­¥éª¤ {steps + 1}: {action_names[action]}")
            
            state, reward, done = env.step(action)
            total_reward += reward
            steps += 1
            
            env.render()
            
            if done:
                if reward > 0:
                    print(f"âœ… æˆåŠŸåˆ°è¾¾ç›®æ ‡! å¥–åŠ±: {total_reward:.1f}, æ­¥æ•°: {steps}")
                    success_count += 1
                else:
                    print(f"âŒ æ‰è¿›æ´é‡Œ! å¥–åŠ±: {total_reward:.1f}")
                break
        
        if not done:
            print(f"â° è¶…æ—¶æœªå®Œæˆ")
    
    # æ¢å¤åŸå§‹Îµå€¼
    agent.epsilon = original_epsilon
    
    success_rate = success_count / num_tests * 100
    print(f"\nğŸ“Š æˆåŠŸç‡: {success_rate:.1f}% ({success_count}/{num_tests})")

def visualize_training_progress(agent):
    """
    å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹
    """
    print("\nğŸ“ˆ ç”Ÿæˆè®­ç»ƒè¿‡ç¨‹å›¾è¡¨...")
    
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 8))
    
    # å›åˆå¥–åŠ±
    episodes = range(len(agent.training_history['episode_rewards']))
    ax1.plot(episodes, agent.training_history['episode_rewards'], alpha=0.6)
    ax1.set_title('æ¯å›åˆå¥–åŠ±')
    ax1.set_xlabel('å›åˆ')
    ax1.set_ylabel('å¥–åŠ±')
    ax1.grid(True)
    
    # ç§»åŠ¨å¹³å‡å¥–åŠ±
    window = 50
    if len(agent.training_history['episode_rewards']) >= window:
        moving_avg = []
        for i in range(window-1, len(agent.training_history['episode_rewards'])):
            avg = np.mean(agent.training_history['episode_rewards'][i-window+1:i+1])
            moving_avg.append(avg)
        
        ax2.plot(range(window-1, len(agent.training_history['episode_rewards'])), moving_avg)
        ax2.set_title(f'{window}å›åˆç§»åŠ¨å¹³å‡å¥–åŠ±')
        ax2.set_xlabel('å›åˆ')
        ax2.set_ylabel('å¹³å‡å¥–åŠ±')
        ax2.grid(True)
    
    # Îµå€¼å˜åŒ–
    ax3.plot(episodes, agent.training_history['epsilon_history'])
    ax3.set_title('æ¢ç´¢ç‡(Îµ)å˜åŒ–')
    ax3.set_xlabel('å›åˆ')
    ax3.set_ylabel('Îµå€¼')
    ax3.grid(True)
    
    # å›åˆæ­¥æ•°
    ax4.plot(episodes, agent.training_history['episode_steps'], alpha=0.6)
    ax4.set_title('æ¯å›åˆæ­¥æ•°')
    ax4.set_xlabel('å›åˆ')
    ax4.set_ylabel('æ­¥æ•°')
    ax4.grid(True)
    
    plt.tight_layout()
    plt.savefig('q_learning_training_progress.png', dpi=150, bbox_inches='tight')
    plt.show()
    print("å›¾è¡¨å·²ä¿å­˜ä¸º q_learning_training_progress.png")

def analyze_q_table(agent, env):
    """
    åˆ†æå­¦ä¹ åˆ°çš„Qè¡¨
    """
    print("\nğŸ” Qè¡¨åˆ†æ")
    print("=" * 20)
    
    print("å­¦ä¹ åˆ°çš„çŠ¶æ€-åŠ¨ä½œä»·å€¼:")
    action_names = ['ä¸Š', 'ä¸‹', 'å·¦', 'å³']
    
    # æ˜¾ç¤ºéƒ¨åˆ†Qå€¼
    states_to_show = [(0, 0), (0, 1), (1, 0), (3, 3)]
    for state in states_to_show:
        if state in agent.q_table:
            q_values = agent.q_table[state]
            print(f"\nçŠ¶æ€ {state}:")
            for action, q_val in enumerate(q_values):
                print(f"  {action_names[action]}: {q_val:.3f}")
            best_action = np.argmax(q_values)
            print(f"  æœ€ä½³åŠ¨ä½œ: {action_names[best_action]}")

def main():
    """
    Q-Learningä¸»ç¨‹åº
    """
    print("ğŸš€ Q-Learningç®—æ³•æ¼”ç¤º")
    print("=" * 50)
    
    # è®­ç»ƒæ™ºèƒ½ä½“
    agent, env = train_q_learning(episodes=1000, render_interval=200)
    
    # æµ‹è¯•æ™ºèƒ½ä½“
    test_trained_agent(agent, env, num_tests=5)
    
    # åˆ†æQè¡¨
    analyze_q_table(agent, env)
    
    # å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹
    try:
        visualize_training_progress(agent)
    except Exception as e:
        print(f"å¯è§†åŒ–æ—¶å‡ºé”™: {e}")
        print("è¯·ç¡®ä¿å®‰è£…äº†matplotlib: pip install matplotlib")
    
    # ä¿å­˜æ¨¡å‹
    agent.save_model('q_learning_model.pkl')
    print("\nğŸ’¾ æ¨¡å‹å·²ä¿å­˜ä¸º q_learning_model.pkl")
    
    print("\nğŸ“ Q-Learningå­¦ä¹ è¦ç‚¹:")
    print("1. Q-Learningæ˜¯off-policyç®—æ³•ï¼Œä¸éœ€è¦éµå¾ªå½“å‰ç­–ç•¥")
    print("2. é€šè¿‡Bellmanæ–¹ç¨‹è¿­ä»£æ›´æ–°Qå€¼")
    print("3. Îµ-è´ªå©ªç­–ç•¥å¹³è¡¡æ¢ç´¢å’Œåˆ©ç”¨")
    print("4. é€‚ç”¨äºçŠ¶æ€ç©ºé—´è¾ƒå°çš„é—®é¢˜")
    print("5. æ”¶æ•›åˆ°æœ€ä¼˜ç­–ç•¥çš„ç†è®ºä¿è¯")

if __name__ == "__main__":
    main()