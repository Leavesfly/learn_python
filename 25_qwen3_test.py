"""
Qwen3 æ¨¡å‹æµ‹è¯•æ–‡ä»¶
æµ‹è¯•å„ä¸ªç»„ä»¶çš„åŠŸèƒ½ï¼ŒéªŒè¯æ¨¡å‹å®ç°çš„æ­£ç¡®æ€§
ä¸ä¾èµ–PyTorchï¼Œä½¿ç”¨çº¯Pythonè¿›è¡Œé€»è¾‘éªŒè¯
"""

import math
import json
from typing import List, Dict, Tuple, Optional, Any


def test_config():
    """æµ‹è¯•é…ç½®ç±»"""
    print("ğŸ§ª æµ‹è¯• Qwen3Config...")
    
    # æ¨¡æ‹Ÿé…ç½®ç±»
    class Qwen3Config:
        def __init__(self, **kwargs):
            # é»˜è®¤é…ç½®
            defaults = {
                'vocab_size': 32000,
                'hidden_size': 2048,
                'intermediate_size': 5632,
                'num_hidden_layers': 24,
                'num_attention_heads': 16,
                'num_key_value_heads': 16,
                'max_position_embeddings': 8192,
                'rope_theta': 10000.0,
                'rms_norm_eps': 1e-6,
                'pad_token_id': 0,
                'bos_token_id': 1,
                'eos_token_id': 2,
                'tie_word_embeddings': False
            }
            
            # åº”ç”¨ç”¨æˆ·é…ç½®
            for key, value in defaults.items():
                setattr(self, key, kwargs.get(key, value))
    
    # æµ‹è¯•é»˜è®¤é…ç½®
    config = Qwen3Config()
    assert config.vocab_size == 32000
    assert config.hidden_size == 2048
    assert config.num_attention_heads == 16
    
    # æµ‹è¯•è‡ªå®šä¹‰é…ç½®
    custom_config = Qwen3Config(
        vocab_size=1000,
        hidden_size=512,
        num_hidden_layers=6
    )
    assert custom_config.vocab_size == 1000
    assert custom_config.hidden_size == 512
    assert custom_config.num_hidden_layers == 6
    assert custom_config.num_attention_heads == 16  # ä¿æŒé»˜è®¤å€¼
    
    print("âœ… é…ç½®ç±»æµ‹è¯•é€šè¿‡")
    return True


def test_rms_norm():
    """æµ‹è¯• RMSNorm å½’ä¸€åŒ–"""
    print("\nğŸ§ª æµ‹è¯• RMSNorm...")
    
    def rms_norm_python(x: List[float], weight: List[float], eps: float = 1e-6) -> List[float]:
        """çº¯Pythonå®ç°çš„RMSNorm"""
        # è®¡ç®—å‡æ–¹æ ¹
        mean_square = sum(val ** 2 for val in x) / len(x)
        rms = math.sqrt(mean_square + eps)
        
        # å½’ä¸€åŒ–å¹¶åº”ç”¨æƒé‡
        return [w * (val / rms) for val, w in zip(x, weight)]
    
    # æµ‹è¯•æ•°æ®
    hidden_size = 4
    test_input = [1.0, 2.0, 3.0, 4.0]
    weight = [1.0, 1.0, 1.0, 1.0]
    
    # è®¡ç®—é¢„æœŸç»“æœ
    expected = rms_norm_python(test_input, weight)
    
    # éªŒè¯å½’ä¸€åŒ–åçš„å‡æ–¹æ ¹æ¥è¿‘1
    normalized_rms = math.sqrt(sum(val ** 2 for val in expected) / len(expected))
    assert abs(normalized_rms - 1.0) < 1e-5, f"å½’ä¸€åŒ–åçš„RMSåº”è¯¥æ¥è¿‘1.0ï¼Œå®é™…ä¸º{normalized_rms}"
    
    print(f"  è¾“å…¥: {test_input}")
    print(f"  è¾“å‡º: {[f'{x:.4f}' for x in expected]}")
    print(f"  å½’ä¸€åŒ–åRMS: {normalized_rms:.6f}")
    print("âœ… RMSNormæµ‹è¯•é€šè¿‡")
    return True


def test_rope():
    """æµ‹è¯•æ—‹è½¬ä½ç½®ç¼–ç """
    print("\nğŸ§ª æµ‹è¯• RoPE (æ—‹è½¬ä½ç½®ç¼–ç )...")
    
    def rope_python(seq_len: int, dim: int, base: float = 10000.0) -> Tuple[List[List[float]], List[List[float]]]:
        """çº¯Pythonå®ç°çš„RoPE"""
        # è®¡ç®—é€†é¢‘ç‡
        inv_freq = [1.0 / (base ** (i / dim)) for i in range(0, dim, 2)]
        
        cos_values = []
        sin_values = []
        
        for pos in range(seq_len):
            cos_row = []
            sin_row = []
            
            for freq in inv_freq:
                angle = pos * freq
                cos_row.extend([math.cos(angle), math.cos(angle)])
                sin_row.extend([math.sin(angle), math.sin(angle)])
            
            cos_values.append(cos_row)
            sin_values.append(sin_row)
        
        return cos_values, sin_values
    
    # æµ‹è¯•å‚æ•°
    seq_len = 3
    dim = 4  # å¤´éƒ¨ç»´åº¦
    
    cos, sin = rope_python(seq_len, dim)
    
    # éªŒè¯è¾“å‡ºå½¢çŠ¶
    assert len(cos) == seq_len, f"cosåº”æœ‰{seq_len}è¡Œï¼Œå®é™…{len(cos)}è¡Œ"
    assert len(cos[0]) == dim, f"cosæ¯è¡Œåº”æœ‰{dim}ä¸ªå…ƒç´ ï¼Œå®é™…{len(cos[0])}ä¸ª"
    assert len(sin) == seq_len, f"sinåº”æœ‰{seq_len}è¡Œï¼Œå®é™…{len(sin)}è¡Œ"
    assert len(sin[0]) == dim, f"sinæ¯è¡Œåº”æœ‰{dim}ä¸ªå…ƒç´ ï¼Œå®é™…{len(sin[0])}ä¸ª"
    
    # éªŒè¯ä¸‰è§’å‡½æ•°æ€§è´¨ï¼šcosÂ²+sinÂ²=1
    for i in range(seq_len):
        for j in range(0, dim, 2):  # æ¯å¯¹é¢‘ç‡
            cos_val = cos[i][j]
            sin_val = sin[i][j]
            identity = cos_val ** 2 + sin_val ** 2
            assert abs(identity - 1.0) < 1e-10, f"ä½ç½®{i}é¢‘ç‡{j//2}: cosÂ²+sinÂ²={identity}, åº”è¯¥ç­‰äº1"
    
    print(f"  åºåˆ—é•¿åº¦: {seq_len}, ç»´åº¦: {dim}")
    print(f"  coså½¢çŠ¶: {len(cos)} x {len(cos[0])}")
    print(f"  sinå½¢çŠ¶: {len(sin)} x {len(sin[0])}")
    print("  ä¸‰è§’å‡½æ•°æ’ç­‰å¼éªŒè¯é€šè¿‡")
    print("âœ… RoPEæµ‹è¯•é€šè¿‡")
    return True


def test_attention_mask():
    """æµ‹è¯•æ³¨æ„åŠ›æ©ç """
    print("\nğŸ§ª æµ‹è¯•æ³¨æ„åŠ›æ©ç ...")
    
    def create_causal_mask(seq_len: int) -> List[List[float]]:
        """åˆ›å»ºå› æœæ©ç """
        mask = []
        for i in range(seq_len):
            row = []
            for j in range(seq_len):
                if j > i:
                    row.append(float('-inf'))  # å±è”½æœªæ¥ä½ç½®
                else:
                    row.append(0.0)  # å…è®¸å½“å‰å’Œè¿‡å»ä½ç½®
            mask.append(row)
        return mask
    
    def create_padding_mask(attention_mask: List[int], seq_len: int) -> List[List[float]]:
        """åˆ›å»ºå¡«å……æ©ç """
        mask = []
        for i in range(len(attention_mask)):
            row = []
            for j in range(seq_len):
                if attention_mask[j] == 0:  # å¡«å……ä½ç½®
                    row.append(float('-inf'))
                else:
                    row.append(0.0)
            mask.append(row)
        return mask
    
    # æµ‹è¯•å› æœæ©ç 
    seq_len = 4
    causal_mask = create_causal_mask(seq_len)
    
    # éªŒè¯å› æœæ©ç çš„ä¸‰è§’å½¢ç»“æ„
    for i in range(seq_len):
        for j in range(seq_len):
            if j > i:
                assert causal_mask[i][j] == float('-inf'), f"ä½ç½®({i},{j})åº”è¯¥è¢«å±è”½"
            else:
                assert causal_mask[i][j] == 0.0, f"ä½ç½®({i},{j})åº”è¯¥å¯è§"
    
    # æµ‹è¯•å¡«å……æ©ç 
    attention_mask = [1, 1, 0, 0]  # å‰ä¸¤ä¸ªtokenæœ‰æ•ˆï¼Œåä¸¤ä¸ªæ˜¯å¡«å……
    padding_mask = create_padding_mask(attention_mask, seq_len)
    
    for j in range(seq_len):
        if attention_mask[j] == 0:
            assert padding_mask[0][j] == float('-inf'), f"å¡«å……ä½ç½®{j}åº”è¯¥è¢«å±è”½"
        else:
            assert padding_mask[0][j] == 0.0, f"æœ‰æ•ˆä½ç½®{j}åº”è¯¥å¯è§"
    
    print(f"  åºåˆ—é•¿åº¦: {seq_len}")
    print("  å› æœæ©ç  (ä¸‹ä¸‰è§’):")
    for i, row in enumerate(causal_mask):
        formatted_row = [f"{x:>6}" if x == 0.0 else " -inf" for x in row]
        print(f"    {i}: {formatted_row}")
    
    print("  å¡«å……æ©ç  (attention_mask=[1,1,0,0]):")
    formatted_padding = [f"{x:>6}" if x == 0.0 else " -inf" for x in padding_mask[0]]
    print(f"    0: {formatted_padding}")
    
    print("âœ… æ³¨æ„åŠ›æ©ç æµ‹è¯•é€šè¿‡")
    return True


def test_swiglu_activation():
    """æµ‹è¯• SwiGLU æ¿€æ´»å‡½æ•°"""
    print("\nğŸ§ª æµ‹è¯• SwiGLU æ¿€æ´»å‡½æ•°...")
    
    def swiglu_python(x: List[float], gate_w: List[List[float]], up_w: List[List[float]]) -> List[float]:
        """çº¯Pythonå®ç°çš„SwiGLU"""
        # é—¨æ§æŠ•å½±: gate = x @ gate_w
        gate = [sum(x[j] * gate_w[j][i] for j in range(len(x))) for i in range(len(gate_w[0]))]
        
        # ä¸ŠæŠ•å½±: up = x @ up_w  
        up = [sum(x[j] * up_w[j][i] for j in range(len(x))) for i in range(len(up_w[0]))]
        
        # Swishæ¿€æ´»: swish(gate) = gate * sigmoid(gate)
        def sigmoid(val):
            return 1.0 / (1.0 + math.exp(-val))
        
        swish_gate = [g * sigmoid(g) for g in gate]
        
        # å…ƒç´ çº§ä¹˜æ³•: swish_gate * up
        result = [sg * u for sg, u in zip(swish_gate, up)]
        
        return result
    
    # æµ‹è¯•æ•°æ®
    input_dim = 3
    intermediate_dim = 4
    
    x = [1.0, 2.0, 3.0]
    
    # ç®€å•çš„æƒé‡çŸ©é˜µ
    gate_w = [[0.1, 0.2, 0.3, 0.4],
              [0.5, 0.6, 0.7, 0.8], 
              [0.9, 1.0, 1.1, 1.2]]
    
    up_w = [[0.2, 0.3, 0.4, 0.5],
            [0.6, 0.7, 0.8, 0.9],
            [1.0, 1.1, 1.2, 1.3]]
    
    result = swiglu_python(x, gate_w, up_w)
    
    # éªŒè¯è¾“å‡ºç»´åº¦
    assert len(result) == intermediate_dim, f"è¾“å‡ºç»´åº¦åº”ä¸º{intermediate_dim}ï¼Œå®é™…ä¸º{len(result)}"
    
    # éªŒè¯è¾“å‡ºä¸å…¨ä¸ºé›¶ï¼ˆæ¿€æ´»å‡½æ•°åº”è¯¥æœ‰è¾“å‡ºï¼‰
    assert any(abs(val) > 1e-6 for val in result), "SwiGLUè¾“å‡ºä¸åº”å…¨ä¸ºé›¶"
    
    print(f"  è¾“å…¥ç»´åº¦: {input_dim}")
    print(f"  ä¸­é—´ç»´åº¦: {intermediate_dim}")
    print(f"  è¾“å…¥: {x}")
    print(f"  è¾“å‡º: {[f'{x:.4f}' for x in result]}")
    print("âœ… SwiGLUæµ‹è¯•é€šè¿‡")
    return True


def test_attention_computation():
    """æµ‹è¯•æ³¨æ„åŠ›è®¡ç®—"""
    print("\nğŸ§ª æµ‹è¯•æ³¨æ„åŠ›è®¡ç®—...")
    
    def attention_python(q: List[List[float]], k: List[List[float]], v: List[List[float]], 
                         mask: Optional[List[List[float]]] = None) -> List[List[float]]:
        """çº¯Pythonå®ç°çš„æ³¨æ„åŠ›è®¡ç®—"""
        seq_len = len(q)
        head_dim = len(q[0])
        
        # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°: scores = Q @ K^T / sqrt(head_dim)
        scores = []
        scale = 1.0 / math.sqrt(head_dim)
        
        for i in range(seq_len):
            score_row = []
            for j in range(seq_len):
                # q[i] Â· k[j]
                dot_product = sum(q[i][d] * k[j][d] for d in range(head_dim))
                scaled_score = dot_product * scale
                score_row.append(scaled_score)
            scores.append(score_row)
        
        # åº”ç”¨æ©ç 
        if mask is not None:
            for i in range(seq_len):
                for j in range(seq_len):
                    scores[i][j] += mask[i][j]
        
        # Softmax
        def softmax(row):
            max_val = max(row)
            exp_vals = [math.exp(x - max_val) for x in row]
            sum_exp = sum(exp_vals)
            return [x / sum_exp for x in exp_vals]
        
        attention_weights = [softmax(row) for row in scores]
        
        # åº”ç”¨æ³¨æ„åŠ›æƒé‡åˆ°å€¼: output = attention_weights @ V
        output = []
        for i in range(seq_len):
            output_row = []
            for d in range(head_dim):
                weighted_sum = sum(attention_weights[i][j] * v[j][d] for j in range(seq_len))
                output_row.append(weighted_sum)
            output.append(output_row)
        
        return output, attention_weights
    
    # æµ‹è¯•æ•°æ®
    seq_len = 3
    head_dim = 4
    
    # ç®€å•çš„æŸ¥è¯¢ã€é”®ã€å€¼çŸ©é˜µ
    q = [[1.0, 0.0, 0.0, 0.0],
         [0.0, 1.0, 0.0, 0.0], 
         [0.0, 0.0, 1.0, 0.0]]
    
    k = [[1.0, 0.0, 0.0, 0.0],
         [0.0, 1.0, 0.0, 0.0],
         [0.0, 0.0, 1.0, 0.0]]
    
    v = [[1.0, 2.0, 3.0, 4.0],
         [5.0, 6.0, 7.0, 8.0],
         [9.0, 10.0, 11.0, 12.0]]
    
    # ä¸ä½¿ç”¨æ©ç çš„æƒ…å†µ
    output, weights = attention_python(q, k, v)
    
    # éªŒè¯æ³¨æ„åŠ›æƒé‡çš„æ€§è´¨
    for i in range(seq_len):
        weight_sum = sum(weights[i])
        assert abs(weight_sum - 1.0) < 1e-6, f"æ³¨æ„åŠ›æƒé‡è¡Œ{i}çš„å’Œåº”ä¸º1.0ï¼Œå®é™…ä¸º{weight_sum}"
    
    # éªŒè¯è¾“å‡ºå½¢çŠ¶
    assert len(output) == seq_len, f"è¾“å‡ºè¡Œæ•°åº”ä¸º{seq_len}"
    assert len(output[0]) == head_dim, f"è¾“å‡ºåˆ—æ•°åº”ä¸º{head_dim}"
    
    print(f"  åºåˆ—é•¿åº¦: {seq_len}, å¤´éƒ¨ç»´åº¦: {head_dim}")
    print("  æ³¨æ„åŠ›æƒé‡çŸ©é˜µ:")
    for i, row in enumerate(weights):
        formatted_row = [f"{x:.4f}" for x in row]
        print(f"    {i}: {formatted_row}")
    
    print("  è¾“å‡º:")
    for i, row in enumerate(output):
        formatted_row = [f"{x:.4f}" for x in row]
        print(f"    {i}: {formatted_row}")
    
    # æµ‹è¯•å¸¦å› æœæ©ç çš„æƒ…å†µ
    causal_mask = [[0.0, float('-inf'), float('-inf')],
                   [0.0, 0.0, float('-inf')],
                   [0.0, 0.0, 0.0]]
    
    masked_output, masked_weights = attention_python(q, k, v, causal_mask)
    
    # éªŒè¯å› æœæ©ç çš„æ•ˆæœ
    for i in range(seq_len):
        for j in range(i+1, seq_len):
            assert masked_weights[i][j] < 1e-6, f"å› æœæ©ç åä½ç½®({i},{j})çš„æƒé‡åº”æ¥è¿‘0"
    
    print("  ä½¿ç”¨å› æœæ©ç åçš„æ³¨æ„åŠ›æƒé‡:")
    for i, row in enumerate(masked_weights):
        formatted_row = [f"{x:.4f}" for x in row]
        print(f"    {i}: {formatted_row}")
    
    print("âœ… æ³¨æ„åŠ›è®¡ç®—æµ‹è¯•é€šè¿‡")
    return True


def test_model_parameters():
    """æµ‹è¯•æ¨¡å‹å‚æ•°è®¡ç®—"""
    print("\nğŸ§ª æµ‹è¯•æ¨¡å‹å‚æ•°è®¡ç®—...")
    
    def calculate_qwen3_parameters(config: Dict[str, int]) -> Dict[str, int]:
        """è®¡ç®—Qwen3æ¨¡å‹çš„å‚æ•°æ•°é‡"""
        vocab_size = config['vocab_size']
        hidden_size = config['hidden_size'] 
        intermediate_size = config['intermediate_size']
        num_layers = config['num_hidden_layers']
        num_heads = config['num_attention_heads']
        num_kv_heads = config['num_key_value_heads']
        
        head_dim = hidden_size // num_heads
        
        # è¯åµŒå…¥å±‚
        embed_params = vocab_size * hidden_size
        
        # æ¯ä¸ªè§£ç å™¨å±‚çš„å‚æ•°
        # æ³¨æ„åŠ›å±‚
        q_proj_params = hidden_size * (num_heads * head_dim)  # é€šå¸¸ç­‰äº hidden_size * hidden_size
        k_proj_params = hidden_size * (num_kv_heads * head_dim)
        v_proj_params = hidden_size * (num_kv_heads * head_dim)
        o_proj_params = (num_heads * head_dim) * hidden_size
        
        attention_params = q_proj_params + k_proj_params + v_proj_params + o_proj_params
        
        # MLPå±‚ (SwiGLU)
        gate_proj_params = hidden_size * intermediate_size
        up_proj_params = hidden_size * intermediate_size
        down_proj_params = intermediate_size * hidden_size
        
        mlp_params = gate_proj_params + up_proj_params + down_proj_params
        
        # RMSNormå±‚ (ä¸¤ä¸ªï¼šattentionå‰åå„ä¸€ä¸ª)
        norm_params = 2 * hidden_size  # æ¯ä¸ªRMSNormåªæœ‰weightå‚æ•°
        
        # æ¯å±‚æ€»å‚æ•°
        layer_params = attention_params + mlp_params + norm_params
        
        # æ‰€æœ‰å±‚çš„å‚æ•°
        all_layers_params = num_layers * layer_params
        
        # æœ€ç»ˆRMSNorm
        final_norm_params = hidden_size
        
        # è¯­è¨€æ¨¡å‹å¤´ (å¦‚æœä¸å…±äº«åµŒå…¥æƒé‡)
        lm_head_params = hidden_size * vocab_size if not config.get('tie_word_embeddings', False) else 0
        
        # æ€»å‚æ•°
        total_params = embed_params + all_layers_params + final_norm_params + lm_head_params
        
        return {
            'embedding': embed_params,
            'attention_per_layer': attention_params,
            'mlp_per_layer': mlp_params,
            'norm_per_layer': norm_params,
            'layer_total': layer_params,
            'all_layers': all_layers_params,
            'final_norm': final_norm_params,
            'lm_head': lm_head_params,
            'total': total_params
        }
    
    # æµ‹è¯•é…ç½®
    configs = [
        {
            'name': 'Qwen3-0.5B',
            'vocab_size': 32000,
            'hidden_size': 1024,
            'intermediate_size': 2752,
            'num_hidden_layers': 24,
            'num_attention_heads': 16,
            'num_key_value_heads': 16,
            'tie_word_embeddings': False
        },
        {
            'name': 'Qwen3-1.8B', 
            'vocab_size': 32000,
            'hidden_size': 2048,
            'intermediate_size': 5632,
            'num_hidden_layers': 24,
            'num_attention_heads': 16,
            'num_key_value_heads': 16,
            'tie_word_embeddings': False
        },
        {
            'name': 'Demoé…ç½®',
            'vocab_size': 1000,
            'hidden_size': 512,
            'intermediate_size': 1024,
            'num_hidden_layers': 6,
            'num_attention_heads': 8,
            'num_key_value_heads': 8,
            'tie_word_embeddings': True
        }
    ]
    
    for config in configs:
        name = config.pop('name')
        params = calculate_qwen3_parameters(config)
        
        print(f"\n  {name}:")
        print(f"    è¯åµŒå…¥: {params['embedding']:,}")
        print(f"    æ¯å±‚æ³¨æ„åŠ›: {params['attention_per_layer']:,}")
        print(f"    æ¯å±‚MLP: {params['mlp_per_layer']:,}")
        print(f"    æ¯å±‚å½’ä¸€åŒ–: {params['norm_per_layer']:,}")
        print(f"    æ¯å±‚æ€»è®¡: {params['layer_total']:,}")
        print(f"    æ‰€æœ‰å±‚: {params['all_layers']:,}")
        print(f"    æœ€ç»ˆå½’ä¸€åŒ–: {params['final_norm']:,}")
        print(f"    è¯­è¨€æ¨¡å‹å¤´: {params['lm_head']:,}")
        print(f"    æ€»å‚æ•°: {params['total']:,}")
        print(f"    æ¨¡å‹å¤§å°: {params['total'] * 4 / 1024 / 1024:.2f} MB (FP32)")
        
        # éªŒè¯å‚æ•°è®¡ç®—çš„åˆç†æ€§
        assert params['total'] > 0, "æ€»å‚æ•°æ•°åº”å¤§äº0"
        assert params['embedding'] > 0, "åµŒå…¥å±‚å‚æ•°åº”å¤§äº0"
        assert params['all_layers'] > 0, "è§£ç å™¨å±‚å‚æ•°åº”å¤§äº0"
    
    print("âœ… æ¨¡å‹å‚æ•°è®¡ç®—æµ‹è¯•é€šè¿‡")
    return True


def test_generation_logic():
    """æµ‹è¯•æ–‡æœ¬ç”Ÿæˆé€»è¾‘"""
    print("\nğŸ§ª æµ‹è¯•æ–‡æœ¬ç”Ÿæˆé€»è¾‘...")
    
    def simulate_generation(input_tokens: List[int], max_new_tokens: int = 5,
                          vocab_size: int = 100, temperature: float = 1.0) -> List[int]:
        """æ¨¡æ‹Ÿæ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹"""
        generated = input_tokens.copy()
        
        for step in range(max_new_tokens):
            # æ¨¡æ‹Ÿæ¨¡å‹è¾“å‡ºlogits (éšæœºç”Ÿæˆï¼Œå®é™…åº”è¯¥æ˜¯æ¨¡å‹æ¨ç†ç»“æœ)
            import random
            logits = [random.random() * 10 - 5 for _ in range(vocab_size)]  # -5åˆ°5çš„éšæœºå€¼
            
            # åº”ç”¨æ¸©åº¦
            if temperature != 1.0:
                logits = [x / temperature for x in logits]
            
            # Softmax
            max_logit = max(logits)
            exp_logits = [math.exp(x - max_logit) for x in logits]
            sum_exp = sum(exp_logits)
            probs = [x / sum_exp for x in exp_logits]
            
            # é‡‡æ · (ç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…åº”è¯¥å®ç°top-p, top-kç­‰)
            # è¿™é‡Œä½¿ç”¨ç´¯ç§¯åˆ†å¸ƒå‡½æ•°é‡‡æ ·
            random_val = random.random()
            cumulative_prob = 0.0
            next_token = vocab_size - 1  # é»˜è®¤æœ€åä¸€ä¸ªtoken
            
            for token_id, prob in enumerate(probs):
                cumulative_prob += prob
                if random_val <= cumulative_prob:
                    next_token = token_id
                    break
            
            generated.append(next_token)
            
            # å¦‚æœé‡åˆ°ç»“æŸtokenï¼Œåœæ­¢ç”Ÿæˆ (å‡è®¾token 2æ˜¯EOS)
            if next_token == 2:
                break
        
        return generated
    
    # æµ‹è¯•ç”Ÿæˆ
    input_tokens = [1, 10, 25, 42]  # å‡è®¾çš„è¾“å…¥åºåˆ—
    
    generated1 = simulate_generation(input_tokens, max_new_tokens=3, temperature=1.0)
    generated2 = simulate_generation(input_tokens, max_new_tokens=3, temperature=0.5)
    
    # éªŒè¯ç”Ÿæˆç»“æœ
    assert len(generated1) >= len(input_tokens), "ç”Ÿæˆåºåˆ—é•¿åº¦åº”ä¸å°äºè¾“å…¥é•¿åº¦"
    assert len(generated2) >= len(input_tokens), "ç”Ÿæˆåºåˆ—é•¿åº¦åº”ä¸å°äºè¾“å…¥é•¿åº¦"
    assert generated1[:len(input_tokens)] == input_tokens, "ç”Ÿæˆåºåˆ—åº”åŒ…å«åŸå§‹è¾“å…¥"
    assert generated2[:len(input_tokens)] == input_tokens, "ç”Ÿæˆåºåˆ—åº”åŒ…å«åŸå§‹è¾“å…¥"
    
    new_tokens1 = generated1[len(input_tokens):]
    new_tokens2 = generated2[len(input_tokens):]
    
    print(f"  è¾“å…¥åºåˆ—: {input_tokens}")
    print(f"  temperature=1.0 ç”Ÿæˆ: {generated1}")
    print(f"  æ–°å¢tokens: {new_tokens1}")
    print(f"  temperature=0.5 ç”Ÿæˆ: {generated2}")
    print(f"  æ–°å¢tokens: {new_tokens2}")
    print("âœ… æ–‡æœ¬ç”Ÿæˆé€»è¾‘æµ‹è¯•é€šè¿‡")
    return True


def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("ğŸš€ å¼€å§‹ Qwen3 æ¨¡å‹æµ‹è¯•")
    print("=" * 60)
    
    tests = [
        ("é…ç½®æµ‹è¯•", test_config),
        ("RMSNormæµ‹è¯•", test_rms_norm),
        ("RoPEæµ‹è¯•", test_rope),
        ("æ³¨æ„åŠ›æ©ç æµ‹è¯•", test_attention_mask),
        ("SwiGLUæµ‹è¯•", test_swiglu_activation),
        ("æ³¨æ„åŠ›è®¡ç®—æµ‹è¯•", test_attention_computation),
        ("å‚æ•°è®¡ç®—æµ‹è¯•", test_model_parameters),
        ("ç”Ÿæˆé€»è¾‘æµ‹è¯•", test_generation_logic),
    ]
    
    passed = 0
    failed = 0
    
    for test_name, test_func in tests:
        try:
            if test_func():
                passed += 1
            else:
                print(f"âŒ {test_name}å¤±è´¥")
                failed += 1
        except Exception as e:
            print(f"âŒ {test_name}å‡ºé”™: {e}")
            failed += 1
    
    print("\n" + "=" * 60)
    print(f"ğŸ¯ æµ‹è¯•æ€»ç»“: {passed}ä¸ªé€šè¿‡, {failed}ä¸ªå¤±è´¥")
    
    if failed == 0:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Qwen3å®ç°éªŒè¯æˆåŠŸï¼")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥å®ç°")
    
    return failed == 0


if __name__ == "__main__":
    success = run_all_tests()
    
    if success:
        print("\nğŸ“‹ æµ‹è¯•æŠ¥å‘Š:")
        print("- âœ… é…ç½®ç³»ç»Ÿæ­£å¸¸å·¥ä½œ")
        print("- âœ… RMSNormå½’ä¸€åŒ–è®¡ç®—æ­£ç¡®")
        print("- âœ… RoPEä½ç½®ç¼–ç é€»è¾‘æ­£ç¡®")
        print("- âœ… æ³¨æ„åŠ›æ©ç ç”Ÿæˆæ­£ç¡®")
        print("- âœ… SwiGLUæ¿€æ´»å‡½æ•°è®¡ç®—æ­£ç¡®")
        print("- âœ… æ³¨æ„åŠ›æœºåˆ¶è®¡ç®—æ­£ç¡®")
        print("- âœ… æ¨¡å‹å‚æ•°ç»Ÿè®¡æ­£ç¡®")
        print("- âœ… æ–‡æœ¬ç”Ÿæˆé€»è¾‘æ­£ç¡®")
        print("\nğŸŠ Qwen3æ¨¡å‹å®ç°éªŒè¯å®Œæˆï¼")
    else:
        print("\nğŸ”§ éœ€è¦ä¿®å¤çš„é—®é¢˜:")
        print("- æ£€æŸ¥å¤±è´¥çš„æµ‹è¯•ç”¨ä¾‹")
        print("- éªŒè¯æ•°å­¦è®¡ç®—çš„å‡†ç¡®æ€§")
        print("- ç¡®ä¿å„ç»„ä»¶æ¥å£æ­£ç¡®")