"""
Qwen3 æ¨¡å‹æ¼”ç¤ºç¨‹åº
å±•ç¤ºå¦‚ä½•ä½¿ç”¨ Qwen3 æ¨¡å‹è¿›è¡Œæ–‡æœ¬ç”Ÿæˆå’Œå¯¹è¯
åŒ…å«åŸºæœ¬çš„tokenizerå®ç°å’Œæ–‡æœ¬ç”Ÿæˆç¤ºä¾‹
"""

import torch
import torch.nn.functional as F
import json
import re
from typing import List, Dict, Optional, Union
import math

# å¯¼å…¥ Qwen3 æ¨¡å‹ç»„ä»¶
try:
    from qwen3_model import Qwen3ForCausalLM, Qwen3Config, create_qwen3_model
    from qwen3_core_components import prepare_attention_mask
except ImportError:
    print("æ³¨æ„ï¼šæ— æ³•å¯¼å…¥Qwen3æ¨¡å‹ç»„ä»¶ï¼ŒæŸäº›åŠŸèƒ½å¯èƒ½ä¸å¯ç”¨")


class SimpleTokenizer:
    """
    ç®€å•çš„åˆ†è¯å™¨å®ç°
    ç”¨äºæ¼”ç¤ºç›®çš„ï¼Œå®é™…ä½¿ç”¨ä¸­åº”è¯¥ä½¿ç”¨æ›´å®Œå–„çš„åˆ†è¯å™¨å¦‚SentencePiece
    """
    
    def __init__(self, vocab_size: int = 32000):
        self.vocab_size = vocab_size
        
        # ç‰¹æ®Štoken
        self.pad_token_id = 0
        self.bos_token_id = 1  # Begin of sequence
        self.eos_token_id = 2  # End of sequence
        self.unk_token_id = 3  # Unknown token
        
        # ç‰¹æ®Štokenå­—ç¬¦ä¸²
        self.pad_token = "<pad>"
        self.bos_token = "<bos>"
        self.eos_token = "<eos>"
        self.unk_token = "<unk>"
        
        # æ„å»ºåŸºç¡€è¯æ±‡è¡¨
        self._build_vocab()
    
    def _build_vocab(self):
        """æ„å»ºåŸºç¡€è¯æ±‡è¡¨"""
        # ç‰¹æ®Štoken
        special_tokens = [
            self.pad_token, self.bos_token, self.eos_token, self.unk_token
        ]
        
        # åŸºç¡€å­—ç¬¦é›†ï¼ˆASCII + å¸¸ç”¨ä¸­æ–‡å­—ç¬¦ï¼‰
        basic_chars = []
        
        # ASCIIå­—ç¬¦
        for i in range(32, 127):  # å¯æ‰“å°ASCIIå­—ç¬¦
            basic_chars.append(chr(i))
        
        # å¸¸ç”¨ä¸­æ–‡å­—ç¬¦ï¼ˆç®€åŒ–ç‰ˆï¼‰
        common_chinese = "çš„ä¸€æ˜¯ä¸äº†äººæˆ‘åœ¨æœ‰ä»–è¿™ä¸ºä¹‹å¤§æ¥ä»¥ä¸ªä¸­ä¸Šä»¬åˆ°è¯´å›½å’Œåœ°ä¹Ÿå­æ—¶é“å‡ºè€Œè¦äºå°±ä¸‹å¾—å¯ä½ å¹´ç”Ÿè‡ªä¼šé‚£åèƒ½å¯¹ç€äº‹å…¶é‡Œæ‰€å»è¡Œè¿‡å®¶åç”¨å‘å¤©å¦‚ç„¶ä½œæ–¹æˆè€…å¤šæ—¥éƒ½ä¸‰å°å†›äºŒæ— åŒä¹ˆç»æ³•å½“èµ·ä¸å¥½çœ‹å­¦è¿›ç§å°†è¿˜åˆ†æ­¤å¿ƒå‰é¢åˆå®šè§åªä¸»æ²¡å…¬ä»"
        for char in common_chinese:
            if char not in basic_chars:
                basic_chars.append(char)
        
        # æ„å»ºtokenåˆ°IDçš„æ˜ å°„
        self.token_to_id = {}
        self.id_to_token = {}
        
        # æ·»åŠ ç‰¹æ®Štoken
        for i, token in enumerate(special_tokens):
            self.token_to_id[token] = i
            self.id_to_token[i] = token
        
        # æ·»åŠ åŸºç¡€å­—ç¬¦
        current_id = len(special_tokens)
        for char in basic_chars:
            if char not in self.token_to_id:
                self.token_to_id[char] = current_id
                self.id_to_token[current_id] = char
                current_id += 1
        
        # å¡«å……åˆ°æŒ‡å®šè¯æ±‡è¡¨å¤§å°
        while current_id < self.vocab_size:
            placeholder_token = f"<unused_{current_id}>"
            self.token_to_id[placeholder_token] = current_id
            self.id_to_token[current_id] = placeholder_token
            current_id += 1
    
    def encode(self, text: str, add_bos: bool = True, add_eos: bool = False) -> List[int]:
        """
        å°†æ–‡æœ¬ç¼–ç ä¸ºtoken IDåºåˆ—
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            add_bos: æ˜¯å¦æ·»åŠ å¼€å§‹token
            add_eos: æ˜¯å¦æ·»åŠ ç»“æŸtoken
        
        Returns:
            token IDåˆ—è¡¨
        """
        # ç®€å•çš„å­—ç¬¦çº§åˆ«tokenization
        tokens = []
        
        if add_bos:
            tokens.append(self.bos_token_id)
        
        for char in text:
            token_id = self.token_to_id.get(char, self.unk_token_id)
            tokens.append(token_id)
        
        if add_eos:
            tokens.append(self.eos_token_id)
        
        return tokens
    
    def decode(self, token_ids: Union[List[int], torch.Tensor], skip_special_tokens: bool = True) -> str:
        """
        å°†token IDåºåˆ—è§£ç ä¸ºæ–‡æœ¬
        
        Args:
            token_ids: token IDåºåˆ—
            skip_special_tokens: æ˜¯å¦è·³è¿‡ç‰¹æ®Štoken
        
        Returns:
            è§£ç åçš„æ–‡æœ¬
        """
        if isinstance(token_ids, torch.Tensor):
            token_ids = token_ids.tolist()
        
        tokens = []
        special_token_ids = {self.pad_token_id, self.bos_token_id, self.eos_token_id}
        
        for token_id in token_ids:
            if skip_special_tokens and token_id in special_token_ids:
                continue
            
            token = self.id_to_token.get(token_id, self.unk_token)
            if not (skip_special_tokens and token.startswith("<") and token.endswith(">")):
                tokens.append(token)
        
        return "".join(tokens)
    
    def batch_encode(self, texts: List[str], padding: bool = True, 
                    max_length: Optional[int] = None, add_bos: bool = True, 
                    add_eos: bool = False) -> Dict[str, torch.Tensor]:
        """
        æ‰¹é‡ç¼–ç æ–‡æœ¬
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            padding: æ˜¯å¦è¿›è¡Œå¡«å……
            max_length: æœ€å¤§é•¿åº¦
            add_bos: æ˜¯å¦æ·»åŠ å¼€å§‹token
            add_eos: æ˜¯å¦æ·»åŠ ç»“æŸtoken
        
        Returns:
            åŒ…å«input_idså’Œattention_maskçš„å­—å…¸
        """
        encoded_batch = []
        
        for text in texts:
            encoded = self.encode(text, add_bos=add_bos, add_eos=add_eos)
            encoded_batch.append(encoded)
        
        if max_length is None:
            max_length = max(len(seq) for seq in encoded_batch)
        
        # å¡«å……æˆ–æˆªæ–­
        input_ids = []
        attention_mask = []
        
        for encoded in encoded_batch:
            if len(encoded) > max_length:
                # æˆªæ–­
                encoded = encoded[:max_length]
                mask = [1] * max_length
            else:
                # å¡«å……
                pad_length = max_length - len(encoded)
                mask = [1] * len(encoded) + [0] * pad_length
                encoded = encoded + [self.pad_token_id] * pad_length
            
            input_ids.append(encoded)
            attention_mask.append(mask)
        
        return {
            'input_ids': torch.tensor(input_ids, dtype=torch.long),
            'attention_mask': torch.tensor(attention_mask, dtype=torch.long)
        }


class Qwen3ChatBot:
    """
    åŸºäºQwen3æ¨¡å‹çš„ç®€å•èŠå¤©æœºå™¨äºº
    """
    
    def __init__(self, model: 'Qwen3ForCausalLM', tokenizer: SimpleTokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self.conversation_history = []
        
        # ç”Ÿæˆå‚æ•°
        self.max_new_tokens = 100
        self.temperature = 0.7
        self.top_p = 0.9
        self.top_k = 50
        self.do_sample = True
    
    def chat(self, user_input: str, system_prompt: str = "ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚") -> str:
        """
        ä¸ç”¨æˆ·è¿›è¡Œå¯¹è¯
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            system_prompt: ç³»ç»Ÿæç¤º
        
        Returns:
            AIå›å¤
        """
        # æ„å»ºå¯¹è¯æç¤º
        if not self.conversation_history:
            # é¦–æ¬¡å¯¹è¯ï¼Œæ·»åŠ ç³»ç»Ÿæç¤º
            prompt = f"{system_prompt}\n\nç”¨æˆ·: {user_input}\nAI:"
        else:
            # ç»§ç»­å¯¹è¯
            prompt = f"ç”¨æˆ·: {user_input}\nAI:"
        
        # ç¼–ç è¾“å…¥
        input_ids = torch.tensor([self.tokenizer.encode(prompt, add_bos=True)], dtype=torch.long)
        
        # ç”Ÿæˆå›å¤
        with torch.no_grad():
            generated = self.model.generate(
                input_ids=input_ids,
                max_new_tokens=self.max_new_tokens,
                temperature=self.temperature,
                top_p=self.top_p,
                top_k=self.top_k,
                do_sample=self.do_sample,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
            )
        
        # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
        generated_text = self.tokenizer.decode(generated[0], skip_special_tokens=True)
        
        # æå–AIå›å¤éƒ¨åˆ†
        ai_response = generated_text[len(self.tokenizer.decode(input_ids[0], skip_special_tokens=True)):]
        
        # æ¸…ç†å›å¤
        ai_response = ai_response.strip()
        if ai_response.startswith("AI:"):
            ai_response = ai_response[3:].strip()
        
        # åœ¨é‡åˆ°æ–°çš„ç”¨æˆ·è¾“å…¥å‰åœæ­¢
        if "ç”¨æˆ·:" in ai_response:
            ai_response = ai_response.split("ç”¨æˆ·:")[0].strip()
        
        # æ›´æ–°å¯¹è¯å†å²
        self.conversation_history.append({
            "user": user_input,
            "ai": ai_response
        })
        
        return ai_response
    
    def clear_history(self):
        """æ¸…é™¤å¯¹è¯å†å²"""
        self.conversation_history = []
    
    def set_generation_params(self, **kwargs):
        """è®¾ç½®ç”Ÿæˆå‚æ•°"""
        for key, value in kwargs.items():
            if hasattr(self, key):
                setattr(self, key, value)


def create_demo_model() -> 'Qwen3ForCausalLM':
    """
    åˆ›å»ºç”¨äºæ¼”ç¤ºçš„Qwen3æ¨¡å‹
    ä½¿ç”¨è¾ƒå°çš„é…ç½®ä»¥ä¾¿å¿«é€Ÿæµ‹è¯•
    """
    config = Qwen3Config(
        vocab_size=32000,        # è¯æ±‡è¡¨å¤§å°
        hidden_size=512,         # éšè—å±‚ç»´åº¦  
        intermediate_size=1024,  # å‰é¦ˆç½‘ç»œä¸­é—´å±‚ç»´åº¦
        num_hidden_layers=6,     # éšè—å±‚æ•°é‡
        num_attention_heads=8,   # æ³¨æ„åŠ›å¤´æ•°é‡
        num_key_value_heads=8,   # é”®å€¼å¤´æ•°é‡
        max_position_embeddings=2048,  # æœ€å¤§ä½ç½®ç¼–ç é•¿åº¦
        rope_theta=10000.0,      # RoPEåŸºç¡€é¢‘ç‡
        rms_norm_eps=1e-6,       # RMSNormçš„epsilon
        pad_token_id=0,
        bos_token_id=1,
        eos_token_id=2,
        tie_word_embeddings=False
    )
    
    model = Qwen3ForCausalLM(config)
    return model


def text_generation_demo():
    """æ–‡æœ¬ç”Ÿæˆæ¼”ç¤º"""
    print("=== Qwen3 æ–‡æœ¬ç”Ÿæˆæ¼”ç¤º ===\n")
    
    try:
        # åˆ›å»ºæ¨¡å‹å’Œåˆ†è¯å™¨
        print("æ­£åœ¨åˆ›å»ºæ¨¡å‹...")
        model = create_demo_model()
        tokenizer = SimpleTokenizer(vocab_size=32000)
        
        print(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
        print(f"è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")
        
        # æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ
        test_prompts = [
            "ä»Šå¤©å¤©æ°”",
            "äººå·¥æ™ºèƒ½",
            "Pythonç¼–ç¨‹",
            "æœºå™¨å­¦ä¹ æ˜¯"
        ]
        
        print(f"\nå¼€å§‹æ–‡æœ¬ç”Ÿæˆæµ‹è¯•...")
        
        for prompt in test_prompts:
            print(f"\nè¾“å…¥æç¤º: '{prompt}'")
            
            # ç¼–ç è¾“å…¥
            input_ids = torch.tensor([tokenizer.encode(prompt, add_bos=True)], dtype=torch.long)
            print(f"è¾“å…¥tokenæ•°: {input_ids.shape[1]}")
            
            # ç”Ÿæˆæ–‡æœ¬
            with torch.no_grad():
                generated = model.generate(
                    input_ids=input_ids,
                    max_new_tokens=20,  # ç”Ÿæˆè¾ƒå°‘tokenä»¥ä¾¿è§‚å¯Ÿ
                    temperature=0.8,
                    top_p=0.9,
                    do_sample=True,
                    pad_token_id=tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                )
            
            # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
            generated_text = tokenizer.decode(generated[0], skip_special_tokens=True)
            new_text = generated_text[len(prompt):]  # åªæ˜¾ç¤ºæ–°ç”Ÿæˆçš„éƒ¨åˆ†
            
            print(f"ç”Ÿæˆç»“æœ: '{generated_text}'")
            print(f"æ–°å¢æ–‡æœ¬: '{new_text}'")
            print(f"æ€»tokenæ•°: {generated.shape[1]}")
    
    except Exception as e:
        print(f"æ¼”ç¤ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        print("è¿™å¯èƒ½æ˜¯ç”±äºPyTorchæœªå®‰è£…æˆ–å…¶ä»–ä¾èµ–é—®é¢˜å¯¼è‡´çš„")


def chat_demo():
    """èŠå¤©æ¼”ç¤º"""
    print("\n=== Qwen3 èŠå¤©æ¼”ç¤º ===\n")
    
    try:
        # åˆ›å»ºèŠå¤©æœºå™¨äºº
        print("æ­£åœ¨åˆå§‹åŒ–èŠå¤©æœºå™¨äºº...")
        model = create_demo_model()
        tokenizer = SimpleTokenizer(vocab_size=32000)
        chatbot = Qwen3ChatBot(model, tokenizer)
        
        # è®¾ç½®ç”Ÿæˆå‚æ•°
        chatbot.set_generation_params(
            max_new_tokens=50,
            temperature=0.7,
            top_p=0.9,
            do_sample=True
        )
        
        print("èŠå¤©æœºå™¨äººå·²å‡†å¤‡å°±ç»ªï¼")
        print("(è¾“å…¥ 'quit' é€€å‡ºï¼Œè¾“å…¥ 'clear' æ¸…é™¤å†å²)\n")
        
        # æ¨¡æ‹Ÿå¯¹è¯
        demo_conversations = [
            "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±",
            "ä½ èƒ½åšä»€ä¹ˆï¼Ÿ", 
            "è§£é‡Šä¸€ä¸‹æœºå™¨å­¦ä¹ ",
            "quit"
        ]
        
        for user_input in demo_conversations:
            print(f"ç”¨æˆ·: {user_input}")
            
            if user_input.lower() == 'quit':
                print("å¯¹è¯ç»“æŸï¼")
                break
            elif user_input.lower() == 'clear':
                chatbot.clear_history()
                print("å¯¹è¯å†å²å·²æ¸…é™¤ï¼")
                continue
            
            # è·å–AIå›å¤
            try:
                ai_response = chatbot.chat(user_input)
                print(f"AI: {ai_response}\n")
            except Exception as e:
                print(f"ç”Ÿæˆå›å¤æ—¶å‡ºé”™: {e}\n")
        
        # æ˜¾ç¤ºå¯¹è¯å†å²
        print("å¯¹è¯å†å²:")
        for i, conv in enumerate(chatbot.conversation_history, 1):
            print(f"{i}. ç”¨æˆ·: {conv['user']}")
            print(f"   AI: {conv['ai']}")
    
    except Exception as e:
        print(f"èŠå¤©æ¼”ç¤ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        print("è¿™å¯èƒ½æ˜¯ç”±äºPyTorchæœªå®‰è£…æˆ–å…¶ä»–ä¾èµ–é—®é¢˜å¯¼è‡´çš„")


def model_info_demo():
    """æ¨¡å‹ä¿¡æ¯æ¼”ç¤º"""
    print("\n=== Qwen3 æ¨¡å‹ä¿¡æ¯ ===\n")
    
    try:
        # åˆ›å»ºæ¨¡å‹
        model = create_demo_model()
        tokenizer = SimpleTokenizer()
        
        # æ¨¡å‹é…ç½®ä¿¡æ¯
        config = model.config
        print("æ¨¡å‹é…ç½®:")
        print(f"  è¯æ±‡è¡¨å¤§å°: {config.vocab_size:,}")
        print(f"  éšè—å±‚ç»´åº¦: {config.hidden_size}")
        print(f"  éšè—å±‚æ•°é‡: {config.num_hidden_layers}")
        print(f"  æ³¨æ„åŠ›å¤´æ•°: {config.num_attention_heads}")
        print(f"  ä¸­é—´å±‚ç»´åº¦: {config.intermediate_size}")
        print(f"  æœ€å¤§åºåˆ—é•¿åº¦: {config.max_position_embeddings}")
        
        # å‚æ•°ç»Ÿè®¡
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        print(f"\nå‚æ•°ç»Ÿè®¡:")
        print(f"  æ€»å‚æ•°æ•°: {total_params:,}")
        print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        print(f"  æ¨¡å‹å¤§å°: {total_params * 4 / 1024 / 1024:.2f} MB (FP32)")
        
        # åˆ†è¯å™¨ä¿¡æ¯
        print(f"\nåˆ†è¯å™¨ä¿¡æ¯:")
        print(f"  è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")
        print(f"  ç‰¹æ®Štoken: {tokenizer.pad_token}, {tokenizer.bos_token}, {tokenizer.eos_token}, {tokenizer.unk_token}")
        
        # æµ‹è¯•ç¼–ç è§£ç 
        test_text = "ä½ å¥½ï¼Œä¸–ç•Œï¼Hello, World!"
        encoded = tokenizer.encode(test_text)
        decoded = tokenizer.decode(encoded)
        
        print(f"\nç¼–ç è§£ç æµ‹è¯•:")
        print(f"  åŸæ–‡: {test_text}")
        print(f"  ç¼–ç : {encoded}")
        print(f"  è§£ç : {decoded}")
        print(f"  é•¿åº¦: {len(encoded)} tokens")
    
    except Exception as e:
        print(f"æ¨¡å‹ä¿¡æ¯æ¼”ç¤ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")


if __name__ == "__main__":
    print("ğŸ¤– Qwen3 æ¨¡å‹æ¼”ç¤ºç¨‹åº")
    print("=" * 50)
    
    # è¿è¡Œå„ç§æ¼”ç¤º
    model_info_demo()
    text_generation_demo()
    chat_demo()
    
    print("\nâœ… æ¼”ç¤ºå®Œæˆï¼")
    print("\nè¯´æ˜ï¼š")
    print("- è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„Qwen3å®ç°ï¼Œç”¨äºå­¦ä¹ å’Œæ¼”ç¤ºç›®çš„")
    print("- å®é™…ä½¿ç”¨ä¸­éœ€è¦é¢„è®­ç»ƒçš„æƒé‡å’Œæ›´å®Œå–„çš„åˆ†è¯å™¨")
    print("- ç”Ÿæˆçš„æ–‡æœ¬è´¨é‡å–å†³äºæ¨¡å‹è®­ç»ƒç¨‹åº¦")
    print("- å¦‚æœé‡åˆ°PyTorchç›¸å…³é”™è¯¯ï¼Œè¯·ç¡®ä¿å·²æ­£ç¡®å®‰è£…PyTorch")