# è®­ç»ƒä¸ä¼˜åŒ–æŠ€å·§

## ğŸ“‹ ç›®å½•

- [å®Œæ•´è®­ç»ƒæµç¨‹](#å®Œæ•´è®­ç»ƒæµç¨‹)
- [æŸå¤±å‡½æ•°](#æŸå¤±å‡½æ•°)
- [ä¼˜åŒ–å™¨è¯¦è§£](#ä¼˜åŒ–å™¨è¯¦è§£)
- [å­¦ä¹ ç‡è°ƒåº¦](#å­¦ä¹ ç‡è°ƒåº¦)
- [æ­£åˆ™åŒ–æŠ€æœ¯](#æ­£åˆ™åŒ–æŠ€æœ¯)
- [è®­ç»ƒæŠ€å·§](#è®­ç»ƒæŠ€å·§)
- [è°ƒè¯•å’Œç›‘æ§](#è°ƒè¯•å’Œç›‘æ§)

---

## ğŸ”„ å®Œæ•´è®­ç»ƒæµç¨‹

### æ ‡å‡†è®­ç»ƒå¾ªç¯

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

# 1. å‡†å¤‡æ•°æ®
X_train = torch.randn(1000, 10)
y_train = torch.randint(0, 2, (1000,))
train_dataset = TensorDataset(X_train, y_train)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

# 2. å®šä¹‰æ¨¡å‹
model = nn.Sequential(
    nn.Linear(10, 64),
    nn.ReLU(),
    nn.Dropout(0.2),
    nn.Linear(64, 32),
    nn.ReLU(),
    nn.Linear(32, 2)
)

# 3. å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 4. è®­ç»ƒå¾ªç¯
num_epochs = 10

for epoch in range(num_epochs):
    model.train()  # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
    
    running_loss = 0.0
    correct = 0
    total = 0
    
    for batch_idx, (data, target) in enumerate(train_loader):
        # å‰å‘ä¼ æ’­
        output = model(data)
        loss = criterion(output, target)
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()  # æ¸…é›¶æ¢¯åº¦
        loss.backward()        # è®¡ç®—æ¢¯åº¦
        optimizer.step()       # æ›´æ–°å‚æ•°
        
        # ç»Ÿè®¡
        running_loss += loss.item()
        _, predicted = output.max(1)
        total += target.size(0)
        correct += predicted.eq(target).sum().item()
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    epoch_loss = running_loss / len(train_loader)
    epoch_acc = 100. * correct / total
    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Acc: {epoch_acc:.2f}%')

print('è®­ç»ƒå®Œæˆï¼')
```

### å¸¦éªŒè¯é›†çš„è®­ç»ƒ

```python
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset

# å‡†å¤‡è®­ç»ƒé›†å’ŒéªŒè¯é›†
X_train = torch.randn(800, 10)
y_train = torch.randint(0, 2, (800,))
X_val = torch.randn(200, 10)
y_val = torch.randint(0, 2, (200,))

train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=32, shuffle=True)
val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=32, shuffle=False)

# æ¨¡å‹ã€æŸå¤±ã€ä¼˜åŒ–å™¨
model = nn.Sequential(
    nn.Linear(10, 64),
    nn.ReLU(),
    nn.Linear(64, 2)
)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# è®­ç»ƒå’ŒéªŒè¯å‡½æ•°
def train_epoch(model, loader, criterion, optimizer):
    model.train()
    total_loss = 0
    correct = 0
    total = 0
    
    for data, target in loader:
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
        _, predicted = output.max(1)
        total += target.size(0)
        correct += predicted.eq(target).sum().item()
    
    return total_loss / len(loader), 100. * correct / total

def validate(model, loader, criterion):
    model.eval()
    total_loss = 0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for data, target in loader:
            output = model(data)
            loss = criterion(output, target)
            
            total_loss += loss.item()
            _, predicted = output.max(1)
            total += target.size(0)
            correct += predicted.eq(target).sum().item()
    
    return total_loss / len(loader), 100. * correct / total

# è®­ç»ƒå¾ªç¯
num_epochs = 20
best_val_acc = 0

for epoch in range(num_epochs):
    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer)
    val_loss, val_acc = validate(model, val_loader, criterion)
    
    print(f'Epoch {epoch+1}/{num_epochs}')
    print(f'  Train: Loss={train_loss:.4f}, Acc={train_acc:.2f}%')
    print(f'  Val:   Loss={val_loss:.4f}, Acc={val_acc:.2f}%')
    
    # ä¿å­˜æœ€ä½³æ¨¡å‹
    if val_acc > best_val_acc:
        best_val_acc = val_acc
        torch.save(model.state_dict(), 'best_model.pth')
        print('  âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹')

print(f'\næœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%')
```

---

## ğŸ“Š æŸå¤±å‡½æ•°

### 1. åˆ†ç±»ä»»åŠ¡

```python
import torch
import torch.nn as nn

# äºŒåˆ†ç±»ï¼šBinary Cross Entropy
bce_loss = nn.BCELoss()
sigmoid_output = torch.sigmoid(torch.randn(10, 1))
target = torch.randint(0, 2, (10, 1)).float()
loss = bce_loss(sigmoid_output, target)
print(f"BCE Loss: {loss.item()}")

# äºŒåˆ†ç±»ï¼ˆå¸¦ Logitsï¼‰ï¼šæ›´ç¨³å®š
bce_with_logits = nn.BCEWithLogitsLoss()
logits = torch.randn(10, 1)
loss = bce_with_logits(logits, target)
print(f"BCE with Logits Loss: {loss.item()}")

# å¤šåˆ†ç±»ï¼šCross Entropy
ce_loss = nn.CrossEntropyLoss()
logits = torch.randn(32, 10)  # batch=32, classes=10
target = torch.randint(0, 10, (32,))
loss = ce_loss(logits, target)
print(f"CE Loss: {loss.item()}")

# å¸¦æƒé‡çš„äº¤å‰ç†µï¼ˆå¤„ç†ç±»åˆ«ä¸å¹³è¡¡ï¼‰
weights = torch.tensor([1.0, 2.0, 3.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0])
weighted_ce = nn.CrossEntropyLoss(weight=weights)
loss = weighted_ce(logits, target)
print(f"Weighted CE Loss: {loss.item()}")

# Negative Log Likelihoodï¼ˆé…åˆ log_softmaxï¼‰
nll_loss = nn.NLLLoss()
log_probs = torch.log_softmax(logits, dim=1)
loss = nll_loss(log_probs, target)
print(f"NLL Loss: {loss.item()}")
```

### 2. å›å½’ä»»åŠ¡

```python
import torch
import torch.nn as nn

# å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰
mse_loss = nn.MSELoss()
output = torch.randn(10, 1)
target = torch.randn(10, 1)
loss = mse_loss(output, target)
print(f"MSE Loss: {loss.item()}")

# å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAE / L1 Lossï¼‰
l1_loss = nn.L1Loss()
loss = l1_loss(output, target)
print(f"L1 Loss: {loss.item()}")

# Smooth L1 Lossï¼ˆHuber Lossï¼‰
smooth_l1 = nn.SmoothL1Loss()
loss = smooth_l1(output, target)
print(f"Smooth L1 Loss: {loss.item()}")
```

### 3. è‡ªå®šä¹‰æŸå¤±å‡½æ•°

```python
import torch
import torch.nn as nn

class FocalLoss(nn.Module):
    """Focal Loss for addressing class imbalance"""
    def __init__(self, alpha=1, gamma=2):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
    
    def forward(self, inputs, targets):
        ce_loss = nn.functional.cross_entropy(inputs, targets, reduction='none')
        pt = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss
        return focal_loss.mean()

# ä½¿ç”¨
focal_loss = FocalLoss(alpha=1, gamma=2)
logits = torch.randn(32, 10)
target = torch.randint(0, 10, (32,))
loss = focal_loss(logits, target)
print(f"Focal Loss: {loss.item()}")
```

---

## âš™ï¸ ä¼˜åŒ–å™¨è¯¦è§£

### 1. å¸¸ç”¨ä¼˜åŒ–å™¨

```python
import torch
import torch.nn as nn
import torch.optim as optim

model = nn.Linear(10, 1)

# SGDï¼ˆéšæœºæ¢¯åº¦ä¸‹é™ï¼‰
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)

# Adamï¼ˆè‡ªé€‚åº”çŸ©ä¼°è®¡ï¼‰
optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), weight_decay=1e-4)

# AdamWï¼ˆAdam + æƒé‡è¡°å‡ï¼‰
optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)

# RMSprop
optimizer = optim.RMSprop(model.parameters(), lr=0.001, alpha=0.99)

# Adagrad
optimizer = optim.Adagrad(model.parameters(), lr=0.01)

# Adadelta
optimizer = optim.Adadelta(model.parameters(), lr=1.0, rho=0.9)
```

### 2. ä¼˜åŒ–å™¨å¯¹æ¯”

```python
import torch
import torch.nn as nn
import matplotlib.pyplot as plt

# ç”Ÿæˆæ•°æ®
X = torch.randn(100, 10)
y = torch.randn(100, 1)

# å®šä¹‰æ¨¡å‹
def create_model():
    return nn.Sequential(nn.Linear(10, 1))

# è®­ç»ƒå‡½æ•°
def train_with_optimizer(optimizer_class, **kwargs):
    model = create_model()
    optimizer = optimizer_class(model.parameters(), **kwargs)
    criterion = nn.MSELoss()
    
    losses = []
    for epoch in range(100):
        optimizer.zero_grad()
        output = model(X)
        loss = criterion(output, y)
        loss.backward()
        optimizer.step()
        losses.append(loss.item())
    
    return losses

# æ¯”è¾ƒä¸åŒä¼˜åŒ–å™¨
sgd_losses = train_with_optimizer(torch.optim.SGD, lr=0.01, momentum=0.9)
adam_losses = train_with_optimizer(torch.optim.Adam, lr=0.01)
rmsprop_losses = train_with_optimizer(torch.optim.RMSprop, lr=0.01)

print("SGD æœ€ç»ˆæŸå¤±:", sgd_losses[-1])
print("Adam æœ€ç»ˆæŸå¤±:", adam_losses[-1])
print("RMSprop æœ€ç»ˆæŸå¤±:", rmsprop_losses[-1])
```

### 3. å‚æ•°ç»„ï¼ˆä¸åŒå­¦ä¹ ç‡ï¼‰

```python
import torch
import torch.nn as nn
import torch.optim as optim

class MyModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.features = nn.Sequential(
            nn.Linear(10, 20),
            nn.ReLU()
        )
        self.classifier = nn.Linear(20, 2)
    
    def forward(self, x):
        x = self.features(x)
        x = self.classifier(x)
        return x

model = MyModel()

# ä¸åŒå‚æ•°ç»„ä½¿ç”¨ä¸åŒå­¦ä¹ ç‡
optimizer = optim.Adam([
    {'params': model.features.parameters(), 'lr': 1e-3},
    {'params': model.classifier.parameters(), 'lr': 1e-2}
])

# éªŒè¯
for param_group in optimizer.param_groups:
    print(f"å­¦ä¹ ç‡: {param_group['lr']}")
```

---

## ğŸ“ˆ å­¦ä¹ ç‡è°ƒåº¦

### 1. StepLRï¼ˆé˜¶æ¢¯å¼ï¼‰

```python
import torch
import torch.optim as optim
from torch.optim.lr_scheduler import StepLR

model = torch.nn.Linear(10, 1)
optimizer = optim.SGD(model.parameters(), lr=0.1)

# æ¯ 10 ä¸ª epochï¼Œå­¦ä¹ ç‡ä¹˜ä»¥ 0.1
scheduler = StepLR(optimizer, step_size=10, gamma=0.1)

for epoch in range(30):
    # è®­ç»ƒä»£ç ...
    optimizer.step()
    scheduler.step()  # æ›´æ–°å­¦ä¹ ç‡
    
    print(f"Epoch {epoch+1}, LR: {optimizer.param_groups[0]['lr']:.6f}")
```

### 2. MultiStepLRï¼ˆå¤šé˜¶æ¢¯ï¼‰

```python
from torch.optim.lr_scheduler import MultiStepLR

optimizer = optim.SGD(model.parameters(), lr=0.1)

# åœ¨æŒ‡å®š epoch é™ä½å­¦ä¹ ç‡
scheduler = MultiStepLR(optimizer, milestones=[30, 80], gamma=0.1)

for epoch in range(100):
    optimizer.step()
    scheduler.step()
    
    if epoch in [0, 29, 30, 79, 80, 99]:
        print(f"Epoch {epoch+1}, LR: {optimizer.param_groups[0]['lr']:.6f}")
```

### 3. ExponentialLRï¼ˆæŒ‡æ•°è¡°å‡ï¼‰

```python
from torch.optim.lr_scheduler import ExponentialLR

optimizer = optim.SGD(model.parameters(), lr=0.1)
scheduler = ExponentialLR(optimizer, gamma=0.95)

for epoch in range(20):
    optimizer.step()
    scheduler.step()
    
    if epoch % 5 == 0:
        print(f"Epoch {epoch+1}, LR: {optimizer.param_groups[0]['lr']:.6f}")
```

### 4. CosineAnnealingLRï¼ˆä½™å¼¦é€€ç«ï¼‰

```python
from torch.optim.lr_scheduler import CosineAnnealingLR

optimizer = optim.SGD(model.parameters(), lr=0.1)
scheduler = CosineAnnealingLR(optimizer, T_max=100, eta_min=0.001)

lrs = []
for epoch in range(100):
    optimizer.step()
    scheduler.step()
    lrs.append(optimizer.param_groups[0]['lr'])

print(f"åˆå§‹LR: {lrs[0]:.6f}")
print(f"æœ€ä½LR: {min(lrs):.6f}")
print(f"æœ€ç»ˆLR: {lrs[-1]:.6f}")
```

### 5. ReduceLROnPlateauï¼ˆåŸºäºæŒ‡æ ‡ï¼‰

```python
from torch.optim.lr_scheduler import ReduceLROnPlateau

optimizer = optim.Adam(model.parameters(), lr=0.001)
scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)

for epoch in range(50):
    # è®­ç»ƒ...
    train_loss = 0.5 - epoch * 0.01  # æ¨¡æ‹ŸæŸå¤±ä¸‹é™
    
    # æ ¹æ®éªŒè¯æŸå¤±è°ƒæ•´å­¦ä¹ ç‡
    scheduler.step(train_loss)
    
    if epoch % 10 == 0:
        print(f"Epoch {epoch+1}, Loss: {train_loss:.4f}, LR: {optimizer.param_groups[0]['lr']:.6f}")
```

### 6. ç»„åˆè°ƒåº¦å™¨

```python
from torch.optim.lr_scheduler import SequentialLR, LinearLR, CosineAnnealingLR

optimizer = optim.SGD(model.parameters(), lr=0.1)

# å‰10ä¸ªepochçº¿æ€§å¢åŠ ï¼Œç„¶åä½™å¼¦é€€ç«
scheduler1 = LinearLR(optimizer, start_factor=0.1, total_iters=10)
scheduler2 = CosineAnnealingLR(optimizer, T_max=90)

scheduler = SequentialLR(optimizer, schedulers=[scheduler1, scheduler2], milestones=[10])

for epoch in range(100):
    optimizer.step()
    scheduler.step()
```

---

## ğŸ›¡ï¸ æ­£åˆ™åŒ–æŠ€æœ¯

### 1. L2 æ­£åˆ™åŒ–ï¼ˆæƒé‡è¡°å‡ï¼‰

```python
import torch
import torch.nn as nn
import torch.optim as optim

model = nn.Linear(10, 1)

# æ–¹å¼1: ä¼˜åŒ–å™¨ä¸­çš„ weight_decay
optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)

# æ–¹å¼2: æ‰‹åŠ¨æ·»åŠ 
optimizer = optim.Adam(model.parameters(), lr=0.001)
criterion = nn.MSELoss()

for epoch in range(100):
    optimizer.zero_grad()
    output = model(torch.randn(32, 10))
    loss = criterion(output, torch.randn(32, 1))
    
    # æ·»åŠ  L2 æ­£åˆ™åŒ–
    l2_lambda = 1e-4
    l2_reg = torch.tensor(0.)
    for param in model.parameters():
        l2_reg += torch.norm(param, 2)
    loss += l2_lambda * l2_reg
    
    loss.backward()
    optimizer.step()
```

### 2. Dropout

```python
import torch
import torch.nn as nn

class ModelWithDropout(nn.Module):
    def __init__(self, dropout_rate=0.5):
        super().__init__()
        self.fc1 = nn.Linear(784, 256)
        self.dropout1 = nn.Dropout(dropout_rate)
        self.fc2 = nn.Linear(256, 128)
        self.dropout2 = nn.Dropout(dropout_rate)
        self.fc3 = nn.Linear(128, 10)
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.dropout1(x)  # è®­ç»ƒæ—¶éšæœºä¸¢å¼ƒ
        x = torch.relu(self.fc2(x))
        x = self.dropout2(x)
        x = self.fc3(x)
        return x

model = ModelWithDropout(dropout_rate=0.5)

# è®­ç»ƒæ¨¡å¼
model.train()
x_train = torch.randn(32, 784)
output_train = model(x_train)
print(f"è®­ç»ƒè¾“å‡º: {output_train.shape}")

# è¯„ä¼°æ¨¡å¼ï¼ˆDropoutä¸ç”Ÿæ•ˆï¼‰
model.eval()
x_test = torch.randn(32, 784)
output_test = model(x_test)
print(f"è¯„ä¼°è¾“å‡º: {output_test.shape}")
```

### 3. Batch Normalization

```python
import torch
import torch.nn as nn

class ModelWithBN(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(784, 256)
        self.bn1 = nn.BatchNorm1d(256)
        self.fc2 = nn.Linear(256, 128)
        self.bn2 = nn.BatchNorm1d(128)
        self.fc3 = nn.Linear(128, 10)
    
    def forward(self, x):
        x = self.fc1(x)
        x = self.bn1(x)  # æ‰¹å½’ä¸€åŒ–
        x = torch.relu(x)
        x = self.fc2(x)
        x = self.bn2(x)
        x = torch.relu(x)
        x = self.fc3(x)
        return x

model = ModelWithBN()

# è®­ç»ƒæ¨¡å¼ï¼šæ›´æ–°è¿è¡Œç»Ÿè®¡é‡
model.train()
output = model(torch.randn(32, 784))

# è¯„ä¼°æ¨¡å¼ï¼šä½¿ç”¨è¿è¡Œç»Ÿè®¡é‡
model.eval()
output = model(torch.randn(32, 784))
```

### 4. æ—©åœï¼ˆEarly Stoppingï¼‰

```python
class EarlyStopping:
    def __init__(self, patience=7, min_delta=0):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = None
        self.early_stop = False
    
    def __call__(self, val_loss):
        if self.best_loss is None:
            self.best_loss = val_loss
        elif val_loss > self.best_loss - self.min_delta:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_loss = val_loss
            self.counter = 0

# ä½¿ç”¨
early_stopping = EarlyStopping(patience=5)

for epoch in range(100):
    # è®­ç»ƒ...
    val_loss = 0.5  # æ¨¡æ‹ŸéªŒè¯æŸå¤±
    
    early_stopping(val_loss)
    if early_stopping.early_stop:
        print(f"Early stopping at epoch {epoch+1}")
        break
```

---

## ğŸ’¡ è®­ç»ƒæŠ€å·§

### 1. æ¢¯åº¦è£å‰ª

```python
import torch
import torch.nn as nn
import torch.optim as optim

model = nn.LSTM(100, 512, num_layers=2)
optimizer = optim.Adam(model.parameters())
criterion = nn.CrossEntropyLoss()

for epoch in range(100):
    optimizer.zero_grad()
    
    # å‰å‘ä¼ æ’­
    x = torch.randn(32, 100, 100)
    output, _ = model(x)
    loss = criterion(output.view(-1, 512), torch.randint(0, 512, (3200,)))
    
    # åå‘ä¼ æ’­
    loss.backward()
    
    # æ¢¯åº¦è£å‰ª
    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
    
    optimizer.step()
```

### 2. æ··åˆç²¾åº¦è®­ç»ƒ

```python
import torch
import torch.nn as nn
from torch.cuda.amp import autocast, GradScaler

model = nn.Linear(10, 5).cuda()
optimizer = torch.optim.Adam(model.parameters())
scaler = GradScaler()

for epoch in range(100):
    optimizer.zero_grad()
    
    # æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
    with autocast():
        output = model(torch.randn(32, 10).cuda())
        loss = nn.functional.mse_loss(output, torch.randn(32, 5).cuda())
    
    # ç¼©æ”¾æŸå¤±å¹¶åå‘ä¼ æ’­
    scaler.scale(loss).backward()
    
    # æ›´æ–°å‚æ•°
    scaler.step(optimizer)
    scaler.update()
```

### 3. æ¢¯åº¦ç´¯ç§¯

```python
import torch
import torch.nn as nn

model = nn.Linear(10, 1)
optimizer = torch.optim.Adam(model.parameters())
criterion = nn.MSELoss()

accumulation_steps = 4

for epoch in range(100):
    for i, (data, target) in enumerate(data_loader):
        output = model(data)
        loss = criterion(output, target)
        
        # ç¼©æ”¾æŸå¤±
        loss = loss / accumulation_steps
        loss.backward()
        
        # ç´¯ç§¯å¤šä¸ªæ‰¹æ¬¡åæ›´æ–°
        if (i + 1) % accumulation_steps == 0:
            optimizer.step()
            optimizer.zero_grad()
```

---

## ğŸ› è°ƒè¯•å’Œç›‘æ§

### 1. æ£€æŸ¥æ¢¯åº¦

```python
import torch
import torch.nn as nn

model = nn.Sequential(
    nn.Linear(10, 20),
    nn.ReLU(),
    nn.Linear(20, 1)
)

# å‰å‘å’Œåå‘
x = torch.randn(32, 10)
y = torch.randn(32, 1)
loss = nn.functional.mse_loss(model(x), y)
loss.backward()

# æ£€æŸ¥æ¢¯åº¦
for name, param in model.named_parameters():
    if param.grad is not None:
        print(f"{name}:")
        print(f"  æ¢¯åº¦èŒƒæ•°: {param.grad.norm().item():.6f}")
        print(f"  æ¢¯åº¦å‡å€¼: {param.grad.mean().item():.6f}")
        print(f"  æ¢¯åº¦æ ‡å‡†å·®: {param.grad.std().item():.6f}")
```

### 2. ä½¿ç”¨TensorBoard

```python
from torch.utils.tensorboard import SummaryWriter
import torch
import torch.nn as nn

# åˆ›å»º writer
writer = SummaryWriter('runs/experiment_1')

model = nn.Linear(10, 1)
optimizer = torch.optim.Adam(model.parameters())

for epoch in range(100):
    # è®­ç»ƒ...
    loss = torch.tensor(0.5 - epoch * 0.005)
    
    # è®°å½•æŸå¤±
    writer.add_scalar('Loss/train', loss.item(), epoch)
    
    # è®°å½•å­¦ä¹ ç‡
    writer.add_scalar('Learning_rate', optimizer.param_groups[0]['lr'], epoch)
    
    # è®°å½•å‚æ•°ç›´æ–¹å›¾
    for name, param in model.named_parameters():
        writer.add_histogram(name, param, epoch)
        if param.grad is not None:
            writer.add_histogram(f'{name}.grad', param.grad, epoch)

writer.close()
print("è¿è¡Œ: tensorboard --logdir=runs")
```

---

## ğŸ“ æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **è®­ç»ƒæµç¨‹**: å‰å‘ â†’ æŸå¤± â†’ åå‘ â†’ æ›´æ–° â†’ æ¸…é›¶
2. **æŸå¤±å‡½æ•°**: CrossEntropyLoss, MSELoss, è‡ªå®šä¹‰
3. **ä¼˜åŒ–å™¨**: Adamï¼ˆé»˜è®¤é€‰æ‹©ï¼‰, SGDï¼ˆéœ€è°ƒå‚ï¼‰
4. **å­¦ä¹ ç‡è°ƒåº¦**: StepLR, CosineAnnealing, ReduceLROnPlateau
5. **æ­£åˆ™åŒ–**: Dropout, BatchNorm, Weight Decay, Early Stopping

### æœ€ä½³å®è·µ

- âœ… ä½¿ç”¨ Adam ä½œä¸ºé»˜è®¤ä¼˜åŒ–å™¨
- âœ… æ·»åŠ å­¦ä¹ ç‡è°ƒåº¦
- âœ… ä½¿ç”¨ Dropout å’Œ BN é˜²æ­¢è¿‡æ‹Ÿåˆ
- âœ… ç›‘æ§è®­ç»ƒå’ŒéªŒè¯æŒ‡æ ‡
- âœ… ä¿å­˜æœ€ä½³æ¨¡å‹
- âœ… ä½¿ç”¨æ¢¯åº¦è£å‰ªï¼ˆRNNï¼‰
- âœ… å°è¯•æ··åˆç²¾åº¦è®­ç»ƒï¼ˆGPUï¼‰

---

**ä¸‹ä¸€æ­¥**: [å®æˆ˜æ¡ˆä¾‹ï¼šå›¾åƒåˆ†ç±»](./7_å®æˆ˜æ¡ˆä¾‹_å›¾åƒåˆ†ç±».md)

*æœ€åæ›´æ–°: 2025-10-17*
