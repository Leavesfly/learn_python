# å¼ é‡æ“ä½œè¯¦è§£

## ğŸ“‹ ç›®å½•

- [å¼ é‡åŸºç¡€](#å¼ é‡åŸºç¡€)
- [åˆ›å»ºå¼ é‡](#åˆ›å»ºå¼ é‡)
- [å¼ é‡å±æ€§](#å¼ é‡å±æ€§)
- [ç´¢å¼•å’Œåˆ‡ç‰‡](#ç´¢å¼•å’Œåˆ‡ç‰‡)
- [å¼ é‡å˜å½¢](#å¼ é‡å˜å½¢)
- [æ•°å­¦è¿ç®—](#æ•°å­¦è¿ç®—)
- [å¹¿æ’­æœºåˆ¶](#å¹¿æ’­æœºåˆ¶)
- [ä¸ NumPy äº’æ“ä½œ](#ä¸-numpy-äº’æ“ä½œ)

---

## ğŸ¯ å¼ é‡åŸºç¡€

å¼ é‡ï¼ˆTensorï¼‰æ˜¯ PyTorch çš„æ ¸å¿ƒæ•°æ®ç»“æ„ï¼Œå¯ä»¥çœ‹ä½œæ˜¯**å¤šç»´æ•°ç»„çš„æ³›åŒ–**ï¼š

- **0ç»´å¼ é‡**ï¼šæ ‡é‡ï¼ˆscalarï¼‰- å•ä¸ªæ•°å­—
- **1ç»´å¼ é‡**ï¼šå‘é‡ï¼ˆvectorï¼‰- æ•°ç»„
- **2ç»´å¼ é‡**ï¼šçŸ©é˜µï¼ˆmatrixï¼‰
- **3ç»´åŠä»¥ä¸Š**ï¼šé«˜ç»´å¼ é‡

```python
import torch

# ä¸åŒç»´åº¦çš„å¼ é‡
scalar = torch.tensor(42)           # 0ç»´
vector = torch.tensor([1, 2, 3])    # 1ç»´
matrix = torch.tensor([[1, 2],      # 2ç»´
                       [3, 4]])
tensor_3d = torch.randn(2, 3, 4)    # 3ç»´

print(f"æ ‡é‡ç»´åº¦: {scalar.ndim}")     # 0
print(f"å‘é‡ç»´åº¦: {vector.ndim}")     # 1
print(f"çŸ©é˜µç»´åº¦: {matrix.ndim}")     # 2
print(f"3Då¼ é‡ç»´åº¦: {tensor_3d.ndim}") # 3
```

---

## ğŸ”¨ åˆ›å»ºå¼ é‡

### 1. ä»æ•°æ®åˆ›å»º

```python
import torch
import numpy as np

# ä» Python åˆ—è¡¨
data = [[1, 2], [3, 4]]
x = torch.tensor(data)
print(x)

# ä» NumPy æ•°ç»„
np_array = np.array(data)
x_np = torch.from_numpy(np_array)
print(x_np)

# ç›´æ¥åˆ›å»ºï¼ˆæŒ‡å®šæ•°æ®ç±»å‹ï¼‰
x_float = torch.tensor([1, 2, 3], dtype=torch.float32)
x_long = torch.tensor([1, 2, 3], dtype=torch.long)
print(f"float32: {x_float.dtype}")
print(f"long: {x_long.dtype}")
```

### 2. ç‰¹æ®Šå¼ é‡åˆ›å»º

```python
import torch

shape = (3, 4)

# å…¨é›¶å¼ é‡
zeros = torch.zeros(shape)

# å…¨ä¸€å¼ é‡
ones = torch.ones(shape)

# éšæœºå¼ é‡ï¼ˆå‡åŒ€åˆ†å¸ƒ [0, 1)ï¼‰
rand = torch.rand(shape)

# éšæœºå¼ é‡ï¼ˆæ ‡å‡†æ­£æ€åˆ†å¸ƒï¼‰
randn = torch.randn(shape)

# ç©ºå¼ é‡ï¼ˆæœªåˆå§‹åŒ–ï¼Œæ›´å¿«ï¼‰
empty = torch.empty(shape)

# å•ä½çŸ©é˜µ
eye = torch.eye(4)

# ç­‰å·®æ•°åˆ—
arange = torch.arange(0, 10, 2)  # [0, 2, 4, 6, 8]

# ç­‰åˆ†æ•°åˆ—
linspace = torch.linspace(0, 10, 5)  # [0, 2.5, 5, 7.5, 10]

print(f"zeros:\n{zeros}\n")
print(f"ones:\n{ones}\n")
print(f"rand:\n{rand}\n")
print(f"randn:\n{randn}\n")
print(f"eye:\n{eye}\n")
print(f"arange: {arange}")
print(f"linspace: {linspace}")
```

### 3. åŸºäºç°æœ‰å¼ é‡åˆ›å»º

```python
import torch

x = torch.tensor([[1, 2], [3, 4]])

# ä¿æŒå½¢çŠ¶ï¼Œæ•°æ®ç±»å‹å¯é€‰
x_ones = torch.ones_like(x)
x_rand = torch.rand_like(x, dtype=torch.float)

# æ–°å½¢çŠ¶
x_new = torch.zeros(5, 3)

print(f"åŸå¼ é‡:\n{x}\n")
print(f"ones_like:\n{x_ones}\n")
print(f"rand_like:\n{x_rand}\n")
```

---

## ğŸ“Š å¼ é‡å±æ€§

æ¯ä¸ªå¼ é‡éƒ½æœ‰é‡è¦çš„å±æ€§ï¼Œäº†è§£è¿™äº›å±æ€§å¯¹äºæ­£ç¡®ä½¿ç”¨å¼ é‡è‡³å…³é‡è¦ã€‚

```python
import torch

x = torch.randn(3, 4, 5)

# åŸºæœ¬å±æ€§
print(f"å½¢çŠ¶ (shape): {x.shape}")           # torch.Size([3, 4, 5])
print(f"å¤§å° (size): {x.size()}")           # åŒä¸Š
print(f"ç»´åº¦æ•° (ndim): {x.ndim}")           # 3
print(f"å…ƒç´ æ€»æ•° (numel): {x.numel()}")     # 60
print(f"æ•°æ®ç±»å‹ (dtype): {x.dtype}")       # torch.float32
print(f"è®¾å¤‡ (device): {x.device}")         # cpu
print(f"å†…å­˜å¸ƒå±€ (layout): {x.layout}")     # torch.strided
```

### æ•°æ®ç±»å‹è¯¦è§£

```python
import torch

# æµ®ç‚¹ç±»å‹
torch.float16  # åŠç²¾åº¦æµ®ç‚¹
torch.float32  # å•ç²¾åº¦æµ®ç‚¹ï¼ˆé»˜è®¤ï¼‰
torch.float64  # åŒç²¾åº¦æµ®ç‚¹

# æ•´æ•°ç±»å‹
torch.int8     # æœ‰ç¬¦å·8ä½æ•´æ•°
torch.int16    # æœ‰ç¬¦å·16ä½æ•´æ•°
torch.int32    # æœ‰ç¬¦å·32ä½æ•´æ•°
torch.int64    # æœ‰ç¬¦å·64ä½æ•´æ•°ï¼ˆé»˜è®¤ï¼‰
torch.uint8    # æ— ç¬¦å·8ä½æ•´æ•°

# å¸ƒå°”ç±»å‹
torch.bool

# ç±»å‹è½¬æ¢
x = torch.tensor([1, 2, 3])
x_float = x.float()       # è½¬ä¸º float32
x_double = x.double()     # è½¬ä¸º float64
x_int = x_float.int()     # è½¬ä¸º int32
x_bool = x.bool()         # è½¬ä¸º bool

print(f"åŸç±»å‹: {x.dtype}")
print(f"float: {x_float.dtype}")
print(f"double: {x_double.dtype}")
print(f"int: {x_int.dtype}")
print(f"bool: {x_bool.dtype}")
```

---

## ğŸ” ç´¢å¼•å’Œåˆ‡ç‰‡

PyTorch çš„ç´¢å¼•å’Œåˆ‡ç‰‡æ“ä½œä¸ NumPy éå¸¸ç›¸ä¼¼ã€‚

### 1. åŸºç¡€ç´¢å¼•

```python
import torch

x = torch.arange(12).reshape(3, 4)
print(f"åŸå¼ é‡:\n{x}\n")

# å•ä¸ªå…ƒç´ 
print(f"x[0, 0]: {x[0, 0]}")       # 0
print(f"x[1, 2]: {x[1, 2]}")       # 6
print(f"x[-1, -1]: {x[-1, -1]}")   # 11

# æ•´è¡Œ/æ•´åˆ—
print(f"ç¬¬ä¸€è¡Œ: {x[0]}")            # tensor([0, 1, 2, 3])
print(f"æœ€åä¸€è¡Œ: {x[-1]}")         # tensor([8, 9, 10, 11])
print(f"ç¬¬ä¸€åˆ—: {x[:, 0]}")         # tensor([0, 4, 8])
print(f"æœ€åä¸€åˆ—: {x[:, -1]}")      # tensor([3, 7, 11])
```

### 2. åˆ‡ç‰‡æ“ä½œ

```python
import torch

x = torch.arange(20).reshape(4, 5)
print(f"åŸå¼ é‡:\n{x}\n")

# åˆ‡ç‰‡è¯­æ³•: start:stop:step
print(f"å‰ä¸¤è¡Œ:\n{x[:2]}\n")
print(f"å‰ä¸‰åˆ—:\n{x[:, :3]}\n")
print(f"éš”è¡Œé‡‡æ ·:\n{x[::2]}\n")
print(f"éš”åˆ—é‡‡æ ·:\n{x[:, ::2]}\n")

# å¤æ‚åˆ‡ç‰‡
print(f"ä¸­é—´2x2å­çŸ©é˜µ:\n{x[1:3, 1:3]}\n")
```

### 3. é«˜çº§ç´¢å¼•

```python
import torch

x = torch.arange(10)

# å¸ƒå°”ç´¢å¼•
mask = x > 5
print(f"å¤§äº5çš„å…ƒç´ : {x[mask]}")  # tensor([6, 7, 8, 9])

# èŠ±å¼ç´¢å¼•ï¼ˆæ•´æ•°æ•°ç»„ï¼‰
indices = torch.tensor([1, 3, 5, 7])
print(f"æŒ‡å®šç´¢å¼•çš„å…ƒç´ : {x[indices]}")  # tensor([1, 3, 5, 7])

# å¤šç»´é«˜çº§ç´¢å¼•
x = torch.arange(12).reshape(3, 4)
rows = torch.tensor([0, 2, 1])
cols = torch.tensor([1, 3, 2])
print(f"é«˜çº§ç´¢å¼•ç»“æœ: {x[rows, cols]}")  # tensor([1, 11, 6])
```

### 4. åŸåœ°ä¿®æ”¹

```python
import torch

x = torch.arange(12).reshape(3, 4)
print(f"åŸå¼ é‡:\n{x}\n")

# ä¿®æ”¹å•ä¸ªå…ƒç´ 
x[0, 0] = 100

# ä¿®æ”¹æ•´è¡Œ
x[1] = torch.tensor([10, 20, 30, 40])

# ä¿®æ”¹åˆ‡ç‰‡
x[:, -1] = 0

print(f"ä¿®æ”¹å:\n{x}\n")
```

---

## ğŸ”„ å¼ é‡å˜å½¢

æ”¹å˜å¼ é‡çš„å½¢çŠ¶æ˜¯æ·±åº¦å­¦ä¹ ä¸­çš„å¸¸è§æ“ä½œã€‚

### 1. reshape å’Œ view

```python
import torch

x = torch.arange(12)
print(f"åŸå¼ é‡: {x.shape}")

# reshape: æ€»æ˜¯è¿”å›æ–°çš„å½¢çŠ¶
x_reshaped = x.reshape(3, 4)
print(f"reshapeå: {x_reshaped.shape}")
print(x_reshaped)

# view: è¦æ±‚å†…å­˜è¿ç»­
x_viewed = x.view(4, 3)
print(f"viewå: {x_viewed.shape}")
print(x_viewed)

# -1 è¡¨ç¤ºè‡ªåŠ¨æ¨æ–­
x_auto = x.reshape(3, -1)  # è‡ªåŠ¨æ¨æ–­ä¸º (3, 4)
print(f"è‡ªåŠ¨æ¨æ–­: {x_auto.shape}")
```

**view vs reshape çš„åŒºåˆ«**ï¼š

```python
import torch

x = torch.arange(12).reshape(3, 4)
y = x.t()  # è½¬ç½®ï¼ˆéè¿ç»­ï¼‰

# view è¦æ±‚è¿ç»­å†…å­˜
try:
    z = y.view(12)  # é”™è¯¯ï¼
except RuntimeError as e:
    print(f"viewé”™è¯¯: {e}")

# reshape å¯ä»¥å¤„ç†éè¿ç»­
z = y.reshape(12)  # æ­£ç¡®
print(f"reshapeæˆåŠŸ: {z}")

# ä½¿ç”¨ contiguous ä½¿å†…å­˜è¿ç»­
y_cont = y.contiguous()
z = y_cont.view(12)  # ç°åœ¨å¯ä»¥äº†
print(f"contiguousåview: {z}")
```

### 2. ç»´åº¦æ“ä½œ

```python
import torch

x = torch.randn(3, 4, 5)

# squeeze: ç§»é™¤å¤§å°ä¸º1çš„ç»´åº¦
y = torch.randn(1, 3, 1, 4)
print(f"åŸå½¢çŠ¶: {y.shape}")
print(f"squeezeå: {y.squeeze().shape}")  # (3, 4)
print(f"squeezeæŒ‡å®šç»´åº¦: {y.squeeze(0).shape}")  # (3, 1, 4)

# unsqueeze: å¢åŠ ç»´åº¦
x = torch.randn(3, 4)
print(f"åŸå½¢çŠ¶: {x.shape}")
print(f"unsqueeze(0): {x.unsqueeze(0).shape}")  # (1, 3, 4)
print(f"unsqueeze(1): {x.unsqueeze(1).shape}")  # (3, 1, 4)
print(f"unsqueeze(-1): {x.unsqueeze(-1).shape}")  # (3, 4, 1)

# ç»´åº¦äº¤æ¢
x = torch.randn(2, 3, 4)
print(f"åŸå½¢çŠ¶: {x.shape}")
print(f"transpose(0,1): {x.transpose(0, 1).shape}")  # (3, 2, 4)
print(f"permute: {x.permute(2, 0, 1).shape}")  # (4, 2, 3)

# å±•å¹³
x = torch.randn(2, 3, 4)
print(f"flatten: {x.flatten().shape}")  # (24,)
print(f"flatten(1): {x.flatten(1).shape}")  # (2, 12) ä»ç»´åº¦1å¼€å§‹å±•å¹³
```

### 3. æ‹¼æ¥å’Œåˆ†å‰²

```python
import torch

# æ‹¼æ¥
x1 = torch.randn(2, 3)
x2 = torch.randn(2, 3)
x3 = torch.randn(2, 3)

# cat: æ²¿ç°æœ‰ç»´åº¦æ‹¼æ¥
y = torch.cat([x1, x2, x3], dim=0)  # (6, 3)
print(f"cat dim=0: {y.shape}")

y = torch.cat([x1, x2, x3], dim=1)  # (2, 9)
print(f"cat dim=1: {y.shape}")

# stack: åˆ›å»ºæ–°ç»´åº¦
y = torch.stack([x1, x2, x3], dim=0)  # (3, 2, 3)
print(f"stack dim=0: {y.shape}")

# åˆ†å‰²
x = torch.arange(12).reshape(4, 3)

# split: æŒ‰å¤§å°åˆ†å‰²
chunks = torch.split(x, 2, dim=0)  # åˆ†æˆä¸¤ä¸ª (2, 3)
print(f"split: {[c.shape for c in chunks]}")

# chunk: åˆ†æˆNä»½
chunks = torch.chunk(x, 2, dim=0)  # åˆ†æˆ2ä»½
print(f"chunk: {[c.shape for c in chunks]}")
```

---

## ğŸ§® æ•°å­¦è¿ç®—

### 1. åŸºç¡€è¿ç®—

```python
import torch

x = torch.tensor([1.0, 2.0, 3.0])
y = torch.tensor([4.0, 5.0, 6.0])

# é€å…ƒç´ è¿ç®—
print(f"åŠ æ³•: {x + y}")           # tensor([5., 7., 9.])
print(f"å‡æ³•: {x - y}")           # tensor([-3., -3., -3.])
print(f"ä¹˜æ³•: {x * y}")           # tensor([4., 10., 18.])
print(f"é™¤æ³•: {x / y}")           # tensor([0.25, 0.4, 0.5])
print(f"å¹‚è¿ç®—: {x ** 2}")        # tensor([1., 4., 9.])

# å‡½æ•°å½¢å¼
print(f"torch.add: {torch.add(x, y)}")
print(f"torch.sub: {torch.sub(x, y)}")
print(f"torch.mul: {torch.mul(x, y)}")
print(f"torch.div: {torch.div(x, y)}")

# åŸåœ°è¿ç®—
x.add_(y)  # x = x + y
print(f"åŸåœ°åŠ æ³•å: {x}")
```

### 2. çŸ©é˜µè¿ç®—

```python
import torch

# å‘é‡ç‚¹ç§¯
v1 = torch.tensor([1.0, 2.0, 3.0])
v2 = torch.tensor([4.0, 5.0, 6.0])
dot_product = torch.dot(v1, v2)
print(f"ç‚¹ç§¯: {dot_product}")  # 1*4 + 2*5 + 3*6 = 32

# çŸ©é˜µä¹˜æ³•
A = torch.randn(3, 4)
B = torch.randn(4, 5)

# @ è¿ç®—ç¬¦
C = A @ B
print(f"A @ B: {C.shape}")  # (3, 5)

# matmul å‡½æ•°
C = torch.matmul(A, B)
print(f"matmul: {C.shape}")

# mm (çŸ©é˜µä¹˜æ³•ï¼Œä»…2D)
C = torch.mm(A, B)
print(f"mm: {C.shape}")

# bmm (æ‰¹é‡çŸ©é˜µä¹˜æ³•)
batch_A = torch.randn(10, 3, 4)
batch_B = torch.randn(10, 4, 5)
batch_C = torch.bmm(batch_A, batch_B)
print(f"bmm: {batch_C.shape}")  # (10, 3, 5)
```

### 3. ç»Ÿè®¡å‡½æ•°

```python
import torch

x = torch.randn(3, 4)

# èšåˆç»Ÿè®¡
print(f"æ€»å’Œ: {x.sum()}")
print(f"å‡å€¼: {x.mean()}")
print(f"æ ‡å‡†å·®: {x.std()}")
print(f"æ–¹å·®: {x.var()}")
print(f"æœ€å¤§å€¼: {x.max()}")
print(f"æœ€å°å€¼: {x.min()}")

# æ²¿æŒ‡å®šç»´åº¦
print(f"æ¯è¡Œçš„å’Œ: {x.sum(dim=1)}")      # shape: (3,)
print(f"æ¯åˆ—çš„å’Œ: {x.sum(dim=0)}")      # shape: (4,)
print(f"æ¯åˆ—çš„å‡å€¼: {x.mean(dim=0)}")   # shape: (4,)

# ä¿æŒç»´åº¦
print(f"ä¿æŒç»´åº¦çš„å’Œ: {x.sum(dim=1, keepdim=True).shape}")  # (3, 1)

# æœ€å¤§/æœ€å°å€¼åŠç´¢å¼•
max_val, max_idx = x.max(dim=1)
print(f"æ¯è¡Œæœ€å¤§å€¼: {max_val}")
print(f"æœ€å¤§å€¼ç´¢å¼•: {max_idx}")
```

### 4. å…¶ä»–æ•°å­¦å‡½æ•°

```python
import torch
import math

x = torch.tensor([0.0, math.pi/4, math.pi/2])

# ä¸‰è§’å‡½æ•°
print(f"sin: {torch.sin(x)}")
print(f"cos: {torch.cos(x)}")
print(f"tan: {torch.tan(x)}")

# æŒ‡æ•°å’Œå¯¹æ•°
x = torch.tensor([1.0, 2.0, 3.0])
print(f"exp: {torch.exp(x)}")       # e^x
print(f"log: {torch.log(x)}")       # ln(x)
print(f"log10: {torch.log10(x)}")   # log10(x)

# å››èˆäº”å…¥
x = torch.tensor([1.2, 2.5, 3.7])
print(f"round: {torch.round(x)}")   # tensor([1., 2., 4.])
print(f"floor: {torch.floor(x)}")   # tensor([1., 2., 3.])
print(f"ceil: {torch.ceil(x)}")     # tensor([2., 3., 4.])

# é™å¹…
x = torch.randn(5)
print(f"åŸå€¼: {x}")
print(f"clamp(0, 1): {torch.clamp(x, 0, 1)}")  # é™åˆ¶åœ¨ [0, 1]
```

---

## ğŸ“¡ å¹¿æ’­æœºåˆ¶

å¹¿æ’­ï¼ˆBroadcastingï¼‰å…è®¸ä¸åŒå½¢çŠ¶çš„å¼ é‡è¿›è¡Œè¿ç®—ï¼ŒPyTorch ä¼šè‡ªåŠ¨æ‰©å±•ç»´åº¦ã€‚

### å¹¿æ’­è§„åˆ™

1. ä»åå‘å‰æ¯”è¾ƒç»´åº¦
2. ç»´åº¦ç›¸ç­‰ï¼Œæˆ–å…¶ä¸­ä¸€ä¸ªä¸º 1ï¼Œæˆ–å…¶ä¸­ä¸€ä¸ªä¸å­˜åœ¨
3. è‡ªåŠ¨æ‰©å±•ç»´åº¦ä¸º 1 çš„ç»´åº¦

```python
import torch

# è§„åˆ™1: æ ‡é‡ä¸å¼ é‡
x = torch.randn(3, 4)
y = 5
z = x + y  # y è¢«å¹¿æ’­ä¸º (3, 4)
print(f"æ ‡é‡å¹¿æ’­: {z.shape}")

# è§„åˆ™2: ä¸åŒå½¢çŠ¶
x = torch.randn(3, 4)
y = torch.randn(4)      # (4,)
z = x + y               # y è¢«å¹¿æ’­ä¸º (3, 4)
print(f"1Då¹¿æ’­: {z.shape}")

# è§„åˆ™3: æ›´å¤æ‚çš„å¹¿æ’­
x = torch.randn(3, 1, 4)  # (3, 1, 4)
y = torch.randn(1, 5, 4)  # (1, 5, 4)
z = x + y                 # ç»“æœ: (3, 5, 4)
print(f"å¤æ‚å¹¿æ’­: {z.shape}")

# æ˜¾å¼å¹¿æ’­
x = torch.randn(3, 1)
y = x.expand(3, 4)  # æ‰©å±•ä¸º (3, 4)
print(f"expand: {y.shape}")

# é‡å¤
y = x.repeat(1, 4)  # é‡å¤ä¸º (3, 4)
print(f"repeat: {y.shape}")
```

### å¹¿æ’­ç¤ºä¾‹

```python
import torch

# ç¤ºä¾‹1: æ ‡å‡†åŒ–
data = torch.randn(100, 5)  # 100ä¸ªæ ·æœ¬ï¼Œ5ä¸ªç‰¹å¾

# è®¡ç®—æ¯ä¸ªç‰¹å¾çš„å‡å€¼å’Œæ ‡å‡†å·®
mean = data.mean(dim=0, keepdim=True)  # (1, 5)
std = data.std(dim=0, keepdim=True)    # (1, 5)

# æ ‡å‡†åŒ–ï¼ˆå¹¿æ’­ï¼‰
normalized = (data - mean) / std
print(f"æ ‡å‡†åŒ–: {normalized.shape}")

# ç¤ºä¾‹2: æ·»åŠ ä½ç½®ç¼–ç 
batch_size, seq_len, d_model = 32, 100, 512
x = torch.randn(batch_size, seq_len, d_model)

# ä½ç½®ç¼–ç 
position = torch.arange(seq_len).unsqueeze(1)  # (seq_len, 1)
encoding = torch.randn(seq_len, d_model)       # (seq_len, d_model)

# å¹¿æ’­æ·»åŠ 
x = x + encoding  # encoding è¢«å¹¿æ’­ä¸º (1, seq_len, d_model)
print(f"æ·»åŠ ç¼–ç : {x.shape}")
```

---

## ğŸ”— ä¸ NumPy äº’æ“ä½œ

PyTorch å’Œ NumPy å¯ä»¥æ— ç¼è½¬æ¢ï¼Œ**å…±äº«åº•å±‚å†…å­˜**ï¼ˆCPU å¼ é‡ï¼‰ã€‚

### 1. è½¬æ¢

```python
import torch
import numpy as np

# NumPy â†’ PyTorch
np_array = np.array([1, 2, 3, 4, 5])
tensor = torch.from_numpy(np_array)
print(f"NumPyè½¬å¼ é‡: {tensor}")

# PyTorch â†’ NumPy
tensor = torch.tensor([1, 2, 3, 4, 5])
np_array = tensor.numpy()
print(f"å¼ é‡è½¬NumPy: {np_array}")

# ç±»å‹: ndarray
print(f"ç±»å‹: {type(np_array)}")
```

### 2. å†…å­˜å…±äº«

```python
import torch
import numpy as np

# å…±äº«å†…å­˜ï¼ˆCPUå¼ é‡ï¼‰
np_array = np.array([1, 2, 3])
tensor = torch.from_numpy(np_array)

# ä¿®æ”¹NumPyæ•°ç»„
np_array[0] = 100
print(f"ä¿®æ”¹åå¼ é‡: {tensor}")  # tensor([100, 2, 3])

# ä¿®æ”¹å¼ é‡
tensor[1] = 200
print(f"ä¿®æ”¹åNumPy: {np_array}")  # [100 200   3]

# âš ï¸ GPUå¼ é‡ä¸å…±äº«å†…å­˜
if torch.cuda.is_available():
    gpu_tensor = torch.tensor([1, 2, 3]).cuda()
    np_array = gpu_tensor.cpu().numpy()  # éœ€è¦å…ˆç§»åˆ°CPU
```

### 3. å¸¸è§æ“ä½œå¯¹æ¯”

```python
import torch
import numpy as np

# NumPy
np_x = np.random.randn(3, 4)
np_sum = np.sum(np_x, axis=0)
np_mean = np.mean(np_x)
np_reshape = np_x.reshape(2, 6)

# PyTorch
torch_x = torch.randn(3, 4)
torch_sum = torch.sum(torch_x, dim=0)
torch_mean = torch.mean(torch_x)
torch_reshape = torch_x.reshape(2, 6)

print("NumPyå’ŒPyTorchæ“ä½œç±»ä¼¼ï¼")
```

---

## ğŸ¯ å®æˆ˜ç»ƒä¹ 

### ç»ƒä¹ 1: å¼ é‡æ“ä½œç»¼åˆ

```python
import torch

# 1. åˆ›å»ºä¸€ä¸ª 5x5 çš„éšæœºçŸ©é˜µ
A = torch.randn(5, 5)

# 2. æå–ä¸»å¯¹è§’çº¿
diagonal = torch.diag(A)
print(f"ä¸»å¯¹è§’çº¿: {diagonal}")

# 3. å°†çŸ©é˜µä¸Šä¸‰è§’ç½®é›¶
A_lower = torch.tril(A)
print(f"ä¸‹ä¸‰è§’çŸ©é˜µ:\n{A_lower}")

# 4. è®¡ç®—æ¯è¡Œçš„å’Œ
row_sums = A.sum(dim=1)
print(f"æ¯è¡Œå’Œ: {row_sums}")

# 5. å½’ä¸€åŒ–ï¼ˆæ¯è¡Œå’Œä¸º1ï¼‰
A_normalized = A / A.sum(dim=1, keepdim=True)
print(f"å½’ä¸€åŒ–åæ¯è¡Œå’Œ: {A_normalized.sum(dim=1)}")
```

### ç»ƒä¹ 2: æ‰¹é‡å¤„ç†

```python
import torch

# æ¨¡æ‹Ÿæ‰¹é‡å›¾åƒ: (batch_size, channels, height, width)
batch_images = torch.randn(32, 3, 224, 224)

# 1. è®¡ç®—æ¯å¼ å›¾åƒçš„å‡å€¼ï¼ˆè·¨é€šé“å’Œç©ºé—´ï¼‰
image_means = batch_images.mean(dim=[1, 2, 3])
print(f"æ¯å¼ å›¾åƒå‡å€¼: {image_means.shape}")  # (32,)

# 2. æ ‡å‡†åŒ–æ¯å¼ å›¾åƒ
mean = batch_images.mean(dim=[2, 3], keepdim=True)  # (32, 3, 1, 1)
std = batch_images.std(dim=[2, 3], keepdim=True)    # (32, 3, 1, 1)
normalized = (batch_images - mean) / std
print(f"æ ‡å‡†åŒ–å: {normalized.shape}")  # (32, 3, 224, 224)

# 3. å°†å›¾åƒå±•å¹³ä¸ºå‘é‡
flattened = batch_images.flatten(1)  # (32, 3*224*224)
print(f"å±•å¹³å: {flattened.shape}")
```

---

## ğŸ“ æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **å¼ é‡åˆ›å»º**: `tensor()`, `zeros()`, `ones()`, `randn()` ç­‰
2. **å¼ é‡å±æ€§**: `shape`, `dtype`, `device`
3. **ç´¢å¼•åˆ‡ç‰‡**: ç±»ä¼¼ NumPyï¼Œæ”¯æŒé«˜çº§ç´¢å¼•
4. **å˜å½¢æ“ä½œ**: `reshape()`, `view()`, `squeeze()`, `unsqueeze()`
5. **æ•°å­¦è¿ç®—**: é€å…ƒç´ ã€çŸ©é˜µã€ç»Ÿè®¡å‡½æ•°
6. **å¹¿æ’­æœºåˆ¶**: è‡ªåŠ¨æ‰©å±•ç»´åº¦
7. **NumPyäº’æ“ä½œ**: `from_numpy()`, `.numpy()`

### æœ€ä½³å®è·µ

- âœ… ä½¿ç”¨ `tensor.shape` è€Œä¸æ˜¯ `tensor.size()`ï¼ˆæ›´ Pythonicï¼‰
- âœ… ä¼˜å…ˆä½¿ç”¨ `reshape()` è€Œä¸æ˜¯ `view()`ï¼ˆæ›´å®‰å…¨ï¼‰
- âœ… ä½¿ç”¨ `@` è€Œä¸æ˜¯ `torch.matmul()`ï¼ˆæ›´ç®€æ´ï¼‰
- âœ… æ³¨æ„å†…å­˜è¿ç»­æ€§ï¼ˆ`contiguous()`ï¼‰
- âš ï¸ å°å¿ƒåŸåœ°æ“ä½œï¼ˆå¯èƒ½å½±å“æ¢¯åº¦ï¼‰

---

**ä¸‹ä¸€æ­¥**: [è‡ªåŠ¨å¾®åˆ†æœºåˆ¶](./7_è‡ªåŠ¨å¾®åˆ†æœºåˆ¶.md)

*æœ€åæ›´æ–°: 2025-10-17*
