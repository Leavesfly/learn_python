# 自动微分机制

## 📋 目录

- [自动微分基础](#自动微分基础)
- [计算图原理](#计算图原理)
- [梯度计算详解](#梯度计算详解)
- [梯度管理技巧](#梯度管理技巧)
- [自定义自动微分](#自定义自动微分)
- [实战案例](#实战案例)

---

## 🎯 自动微分基础

**自动微分（Automatic Differentiation, Autograd）**是 PyTorch 的核心功能，它能自动计算张量的梯度，是深度学习反向传播的基础。

### 为什么需要自动微分？

在深度学习中，我们需要计算损失函数对模型参数的梯度，以便进行梯度下降优化。手动计算梯度：

- ❌ 容易出错
- ❌ 耗时费力
- ❌ 难以处理复杂模型

自动微分：

- ✅ 自动计算，准确无误
- ✅ 高效快速
- ✅ 支持任意复杂的计算图

### 快速示例

```python
import torch

# 1. 创建需要梯度的张量
x = torch.tensor([2.0], requires_grad=True)

# 2. 定义计算（前向传播）
y = x ** 2          # y = x²
z = 2 * y + 3       # z = 2x² + 3

# 3. 反向传播
z.backward()

# 4. 获取梯度
print(f"x = {x.item()}")
print(f"z = {z.item()}")
print(f"dz/dx = {x.grad.item()}")  # 理论值: 4x = 8

# 验证: z = 2x² + 3, dz/dx = 4x = 4*2 = 8 ✓
```

---

## 🔄 计算图原理

PyTorch 使用**动态计算图（Dynamic Computational Graph）**来跟踪所有操作。

### 计算图的构成

```python
import torch

x = torch.tensor([3.0], requires_grad=True)
y = x ** 2
z = y + 5
w = z * 2

# 查看计算图
print(f"w.grad_fn: {w.grad_fn}")  # MulBackward0
print(f"z.grad_fn: {z.grad_fn}")  # AddBackward0
print(f"y.grad_fn: {y.grad_fn}")  # PowBackward0
print(f"x.grad_fn: {x.grad_fn}")  # None (叶子节点)
```

**计算图可视化**：

```
前向传播:
┌─────┐   **2    ┌─────┐   +5    ┌─────┐   *2    ┌─────┐
│ x=3 │ ───────> │ y=9 │ ───────> │z=14 │ ───────> │w=28 │
└─────┘          └─────┘          └─────┘          └─────┘
  叶子节点        中间节点         中间节点         输出节点

反向传播:
┌─────┐  dw/dx   ┌─────┐  dw/dy   ┌─────┐  dw/dz   ┌─────┐
│ x=3 │ <─────── │ y=9 │ <─────── │z=14 │ <─────── │w=28 │
└─────┘   =12    └─────┘   =2     └─────┘   =1     └─────┘
```

### 叶子节点与非叶子节点

```python
import torch

x = torch.tensor([1.0], requires_grad=True)
y = x + 2
z = y ** 2

# 检查是否是叶子节点
print(f"x.is_leaf: {x.is_leaf}")  # True  - 用户创建
print(f"y.is_leaf: {y.is_leaf}")  # False - 运算结果
print(f"z.is_leaf: {z.is_leaf}")  # False - 运算结果

# 默认只保留叶子节点的梯度
z.backward()
print(f"x.grad: {x.grad}")        # 可以访问
print(f"y.grad: {y.grad}")        # None (非叶子节点)

# 保留中间节点梯度
x = torch.tensor([1.0], requires_grad=True)
y = x + 2
y.retain_grad()  # 显式保留
z = y ** 2
z.backward()
print(f"y.grad: {y.grad}")  # 现在可以访问了
```

---

## 📐 梯度计算详解

### 1. 标量输出的梯度

```python
import torch

x = torch.tensor([2.0, 3.0], requires_grad=True)

# 标量输出
y = (x ** 2).sum()  # y = x₁² + x₂²

y.backward()

print(f"x: {x}")
print(f"y: {y}")
print(f"x.grad: {x.grad}")

# 验证: dy/dx₁ = 2x₁ = 4, dy/dx₂ = 2x₂ = 6 ✓
```

### 2. 非标量输出的梯度

对于非标量输出，需要指定 `gradient` 参数：

```python
import torch

x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)
y = x ** 2  # 非标量输出

# 需要传入与输出同形状的梯度
gradient = torch.tensor([1.0, 1.0, 1.0])
y.backward(gradient)

print(f"x.grad: {x.grad}")  # tensor([2., 4., 6.])

# 理解: 相当于计算 (y * gradient).sum() 的梯度
```

**为什么需要 gradient 参数？**

```python
import torch

x = torch.tensor([1.0, 2.0], requires_grad=True)
y = x ** 2

# 方式1: 传入 gradient
y.backward(torch.tensor([1.0, 1.0]))
print(f"方式1 - x.grad: {x.grad}")

x.grad.zero_()  # 清零梯度

# 方式2: 先求和再反向传播（等价）
y = x ** 2
y.sum().backward()
print(f"方式2 - x.grad: {x.grad}")
```

### 3. 多变量梯度

```python
import torch

# 两个输入
x = torch.tensor([2.0], requires_grad=True)
y = torch.tensor([3.0], requires_grad=True)

# 复杂函数
z = x ** 2 + y ** 3 + 2 * x * y

z.backward()

print(f"z = {z.item()}")
print(f"dz/dx = {x.grad.item()}")  # 2x + 2y = 4 + 6 = 10
print(f"dz/dy = {y.grad.item()}")  # 3y² + 2x = 27 + 4 = 31
```

### 4. 链式法则示例

```python
import torch

x = torch.tensor([1.0], requires_grad=True)

# 复杂的链式计算
a = x ** 2          # a = x²
b = a + 3           # b = x² + 3
c = b ** 3          # c = (x² + 3)³
d = c.sum()

d.backward()

print(f"d/dx = {x.grad.item()}")

# 手动计算验证:
# d = (x² + 3)³
# d/dx = 3(x² + 3)² · 2x = 3(1 + 3)² · 2 = 3 · 16 · 2 = 96
print("理论值: 96")
```

---

## 🎛️ 梯度管理技巧

### 1. 停止梯度跟踪

#### 方法1: torch.no_grad()

```python
import torch

x = torch.tensor([1.0], requires_grad=True)

# 在 no_grad 上下文中，不会构建计算图
with torch.no_grad():
    y = x ** 2
    print(f"y.requires_grad: {y.requires_grad}")  # False

# 常用于推理阶段
@torch.no_grad()
def inference(model, data):
    return model(data)
```

#### 方法2: detach()

```python
import torch

x = torch.tensor([1.0], requires_grad=True)
y = x ** 2

# detach 创建一个不需要梯度的副本
z = y.detach()
print(f"z.requires_grad: {z.requires_grad}")  # False

# 原张量不受影响
w = y ** 2
w.backward()
print(f"x.grad: {x.grad}")
```

#### 方法3: requires_grad_()

```python
import torch

x = torch.randn(3, 4)
print(f"初始: {x.requires_grad}")  # False

# 原地修改
x.requires_grad_(True)
print(f"修改后: {x.requires_grad}")  # True

# 也可以设置为 False
x.requires_grad_(False)
print(f"再次修改: {x.requires_grad}")  # False
```

### 2. 梯度清零

```python
import torch

x = torch.tensor([2.0], requires_grad=True)

# 第一次计算
y = x ** 2
y.backward()
print(f"第一次梯度: {x.grad}")  # 4

# 第二次计算（梯度会累积！）
y = x ** 3
y.backward()
print(f"累积梯度: {x.grad}")  # 4 + 12 = 16

# 清零梯度
x.grad.zero_()
print(f"清零后: {x.grad}")  # 0

# 第三次计算
y = x ** 3
y.backward()
print(f"清零后梯度: {x.grad}")  # 12
```

**为什么梯度会累积？**

```python
import torch

# 模拟小批量训练
x = torch.tensor([1.0], requires_grad=True)

for i in range(3):
    y = x ** 2
    y.backward()
    print(f"迭代 {i+1}, 梯度: {x.grad}")

# 输出:
# 迭代 1, 梯度: tensor([2.])
# 迭代 2, 梯度: tensor([4.])  <- 累积了
# 迭代 3, 梯度: tensor([6.])  <- 继续累积

# 正确做法: 每次迭代后清零
x.grad.zero_()
```

### 3. 梯度裁剪

防止梯度爆炸：

```python
import torch
import torch.nn as nn

# 模拟模型参数
params = [torch.randn(100, 100, requires_grad=True) for _ in range(5)]

# 计算损失并反向传播
loss = sum((p ** 2).sum() for p in params)
loss.backward()

# 方法1: 裁剪梯度范数
torch.nn.utils.clip_grad_norm_(params, max_norm=1.0)

# 方法2: 裁剪梯度值
torch.nn.utils.clip_grad_value_(params, clip_value=0.5)

print("梯度已裁剪")
```

---

## 🛠️ 自定义自动微分

### 1. 自定义 Function

```python
import torch

class MyReLU(torch.autograd.Function):
    @staticmethod
    def forward(ctx, input):
        """
        前向传播
        ctx: 上下文对象，用于保存反向传播需要的信息
        """
        ctx.save_for_backward(input)
        return input.clamp(min=0)
    
    @staticmethod
    def backward(ctx, grad_output):
        """
        反向传播
        grad_output: 上游梯度
        """
        input, = ctx.saved_tensors
        grad_input = grad_output.clone()
        grad_input[input < 0] = 0
        return grad_input

# 使用自定义函数
x = torch.randn(5, requires_grad=True)
y = MyReLU.apply(x)
y.sum().backward()

print(f"x: {x}")
print(f"y: {y}")
print(f"x.grad: {x.grad}")
```

### 2. 梯度检查

验证自定义梯度是否正确：

```python
import torch
from torch.autograd import gradcheck

class MySquare(torch.autograd.Function):
    @staticmethod
    def forward(ctx, input):
        ctx.save_for_backward(input)
        return input ** 2
    
    @staticmethod
    def backward(ctx, grad_output):
        input, = ctx.saved_tensors
        return 2 * input * grad_output

# 梯度检查
x = torch.randn(3, 4, dtype=torch.double, requires_grad=True)
test = gradcheck(MySquare.apply, x, eps=1e-6, atol=1e-4)
print(f"梯度检查通过: {test}")
```

---

## 💡 实战案例

### 案例1: 手写线性回归

```python
import torch
import matplotlib.pyplot as plt

# 生成数据
torch.manual_seed(42)
n_samples = 100
X = torch.randn(n_samples, 1) * 10
y_true = 3 * X + 2 + torch.randn(n_samples, 1) * 2

# 初始化参数
w = torch.randn(1, 1, requires_grad=True)
b = torch.randn(1, requires_grad=True)

# 训练参数
learning_rate = 0.01
n_epochs = 100
losses = []

for epoch in range(n_epochs):
    # 前向传播
    y_pred = X @ w + b
    
    # 计算损失（MSE）
    loss = ((y_pred - y_true) ** 2).mean()
    losses.append(loss.item())
    
    # 反向传播
    loss.backward()
    
    # 更新参数（手动梯度下降）
    with torch.no_grad():
        w -= learning_rate * w.grad
        b -= learning_rate * b.grad
    
    # 清零梯度
    w.grad.zero_()
    b.grad.zero_()
    
    if (epoch + 1) % 20 == 0:
        print(f"Epoch {epoch+1}: Loss = {loss.item():.4f}, w = {w.item():.4f}, b = {b.item():.4f}")

print(f"\n最终结果: w = {w.item():.4f}, b = {b.item():.4f}")
print(f"真实值: w = 3.0, b = 2.0")
```

### 案例2: 多层感知机

```python
import torch

class MLP:
    def __init__(self, input_size, hidden_size, output_size):
        # 初始化参数
        self.w1 = torch.randn(input_size, hidden_size, requires_grad=True)
        self.b1 = torch.zeros(hidden_size, requires_grad=True)
        self.w2 = torch.randn(hidden_size, output_size, requires_grad=True)
        self.b2 = torch.zeros(output_size, requires_grad=True)
    
    def forward(self, x):
        # 第一层
        h = x @ self.w1 + self.b1
        h = torch.relu(h)
        
        # 第二层
        out = h @ self.w2 + self.b2
        return out
    
    def parameters(self):
        return [self.w1, self.b1, self.w2, self.b2]

# 创建模型
mlp = MLP(input_size=10, hidden_size=20, output_size=1)

# 生成数据
x = torch.randn(32, 10)
y = torch.randn(32, 1)

# 前向传播
y_pred = mlp.forward(x)

# 计算损失
loss = ((y_pred - y) ** 2).mean()

# 反向传播
loss.backward()

# 查看梯度
for i, param in enumerate(mlp.parameters()):
    print(f"参数 {i} 的梯度形状: {param.grad.shape}")
```

### 案例3: 梯度累积

模拟大批量训练：

```python
import torch
import torch.nn as nn

# 模型
model = nn.Linear(10, 1)
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
criterion = nn.MSELoss()

# 模拟数据
large_batch = torch.randn(128, 10)
targets = torch.randn(128, 1)

# 方法1: 直接处理（可能OOM）
# loss = criterion(model(large_batch), targets)
# loss.backward()

# 方法2: 梯度累积
accumulation_steps = 4
mini_batch_size = 128 // accumulation_steps

optimizer.zero_grad()

for i in range(accumulation_steps):
    start = i * mini_batch_size
    end = start + mini_batch_size
    
    # 小批量
    mini_x = large_batch[start:end]
    mini_y = targets[start:end]
    
    # 前向传播
    output = model(mini_x)
    loss = criterion(output, mini_y)
    
    # 缩放损失
    loss = loss / accumulation_steps
    
    # 反向传播（累积梯度）
    loss.backward()

# 更新参数（累积的梯度）
optimizer.step()

print("梯度累积完成")
```

---

## 🎯 常见陷阱

### 陷阱1: 忘记清零梯度

```python
import torch

x = torch.tensor([1.0], requires_grad=True)
optimizer = torch.optim.SGD([x], lr=0.1)

for i in range(3):
    y = x ** 2
    y.backward()
    
    print(f"迭代 {i+1}: x.grad = {x.grad}")
    # optimizer.zero_grad()  # ❌ 忘记清零！

# 正确做法
optimizer.zero_grad()  # 或 x.grad.zero_()
```

### 陷阱2: 在计算图中原地修改

```python
import torch

x = torch.tensor([1.0], requires_grad=True)
y = x ** 2

# ❌ 错误: 原地修改
# x.add_(1)
# y.backward()  # RuntimeError!

# ✅ 正确: 使用 detach
x_detached = x.detach()
x_detached.add_(1)
```

### 陷阱3: 多次反向传播

```python
import torch

x = torch.tensor([1.0], requires_grad=True)
y = x ** 2

y.backward()

# ❌ 默认情况下不能多次反向传播
# y.backward()  # RuntimeError!

# ✅ 保留计算图
x = torch.tensor([1.0], requires_grad=True)
y = x ** 2
y.backward(retain_graph=True)
y.backward()  # 现在可以了
```

---

## 📝 总结

### 核心要点

1. **创建可微张量**: `requires_grad=True`
2. **前向传播**: 构建计算图
3. **反向传播**: `.backward()`
4. **获取梯度**: `.grad`
5. **清零梯度**: `.zero_grad()` 或 `optimizer.zero_grad()`
6. **停止梯度**: `with torch.no_grad()` 或 `.detach()`

### 最佳实践

- ✅ 训练时启用梯度，推理时禁用
- ✅ 每次迭代后清零梯度
- ✅ 使用梯度裁剪防止梯度爆炸
- ✅ 自定义 Function 时进行梯度检查
- ⚠️ 小心原地操作
- ⚠️ 注意梯度累积

### 调试技巧

```python
import torch

x = torch.tensor([1.0], requires_grad=True)
y = x ** 2

# 检查是否需要梯度
print(f"requires_grad: {y.requires_grad}")

# 检查计算图
print(f"grad_fn: {y.grad_fn}")

# 检查是否是叶子节点
print(f"is_leaf: {y.is_leaf}")

# 查看梯度
y.backward()
print(f"grad: {x.grad}")
```

---

**下一步**: [神经网络构建](./7_神经网络构建.md)

*最后更新: 2025-10-17*
