# PyTorch æ¶æ„ä¸æ ¸å¿ƒåŸç†

## ğŸ“‹ ç›®å½•

- [æ•´ä½“æ¶æ„](#æ•´ä½“æ¶æ„)
- [æ ¸å¿ƒç»„ä»¶è¯¦è§£](#æ ¸å¿ƒç»„ä»¶è¯¦è§£)
- [åŠ¨æ€è®¡ç®—å›¾åŸç†](#åŠ¨æ€è®¡ç®—å›¾åŸç†)
- [å†…å­˜ç®¡ç†å’Œä¼˜åŒ–](#å†…å­˜ç®¡ç†å’Œä¼˜åŒ–)
- [å¤šè®¾å¤‡æ”¯æŒ](#å¤šè®¾å¤‡æ”¯æŒ)
- [æ ¸å¿ƒè®¾è®¡åŸåˆ™](#æ ¸å¿ƒè®¾è®¡åŸåˆ™)
- [ä¸å…¶ä»–æ¡†æ¶å¯¹æ¯”](#ä¸å…¶ä»–æ¡†æ¶å¯¹æ¯”)

---

## ğŸ—ï¸ æ•´ä½“æ¶æ„

PyTorch é‡‡ç”¨**åˆ†å±‚æ¶æ„è®¾è®¡**ï¼Œä»ä¸Šåˆ°ä¸‹åˆ†ä¸ºå¤šä¸ªå±‚æ¬¡ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       Python å‰ç«¯ API (ç”¨æˆ·æ¥å£)          â”‚
â”‚   torch, torch.nn, torch.optim ç­‰        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚     torch.nn (ç¥ç»ç½‘ç»œæ„å»ºæ¨¡å—)           â”‚
â”‚   Module, Linear, Conv2d, ReLU ç­‰        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   torch.autograd (è‡ªåŠ¨å¾®åˆ†å¼•æ“)          â”‚
â”‚   Function, backward(), grad ç­‰          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚     ATen (A Tensor Library)             â”‚
â”‚   å¼ é‡è¿ç®—çš„æ ¸å¿ƒå®ç°                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚    C++ åç«¯ (é«˜æ€§èƒ½è®¡ç®—)                 â”‚
â”‚   CPU/CUDA/ROCm/MPS ç­‰ç¡¬ä»¶åŠ é€Ÿ           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### å„å±‚èŒè´£

| å±‚æ¬¡ | èŒè´£ | ä¸»è¦ç»„ä»¶ |
|------|------|----------|
| **Python å‰ç«¯** | æä¾›ç”¨æˆ·å‹å¥½çš„ API | `torch.*` |
| **ç¥ç»ç½‘ç»œæ¨¡å—** | æ„å»ºç½‘ç»œå±‚å’Œæ¨¡å‹ | `nn.Module`, `nn.Linear` |
| **è‡ªåŠ¨å¾®åˆ†å¼•æ“** | è‡ªåŠ¨è®¡ç®—æ¢¯åº¦ | `autograd`, `Function` |
| **ATen å¼ é‡åº“** | é«˜æ•ˆçš„å¼ é‡è¿ç®— | C++ å¼ é‡æ“ä½œ |
| **åç«¯åŠ é€Ÿ** | ç¡¬ä»¶åŠ é€Ÿè®¡ç®— | CUDA, MKL, cuDNN |

---

## ğŸ”§ æ ¸å¿ƒç»„ä»¶è¯¦è§£

### 1. å¼ é‡ï¼ˆTensorï¼‰ç³»ç»Ÿ

å¼ é‡æ˜¯ PyTorch çš„**æ ¸å¿ƒæ•°æ®ç»“æ„**ï¼Œæ˜¯å¤šç»´æ•°ç»„çš„æ³›åŒ–ã€‚

#### å¼ é‡çš„ç‰¹ç‚¹

```python
import torch

x = torch.tensor([1.0, 2.0, 3.0])

# æ ¸å¿ƒç‰¹æ€§
print(f"æ•°æ®ç±»å‹: {x.dtype}")          # torch.float32
print(f"å½¢çŠ¶: {x.shape}")              # torch.Size([3])
print(f"è®¾å¤‡: {x.device}")             # cpu
print(f"æ˜¯å¦éœ€è¦æ¢¯åº¦: {x.requires_grad}") # False
```

#### å¼ é‡ä¸ NumPy çš„åŒºåˆ«

| ç‰¹æ€§ | PyTorch Tensor | NumPy ndarray |
|------|----------------|---------------|
| GPU åŠ é€Ÿ | âœ… æ”¯æŒ | âŒ ä¸æ”¯æŒ |
| è‡ªåŠ¨å¾®åˆ† | âœ… æ”¯æŒ | âŒ ä¸æ”¯æŒ |
| æ·±åº¦å­¦ä¹ ä¼˜åŒ– | âœ… é«˜åº¦ä¼˜åŒ– | âš ï¸ æœ‰é™ |
| ç”Ÿæ€ç³»ç»Ÿ | æ·±åº¦å­¦ä¹  | ç§‘å­¦è®¡ç®— |

#### å¼ é‡çš„å†…éƒ¨ç»“æ„

```python
import torch

x = torch.randn(3, 4)

# å†…éƒ¨ç»“æ„
print(f"å­˜å‚¨: {x.storage()}")      # åº•å±‚æ•°æ®å­˜å‚¨
print(f"æ­¥é•¿: {x.stride()}")       # (4, 1) - æ¯ä¸ªç»´åº¦çš„æ­¥é•¿
print(f"åç§»: {x.storage_offset()}")  # 0
print(f"æ˜¯å¦è¿ç»­: {x.is_contiguous()}")  # True
```

**å†…å­˜å¸ƒå±€ç¤ºä¾‹**ï¼š

```
å¼ é‡å½¢çŠ¶: [2, 3]
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1.0  2.0  3.0 â”‚ <- ç¬¬ä¸€è¡Œ
â”‚ 4.0  5.0  6.0 â”‚ <- ç¬¬äºŒè¡Œ
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

åº•å±‚å­˜å‚¨ (ä¸€ç»´): [1.0, 2.0, 3.0, 4.0, 5.0, 6.0]
æ­¥é•¿ (stride): (3, 1)
- ç¬¬ä¸€ç»´æ­¥é•¿=3: è·¨è¶Š3ä¸ªå…ƒç´ åˆ°ä¸‹ä¸€è¡Œ
- ç¬¬äºŒç»´æ­¥é•¿=1: è·¨è¶Š1ä¸ªå…ƒç´ åˆ°ä¸‹ä¸€åˆ—
```

---

### 2. è‡ªåŠ¨å¾®åˆ†å¼•æ“ï¼ˆAutogradï¼‰

è¿™æ˜¯ PyTorch **æœ€æ ¸å¿ƒçš„ç‰¹æ€§**ï¼Œå®ç°äº†åå‘ä¼ æ’­çš„è‡ªåŠ¨åŒ–ã€‚

#### å·¥ä½œåŸç†

```python
import torch

# 1. åˆ›å»ºéœ€è¦æ¢¯åº¦çš„å¼ é‡
x = torch.tensor([2.0], requires_grad=True)

# 2. å‰å‘ä¼ æ’­ï¼ˆæ„å»ºè®¡ç®—å›¾ï¼‰
y = x ** 2
z = y * 3

# 3. æŸ¥çœ‹è®¡ç®—å›¾
print(f"z.grad_fn: {z.grad_fn}")  # MulBackward0
print(f"y.grad_fn: {y.grad_fn}")  # PowBackward0

# 4. åå‘ä¼ æ’­ï¼ˆè®¡ç®—æ¢¯åº¦ï¼‰
z.backward()

# 5. è·å–æ¢¯åº¦
print(f"x.grad: {x.grad}")  # tensor([12.]) = dz/dx = 3 * 2 * x
```

#### è®¡ç®—å›¾å¯è§†åŒ–

```
å‰å‘ä¼ æ’­:
x (2.0) --[**2]--> y (4.0) --[*3]--> z (12.0)

åå‘ä¼ æ’­:
x <--[grad=12]-- y <--[grad=6]-- z <--[grad=1]

è®¡ç®—è¿‡ç¨‹:
dz/dz = 1
dz/dy = 3 (å› ä¸º z = y * 3)
dz/dx = dz/dy * dy/dx = 3 * 2x = 3 * 2 * 2 = 12
```

#### Autograd çš„æ ¸å¿ƒæ¦‚å¿µ

```python
import torch

# grad_fn: è®°å½•æ“ä½œ
x = torch.tensor([1.0], requires_grad=True)
y = x + 2
print(y.grad_fn)  # <AddBackward0 object>

# is_leaf: æ˜¯å¦æ˜¯å¶å­èŠ‚ç‚¹
print(x.is_leaf)  # True (ç”¨æˆ·åˆ›å»º)
print(y.is_leaf)  # False (è¿ç®—äº§ç”Ÿ)

# retain_grad: ä¿ç•™ä¸­é—´æ¢¯åº¦
y.retain_grad()
z = y ** 2
z.backward()
print(y.grad)  # å¯ä»¥è®¿é—®ä¸­é—´æ¢¯åº¦
```

---

### 3. ç¥ç»ç½‘ç»œæ¨¡å—ï¼ˆtorch.nnï¼‰

æä¾›æ„å»ºç¥ç»ç½‘ç»œçš„**é«˜çº§æŠ½è±¡**ã€‚

#### nn.Module æ¶æ„

```python
import torch.nn as nn

class MyModel(nn.Module):
    def __init__(self):
        super().__init__()
        # å®šä¹‰å±‚
        self.layer1 = nn.Linear(10, 20)
        self.activation = nn.ReLU()
        self.layer2 = nn.Linear(20, 1)
    
    def forward(self, x):
        # å®šä¹‰å‰å‘ä¼ æ’­
        x = self.layer1(x)
        x = self.activation(x)
        x = self.layer2(x)
        return x

model = MyModel()

# æŸ¥çœ‹æ¨¡å‹ç»“æ„
print(model)

# è®¿é—®å‚æ•°
for name, param in model.named_parameters():
    print(f"{name}: {param.shape}")
```

#### nn.Module çš„æ ¸å¿ƒåŠŸèƒ½

```python
# 1. å‚æ•°ç®¡ç†
model.parameters()  # æ‰€æœ‰å¯è®­ç»ƒå‚æ•°
model.state_dict()  # å‚æ•°å­—å…¸ï¼ˆä¿å­˜/åŠ è½½ï¼‰

# 2. æ¨¡å¼åˆ‡æ¢
model.train()  # è®­ç»ƒæ¨¡å¼
model.eval()   # è¯„ä¼°æ¨¡å¼

# 3. è®¾å¤‡è½¬ç§»
model.to('cuda')  # ç§»åˆ° GPU
model.cpu()       # ç§»åˆ° CPU

# 4. å­æ¨¡å—è®¿é—®
model.children()  # ç›´æ¥å­æ¨¡å—
model.modules()   # æ‰€æœ‰æ¨¡å—ï¼ˆé€’å½’ï¼‰
```

---

## ğŸ”„ åŠ¨æ€è®¡ç®—å›¾åŸç†

PyTorch ä½¿ç”¨**åŠ¨æ€è®¡ç®—å›¾**ï¼ˆDefine-by-Runï¼‰ï¼Œè¿™æ˜¯å…¶æ ¸å¿ƒç‰¹è‰²ã€‚

### åŠ¨æ€ vs é™æ€è®¡ç®—å›¾

| ç‰¹æ€§ | åŠ¨æ€è®¡ç®—å›¾ (PyTorch) | é™æ€è®¡ç®—å›¾ (TensorFlow 1.x) |
|------|----------------------|----------------------------|
| **å®šä¹‰æ—¶æœº** | è¿è¡Œæ—¶æ„å»º | é¢„å…ˆå®šä¹‰ |
| **çµæ´»æ€§** | âœ… æé«˜ | âš ï¸ æœ‰é™ |
| **è°ƒè¯•** | âœ… å®¹æ˜“ | âŒ å›°éš¾ |
| **ä¼˜åŒ–** | âš ï¸ æœ‰é™ | âœ… å……åˆ† |
| **æ§åˆ¶æµ** | âœ… Python åŸç”Ÿ | âš ï¸ éœ€è¦ç‰¹æ®Šæ“ä½œ |

### åŠ¨æ€è®¡ç®—å›¾ç¤ºä¾‹

```python
import torch

def dynamic_network(x, use_extra_layer):
    """åŠ¨æ€ç½‘ç»œï¼šæ ¹æ®æ¡ä»¶æ”¹å˜ç»“æ„"""
    x = x * 2
    
    # åŠ¨æ€æ§åˆ¶æµ
    if use_extra_layer:
        x = x + 10
    
    # åŠ¨æ€å¾ªç¯
    for i in range(3):
        x = x * 1.1
    
    return x

x = torch.tensor([1.0], requires_grad=True)

# æ¯æ¬¡è°ƒç”¨éƒ½æ„å»ºä¸åŒçš„è®¡ç®—å›¾
y1 = dynamic_network(x, use_extra_layer=True)
y2 = dynamic_network(x, use_extra_layer=False)

print(f"y1: {y1}")  # ä¸åŒçš„ç»“æœ
print(f"y2: {y2}")
```

### è®¡ç®—å›¾çš„ç”Ÿå‘½å‘¨æœŸ

```python
import torch

x = torch.tensor([2.0], requires_grad=True)

# é˜¶æ®µ1: æ„å»ºè®¡ç®—å›¾
y = x ** 2
z = y + 3

# é˜¶æ®µ2: åå‘ä¼ æ’­
z.backward()

# é˜¶æ®µ3: è®¡ç®—å›¾è¢«é‡Šæ”¾
# å°è¯•å†æ¬¡åå‘ä¼ æ’­ä¼šæŠ¥é”™
try:
    z.backward()  # RuntimeError!
except RuntimeError as e:
    print("è®¡ç®—å›¾å·²è¢«é‡Šæ”¾")

# å¦‚æœéœ€è¦å¤šæ¬¡åå‘ä¼ æ’­
x = torch.tensor([2.0], requires_grad=True)
y = x ** 2
y.backward(retain_graph=True)  # ä¿ç•™è®¡ç®—å›¾
y.backward()  # å¯ä»¥å†æ¬¡è°ƒç”¨
```

---

## ğŸ’¾ å†…å­˜ç®¡ç†å’Œä¼˜åŒ–

### 1. å°±åœ°æ“ä½œï¼ˆIn-place Operationsï¼‰

```python
import torch

x = torch.tensor([1.0, 2.0, 3.0])

# éå°±åœ°æ“ä½œï¼ˆåˆ›å»ºæ–°å¼ é‡ï¼‰
y = x + 5
print(id(x), id(y))  # ä¸åŒçš„å†…å­˜åœ°å€

# å°±åœ°æ“ä½œï¼ˆä¿®æ”¹åŸå¼ é‡ï¼‰
x.add_(5)  # æ³¨æ„ä¸‹åˆ’çº¿åç¼€
print(x)  # tensor([6., 7., 8.])

# å¸¸è§å°±åœ°æ“ä½œ
x.mul_(2)     # x *= 2
x.zero_()     # x = 0
x.fill_(5)    # x = 5
```

âš ï¸ **æ³¨æ„**ï¼šå°±åœ°æ“ä½œä¼šå½±å“æ¢¯åº¦è®¡ç®—ï¼Œæ…ç”¨ï¼

```python
x = torch.tensor([2.0], requires_grad=True)
y = x ** 2
x.add_(1)  # ä¿®æ”¹äº† xï¼Œä½†è®¡ç®—å›¾å·²ç»è®°å½•äº†æ—§å€¼
# åå‘ä¼ æ’­å¯èƒ½å‡ºç°é—®é¢˜
```

### 2. æ¢¯åº¦ç®¡ç†

```python
import torch

# åœæ­¢æ¢¯åº¦è·Ÿè¸ª
x = torch.randn(3, requires_grad=True)

# æ–¹æ³•1: with torch.no_grad()
with torch.no_grad():
    y = x * 2  # ä¸ä¼šæ„å»ºè®¡ç®—å›¾
    print(y.requires_grad)  # False

# æ–¹æ³•2: @torch.no_grad() è£…é¥°å™¨
@torch.no_grad()
def inference(model, x):
    return model(x)

# æ–¹æ³•3: .detach()
y = x.detach()  # åˆ†ç¦»å‡ºä¸€ä¸ªä¸éœ€è¦æ¢¯åº¦çš„å¼ é‡
```

### 3. å†…å­˜ä¼˜åŒ–æŠ€å·§

```python
import torch

# 1. åˆ é™¤ä¸éœ€è¦çš„å¼ é‡
x = torch.randn(1000, 1000)
del x  # é‡Šæ”¾å†…å­˜

# 2. æ¸…ç©º CUDA ç¼“å­˜
if torch.cuda.is_available():
    torch.cuda.empty_cache()

# 3. ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯å‡å°‘å†…å­˜
# è€Œä¸æ˜¯ï¼š
# loss = criterion(model(large_batch), target)
# loss.backward()

# ä½¿ç”¨ï¼š
for mini_batch in split_batch(large_batch):
    loss = criterion(model(mini_batch), target)
    loss = loss / num_mini_batches
    loss.backward()  # æ¢¯åº¦ç´¯ç§¯

# 4. ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()
with autocast():
    output = model(input)
    loss = criterion(output, target)
scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

---

## ğŸ–¥ï¸ å¤šè®¾å¤‡æ”¯æŒ

### è®¾å¤‡ç®¡ç†

```python
import torch

# æ£€æŸ¥å¯ç”¨è®¾å¤‡
print(f"CUDA å¯ç”¨: {torch.cuda.is_available()}")
print(f"CUDA è®¾å¤‡æ•°é‡: {torch.cuda.device_count()}")
print(f"å½“å‰è®¾å¤‡: {torch.cuda.current_device()}")

# åˆ›å»ºè®¾å¤‡å¯¹è±¡
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# å°†å¼ é‡ç§»åˆ°è®¾å¤‡
x = torch.randn(3, 4)
x = x.to(device)

# å°†æ¨¡å‹ç§»åˆ°è®¾å¤‡
import torch.nn as nn
model = nn.Linear(10, 5)
model = model.to(device)

# ç¡®ä¿æ•°æ®å’Œæ¨¡å‹åœ¨åŒä¸€è®¾å¤‡
input_data = torch.randn(32, 10).to(device)
output = model(input_data)
```

### å¤š GPU è®­ç»ƒ

```python
import torch.nn as nn

model = nn.Linear(10, 5)

# æ•°æ®å¹¶è¡Œ
if torch.cuda.device_count() > 1:
    model = nn.DataParallel(model)
    
model = model.to('cuda')

# ä½¿ç”¨
input_data = torch.randn(32, 10).to('cuda')
output = model(input_data)  # è‡ªåŠ¨åˆ†é…åˆ°å¤šä¸ª GPU
```

---

## ğŸ¨ æ ¸å¿ƒè®¾è®¡åŸåˆ™

### 1. Pythonic

```python
# PyTorch è®¾è®¡è´´è¿‘ Python
import torch

# åˆ—è¡¨æ¨å¯¼å¼
tensors = [torch.randn(3, 4) for _ in range(5)]

# æ¡ä»¶è¡¨è¾¾å¼
x = torch.randn(10)
y = x if x.sum() > 0 else -x

# è¿­ä»£å™¨
for param in model.parameters():
    print(param.shape)
```

### 2. åŠ¨æ€æ€§

```python
# è¿è¡Œæ—¶å†³å®šç½‘ç»œç»“æ„
def adaptive_network(x, depth):
    for i in range(depth):
        x = x * 2
    return x
```

### 3. æ¨¡å—åŒ–

```python
# æ¯ä¸ªç»„ä»¶éƒ½å¯ä»¥ç‹¬ç«‹ä½¿ç”¨
import torch
import torch.nn as nn
import torch.optim as optim

# åªç”¨å¼ é‡
x = torch.randn(10)

# åªç”¨è‡ªåŠ¨å¾®åˆ†
x = torch.randn(10, requires_grad=True)
y = x.sum()
y.backward()

# åªç”¨ç¥ç»ç½‘ç»œæ¨¡å—
layer = nn.Linear(10, 5)
```

### 4. æ‰©å±•æ€§

```python
# è‡ªå®šä¹‰è‡ªåŠ¨å¾®åˆ†å‡½æ•°
class MyFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, input):
        ctx.save_for_backward(input)
        return input.clamp(min=0)
    
    @staticmethod
    def backward(ctx, grad_output):
        input, = ctx.saved_tensors
        grad_input = grad_output.clone()
        grad_input[input < 0] = 0
        return grad_input
```

---

## âš–ï¸ ä¸å…¶ä»–æ¡†æ¶å¯¹æ¯”

### PyTorch vs TensorFlow

| ç‰¹æ€§ | PyTorch | TensorFlow 2.x |
|------|---------|----------------|
| **è®¡ç®—å›¾** | åŠ¨æ€ | åŠ¨æ€ + é™æ€ï¼ˆ`@tf.function`ï¼‰ |
| **API è®¾è®¡** | Pythonic | Keras é£æ ¼ |
| **è°ƒè¯•** | âœ… å®¹æ˜“ | âš ï¸ ä¸­ç­‰ |
| **éƒ¨ç½²** | TorchScript | TF Serving, TFLite |
| **ç¤¾åŒº** | å­¦æœ¯ç•Œä¸»å¯¼ | å·¥ä¸šç•Œä¸»å¯¼ |
| **å¯è§†åŒ–** | TensorBoard | TensorBoard |
| **ç§»åŠ¨ç«¯** | PyTorch Mobile | TensorFlow Lite |

### ä»£ç å¯¹æ¯”

```python
# PyTorch
import torch
import torch.nn as nn

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = nn.Linear(10, 1)
    
    def forward(self, x):
        return self.linear(x)

model = Model()
```

```python
# TensorFlow 2.x
import tensorflow as tf

class Model(tf.keras.Model):
    def __init__(self):
        super().__init__()
        self.linear = tf.keras.layers.Dense(1)
    
    def call(self, x):
        return self.linear(x)

model = Model()
```

---

## ğŸ“Š æ€§èƒ½è€ƒé‡

### è®¡ç®—å›¾å¼€é”€

```python
import torch
import time

x = torch.randn(1000, 1000, requires_grad=True)

# åŠ¨æ€å›¾ï¼šæ¯æ¬¡éƒ½é‡æ–°æ„å»º
start = time.time()
for _ in range(100):
    y = x * 2
    y = y.sum()
    y.backward()
    x.grad.zero_()
print(f"åŠ¨æ€å›¾æ—¶é—´: {time.time() - start:.4f}s")

# ä½¿ç”¨ JIT ç¼–è¯‘ä¼˜åŒ–
@torch.jit.script
def optimized_op(x):
    return (x * 2).sum()

start = time.time()
for _ in range(100):
    y = optimized_op(x)
    y.backward()
    x.grad.zero_()
print(f"JIT ç¼–è¯‘æ—¶é—´: {time.time() - start:.4f}s")
```

---

## ğŸ¯ æ€»ç»“

PyTorch çš„æ¶æ„è®¾è®¡ä½“ç°äº†ä»¥ä¸‹æ ¸å¿ƒæ€æƒ³ï¼š

1. **åŠ¨æ€ä¼˜å…ˆ**ï¼šè¿è¡Œæ—¶æ„å»ºè®¡ç®—å›¾ï¼Œæä¾›æœ€å¤§çµæ´»æ€§
2. **ç”¨æˆ·å‹å¥½**ï¼šPythonic çš„ API è®¾è®¡ï¼Œé™ä½å­¦ä¹ æ›²çº¿
3. **é«˜æ€§èƒ½**ï¼šåº•å±‚ C++/CUDA å®ç°ï¼Œä¿è¯è®¡ç®—æ•ˆç‡
4. **æ¨¡å—åŒ–**ï¼šç»„ä»¶å¯ç‹¬ç«‹ä½¿ç”¨ï¼Œæ˜“äºæ‰©å±•
5. **ç ”ç©¶å¯¼å‘**ï¼šä¸“æ³¨äºå¿«é€Ÿå®éªŒå’ŒåŸå‹å¼€å‘

è¿™äº›è®¾è®¡è®© PyTorch æˆä¸ºæ·±åº¦å­¦ä¹ ç ”ç©¶çš„é¦–é€‰æ¡†æ¶ï¼

---

**ä¸‹ä¸€æ­¥**: [å¼ é‡æ“ä½œè¯¦è§£](./7_å¼ é‡æ“ä½œè¯¦è§£.md)

*æœ€åæ›´æ–°: 2025-10-17*
